---
title: "Lesson 12"
author: "James B. Elsner"
date: "February 17, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The most important single aspect of software development is to be clear about what you are trying to build."** – Bjarne Stroustrup

### Example: Population and tornadoes in Iowa

We are interested in quantifying the bi-variate autocorrelation between tornado occurrences and population.

Get U.S. Census data with functions from the {tidycensus} package. When working outside this class you first need to obtain a API key from http://api.census.gov/data/key_signup.html. Then set and save the key.
```{r}
library(tidycensus)
```

The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how you would get county-level population  for Iowa.
```{r}
IA_counties.sf <- get_acs(geography = "county", 
                          variables = "B02001_001E", 
                          state = "IA", 
                          geometry = TRUE)
```

This returns a simple feature data frame with county borders as multipolygons. The variable `B02001_001E` is the 2015 (mid year of the 5-year period 2013-2017) population in each county.

Now get the tornado locations and compute the annual tornado occurrence rate for each county. We did this at the state level in Example 2 of Lesson 8.

Start by first determining the intersections of the county polygons and the tornado points.
```{r}
Alltors.sfdf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
                        filter(yr >= 1994) %>%
                        st_transform(crs = st_crs(IA_counties.sf))

mtrx <- st_contains(IA_counties.sf, 
                    Alltors.sfdf, 
                    sparse = FALSE)

nT <- rowSums(mtrx)
CountyArea <- st_area(IA_counties.sf)

( IA_counties.sf <- IA_counties.sf %>%
  mutate(nT,
         rate = nT/CountyArea/(2018 - 1994 + 1) * 10^10, 
         lpop = log(estimate)) )
```

Make a two-panel map displaying the log of the population and the tornado rates.
```{r, eval=FALSE}
map1 <- tm_shape(IA_counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "lpop",
          title = "Log Population",
          palette = "Blues") +
  tm_layout(legend.outside = "TRUE")

map2 <- tm_shape(IA_counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "rate",
          title = "Annual Rate\n[/10,000 sq. km]",
          palette = "Greens") +
  tm_layout(legend.outside = "TRUE")

tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function.
```{r}
lpop <- IA_counties.sf$lpop
rate <- as.numeric(IA_counties.sf$rate)

cor.test(lpop, rate)
```

The bivariate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bivariate spatial autocorrelation is done using a Monte Carlo simulation.
```{r}
nbs <- poly2nb(IA_counties.sf)
wts <- nb2listw(nbs)

lee_stat <- lee(lpop, rate, 
                listw = wts, 
                n = length(nbs))
lee_stat$L

lee.mc(lpop, rate, listw = wts, nsim = 999)
```

Finally we map out the local variation in the bivariate spatial autocorrelation.
```{r}
IA_counties.sf$localL <- lee_stat$localL

tm_shape(IA_counties.sf) +
  tm_fill("localL",
          title = "Local Bivariate\nSpatial Autocorrelation") +
  tm_borders(col = "gray70") +
  tm_layout(legend.outside = TRUE)
```

What might cause this?

## Constraining group membership based on autocorrelation

From: https://www.dshkol.com/post/spatially-constrained-clustering-and-regionalization/

The advantage of spatially constrained clustering methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. 

There are a lot of cases that require separating geographies into discrete but contiguous regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. 

There are many situations where the optimal grouping, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.

Unconstrained grouping on data with spatial characteristics will result in contiguous regions because of autocorrelation, but if we want to ensure that all objects are in entirely spatially-contiguous groups we need a method specifically designed for the task. The 'skater' algorithm available in R via the {spdep} package is well-implemented and well-documented.

The 'skater' (spatial ’k’luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas (see https://www.tandfonline.com/doi/abs/10.1080/13658810600665111). Edge costs are calculated by evaluating the dissimilarity between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates.

Consider crime data at the tract level in the city of Columbus, Ohio (Anselin, 1988: Spatial Econometrics. Boston, Kluwer Academic). The tract polygons are projected with arbitrary spatial coordinates.
```{r}
if(!"columbus" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = "columbus.zip")
unzip("columbus.zip")
}

( CC.sf <- read_sf(dsn = "columbus") )
```

We check if there is a clear spatial pattern to this data.
```{r}
plot(CC.sf[,7:9])
```

Promising start as there looks to be some fairly distinct regional patterns happening. 

Next, scale variable values and center them. This is done regardless of clustering approach.
```{r}
( CCs.df <- CC.sf %>% 
    mutate(HOVAL = scale(HOVAL),
           INC = scale(INC),
           CRIME = scale(CRIME)) %>%
    select(HOVAL, INC, CRIME) %>%
    st_drop_geometry() )
```

Next create the adjacency neighbor structure using rook contiquity.
```{r}
nbs <- poly2nb(CC.sf, 
               queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     st_centroid(st_geometry(CC.sf)),
     add = TRUE)
```

Next we combine the contiguity graph with our scaled attribute data to calculate edge costs based on the statistical distance between each node. The function `nbcosts()` from the {spdep} package provides distance methods for Euclidian, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified.
```{r}
costs <- nbcosts(nbs, 
                 data = CC.sf)
```

Next we transform the edge costs into spatial weights using the `nb2list2()` function before constructing the minimum spanning tree with the weights list.
```{r}
wts <- nb2listw(nbs,
                glist = costs,
                style = "B")
mst <- mstree(wts)

plot(mst, 
     coordinates(as_Spatial(CC.sf)), 
     col = "blue")
```

Edges with higher dissimilarity are removed sequentially until left with a spanning tree that takes the minimum sum of dissimilarities across all edges of the tree, hence minimum spanning tree. At this point, any further reduction in edges would create disconnected sub-graphs which then lead to the resulting spatial clusters.

Once the minimum spanning tree is in place, the SKATER algorithm comes in to partition the MST. It partitions the graph identifying which edge to remove to maximize the quality of resulting clusters as measured by the sum of the inter-cluster square deviations SSD. Regions that are similar to one another have lower values. This is implemented with the `skater` function and the `ncuts =` argument indicates the number of partitions to make, resulting in ncuts + 1 groups.
```{r}
clus5 <- skater(edges = mst[,1:2], 
                data = CCs.df, 
                ncuts = 4)
```

Where are these groups located?
```{r}
CC.sf <- CC.sf %>%
  mutate(Group = clus5$groups)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

Hierarchical clustering
```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)
CC.sf <- CC.sf %>%
  mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))

```