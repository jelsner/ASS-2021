---
title: "Lesson 12"
author: "James B. Elsner"
date: "February 17, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The most important single aspect of software development is to be clear about what you are trying to build."** – Bjarne Stroustrup

## Constraining group membership based on autocorrelation

From: https://www.dshkol.com/post/spatially-constrained-clustering-and-regionalization/

The advantage of spatially constrained clustering methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. 

There are a lot of cases that require separating geographies into discrete but contiguous regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. 

There are many situations where the optimal grouping, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.

Unconstrained grouping on data with spatial characteristics will result in contiguous regions because of autocorrelation, but if we want to ensure that all objects are in entirely spatially-contiguous groups we need a method specifically designed for the task. The 'skater' algorithm available in R via the {spdep} package is well-implemented and well-documented.

The 'skater' (spatial ’k’luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas (see https://www.tandfonline.com/doi/abs/10.1080/13658810600665111). Edge costs are calculated by evaluating the dissimilarity between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates.

Consider crime data at the tract level in the city of Columbus, Ohio (Anselin, 1988: Spatial Econometrics. Boston, Kluwer Academic). The tract polygons are projected with arbitrary spatial coordinates.
```{r}
if(!"columbus" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = "columbus.zip")
unzip("columbus.zip")
}

( CC.sf <- read_sf(dsn = "columbus") )
```

We check if there is a clear spatial pattern to this data.
```{r}
plot(CC.sf[,7:9])
```

Promising start as there looks to be some fairly distinct regional patterns happening. 

Next, scale variable values and center them. This is done regardless of clustering approach.
```{r}
( CCs.df <- CC.sf %>% 
    mutate(HOVAL = scale(HOVAL),
           INC = scale(INC),
           CRIME = scale(CRIME)) %>%
    select(HOVAL, INC, CRIME) %>%
    st_drop_geometry() )
```

Next create the adjacency neighbor structure using rook contiguity.
```{r}
nbs <- poly2nb(CC.sf, 
               queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     st_centroid(st_geometry(CC.sf)),
     add = TRUE)
```

Next we combine the contiguity graph with our scaled attribute data to calculate edge costs based on the statistical distance between each node. The function `nbcosts()` from the {spdep} package provides distance methods for Euclidian, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified.
```{r}
costs <- nbcosts(nbs, 
                 data = CC.sf)
```

Next we transform the edge costs into spatial weights using the `nb2list2()` function before constructing the minimum spanning tree with the weights list.
```{r}
wts <- nb2listw(nbs,
                glist = costs,
                style = "B")
mst <- mstree(wts)

plot(mst, 
     coordinates(as_Spatial(CC.sf)), 
     col = "blue")
```

Edges with higher dissimilarity are removed sequentially until left with a spanning tree that takes the minimum sum of dissimilarities across all edges of the tree, hence minimum spanning tree. At this point, any further reduction in edges would create disconnected sub-graphs which then lead to the resulting spatial clusters.

Once the minimum spanning tree is in place, the SKATER algorithm comes in to partition the MST. It partitions the graph identifying which edge to remove to maximize the quality of resulting clusters as measured by the sum of the inter-cluster square deviations SSD. Regions that are similar to one another have lower values. This is implemented with the `skater` function and the `ncuts =` argument indicates the number of partitions to make, resulting in ncuts + 1 groups.
```{r}
clus5 <- skater(edges = mst[,1:2], 
                data = CCs.df, 
                ncuts = 4)
```

Where are these groups located?
```{r}
CC.sf <- CC.sf %>%
  mutate(Group = clus5$groups)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

Hierarchical clustering
```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)
CC.sf <- CC.sf %>%
  mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))

```
## Autocorrelation in model residuals

Knowledge of significant autocorrelation by itself is not typically useful. But knowledge of significant autocorrelation in the residuals from some statistical model can help us build a more precise model.

A spatial regression model may be needed whenever the residuals resulting from a aspatial regression model exhibit significant spatial autocorrelation. So a common way to proceed is to first regress the response variable onto the explanatory variables and check for spatial autocorrelation in the residuals.

If the explanatory variables remove the spatial autocorrelation then a spatial regression model is not needed.

Let's continue with the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables.
```{r}
model <- lm(CRIME ~ INC + HOVAL, 
            data = CC.sf)
summary(model)
```

The model statistically explains 55% of the variation in crime. As income and housing values increase crime goes down. 

We use the `residuals()` method to extract the vector of residuals from the model.
```{r}
res <- residuals(model)
```

We then check on the distribution of the residuals relative to a normal distribution.
```{r}
sm::sm.density(res, 
               model = "Normal")
```

The next step is to create a choropleth map of the model residuals. Are the residuals clustered?
```{r}
CC.sf$res <- res

library(tmap)
tm_shape(CC.sf) +
  tm_fill("res") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Linear Model Residuals")
```

Yes. There are clustered regions where the model over predicts crime conditional on household income and housing values and where it under predicts crime.

The amount of clustering is less than before. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced.

To determine I on the residuals we use the `lm.morantest()` function and pass the regression model object and the weights object to it.
```{r}
nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)
lm.morantest(model, wts)
```

Moran I on the model residuals is .222.  This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors.

Do we need a spatial regression model?  The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.