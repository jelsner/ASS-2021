---
title: "Lesson 12"
author: "James B. Elsner"
date: "February 17, 2021"
output:
  pdf_document: default
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The most important single aspect of software development is to be clear about what you are trying to build."** â€“ Bjarne Stroustrup

Friday is a mid-semester wellness day. No lesson and no assignment!

## Reviewing autocorrelation through an extended example: Population and tornadoes

Tornado reports, not tornadoes.

Is the frequency of tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring region?

To answer these questions we quantify the aspatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable in terms of computational time we restrict the focus to the state of Iowa.

We start by getting the U.S. Census data with functions from the {tidycensus} package. The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how we get county-level population for Iowa. The variable `B02001_001E` is the 2015 (mid year of the 5-year period 2013-2017) population in each county.
```{r}
library(tidycensus)
library(sf)

( Counties.sf <- get_acs(geography = "county", 
                       variables = "B02001_001E", 
                       state = "IA", 
                       geometry = TRUE) )
```

This returns a simple feature data frame with 99 rows (one row for each  of the 99 counties in the state) and the county borders as a multi-polygon simple feature column in the column labeled `geometry`. The variable `GEOID` is unique for each county. And the variable `estimate` is the population.

Next we get the tornado report (initial) locations filtering by year greater or equal to 2013.
```{r}
library(tidyverse)

if(!"1950-2018-torn-initpoint" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/1950-2018-torn-initpoint.zip",
              destfile = "1950-2018-torn-initpoint.zip")
unzip("1950-2018-torn-aspath.zip")
}

( Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint", 
                   layer = "1950-2018-torn-initpoint") %>%
  st_transform(crs = st_crs(Counties.sf)) %>%
  filter(yr >= 2013) )
```

Then we determine the intersections of tornado point geometries with the county polygon geometries and count the number of intersections grouped by `GEOID`.
```{r}
( TorCounts.df <- Torn.sf %>%
  st_intersection(Counties.sf) %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarize(nT = n()) )
```

The result is a two-column data frame containing the `GEOID` and the number of tornadoes (`nT`). Note that there are only 86 rows indicating that some counties did not experience a tornado genesis over this time period.

Then we join the counts to the simple feature data frame by `GEOID`. Counties without tornadoes are given a value of `NA` in the column `nT`. We replace those with 0 with the `replace_nt()` function inside the `mutate()` function. We then compute the county area (in square meters), the annual tornado rate (in 100 square kilometers-10^10), and the logarithm (base 10) of the population estimate.
```{r}
( Counties.sf <- Counties.sf %>%
  left_join(TorCounts.df,
            by = "GEOID") %>%
  mutate(nT = replace_na(nT, 0)) %>%
  mutate(Area = st_area(Counties.sf),
         rate = nT/Area/(2018 - 2013 + 1) * 10^10,
         lpop = log10(estimate)) )
```

Make a two-panel map displaying the log of the population and the tornado rates.
```{r}
library(tmap)

map1 <- tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "lpop",
          title = "Log Population",
          palette = "Blues") +
  tm_layout(legend.outside = "TRUE")

map2 <- tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "rate",
          title = "Annual Rate\n[/10,000 sq. km]",
          palette = "Greens") +
  tm_layout(legend.outside = "TRUE")

tmap_arrange(map1, map2)
```

There appears to be some relationship. Counties with more people tend to have more tornado reports. This is quantified with the aspatial correlation between the two variables obtained with the `cor.test()` function.
```{r}
lpop <- Counties.sf$lpop
rate <- as.numeric(Counties.sf$rate)

cor.test(lpop, rate)
```

The value of .35 is small, but statistically significant ($p$-value < .01) against the null hypothesis of no correlation.

The bi-variate spatial autocorrelation is estimated using the Lee statistic. A formal non-parametric test under the null hypothesis of no correlation is done using a Monte Carlo simulation.
```{r}
library(spdep)

nbs <- poly2nb(Counties.sf)
wts <- nb2listw(nbs)

lee_stat <- lee(lpop, rate, 
                listw = wts, 
                n = length(nbs))
lee_stat$L

lee.mc(lpop, rate, 
       listw = wts, 
       nsim = 9999)
```

Finally we map out the local variation in the bi-variate spatial autocorrelation.
```{r}
Counties.sf$localL <- lee_stat$localL

tm_shape(Counties.sf) +
  tm_fill("localL",
          title = "Local bi-variate\nspatial autocorrelation") +
  tm_borders(col = "gray70") +
  tm_layout(legend.outside = TRUE)
```

What might cause this? Compare with Kansas.

Also, compare local Lee with local Moran I.
```{r}
Ii_stats <- localmoran(rate, 
                       listw = wts)
Counties.sf$localI = Ii_stats[, 1]

tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "localI",
          title = "Local autocorrelation",
          palette = "Purples") +
  tm_layout(legend.outside = "TRUE")
```

## Constraining group membership based on autocorrelation

We often face the situation where we have a large number of variables and we want to group them in a way that minimizes inter-group variation but maximizes between-group variation. This grouping or 'clustering' is important in data analysis and data mining. Traditional clustering methods include K-means and hierarchical.

With spatial data we want the additional constraint that groups be geographically linked. In fact there are many situations that require separating geographies into discrete but contiguous regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints.

There are many situations where the optimal grouping using traditional cluster metrics is sub-optimal in practice because of these geographic constraints.

Unconstrained grouping on data with spatial characteristics will result in contiguous regions because of autocorrelation, but if we want to ensure that all groups are spatially-contiguous we need a method specifically designed for the task. The 'skater' algorithm available in the {spdep} package is well-implemented and well-documented.

The 'skater' algorithm (spatial 'k'luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity in attribute space between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity.

More information: https://www.tandfonline.com/doi/abs/10.1080/13658810600665111

Consider again the crime data at the tract level in the city of Columbus, Ohio. The tract polygons are projected with arbitrary spatial coordinates.
```{r}
if(!"columbus" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = "columbus.zip")
unzip("columbus.zip")
}

( CC.sf <- read_sf(dsn = "columbus") )
```

We create choropleth maps of housing value, income, and crime.
```{r}
tm_shape(CC.sf) +
  tm_fill(col = c("HOVAL", "INC", "CRIME"))
```

The maps indicate distinct regional patterns.

Next we scale the values and center them using the `scale()` function. This should always be done even with aspatial clustering approaches.
```{r}
( CCs.df <- CC.sf %>% 
    mutate(HOVAL = scale(HOVAL),
           INC = scale(INC),
           CRIME = scale(CRIME)) %>%
    select(HOVAL, INC, CRIME) %>%
    st_drop_geometry() )
```

Next create adjacency neighborhoods using queen contiguity.
```{r}
nbs <- poly2nb(CC.sf, 
               queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     st_centroid(st_geometry(CC.sf)),
     add = TRUE)
```

Next we combine the contiguity graph with our scaled attribute data to calculate edge costs based on the distance between each node. The function `nbcosts()` from the {spdep} package provides distance methods for Euclidian, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified.
```{r}
costs <- nbcosts(nbs, 
                 data = CC.sf)
```

Next we transform the edge costs into spatial weights using the `nb2list2()` function before constructing the minimum spanning tree with the weights list.
```{r}
wts <- nb2listw(nbs,
                glist = costs,
                style = "B")
mst <- mstree(wts)

head(mst)
```

Edges with higher dissimilarity are removed leaving a set of nodes and edges that take the minimum sum of dissimilarities across all edges of the tree (a minimum spanning tree).

The edge connecting node 23 with node 32 has a dissimilarity of 15.7 units. The edge connection node 32 with node 41 has a dissimilarity of 14.1 units.

Finally, the `skater()` function partitions the graph by identifying which edges to remove based on dissimilarity while maximizing the between-group variation. The `ncuts =` argument specifies the number of partitions to make, resulting in `ncuts` + 1 groups.
```{r}
clus5 <- skater(edges = mst[,1:2], 
                data = CCs.df, 
                ncuts = 4)
```

Where are these groups located?
```{r}
CC.sf <- CC.sf %>%
  mutate(Group = clus5$groups)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

As a comparison, here is the result of grouping the same three variables using hierarchical clustering using the method of minimum variance (Ward) and without regard to spatial contiguity.
```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)
CC.sf <- CC.sf %>%
  mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))
```

## Estimating autocorrelation in the model residuals

Knowledge of autocorrelation by itself is not typically useful. But autocorrelation in the residuals can help us build a more precise model.

A spatial regression model should be considered whenever the residuals resulting from a aspatial regression model exhibit spatial autocorrelation. A common way to proceed is to first regress the response variable onto the explanatory variables and check for autocorrelation in the residuals.

If the explanatory variables remove the autocorrelation then a spatial regression model is not needed.

Let's continue with the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables.
```{r}
model <- lm(CRIME ~ INC + HOVAL, 
            data = CC.sf)
summary(model)
```

The model statistically explains 55% of the variation in crime as can be seen by the multiple R-squared value. Looking at the coefficients (values under the `Estimate` column), we see that _higher_ incomes are associated with lower values of crime (negative coefficient) and _higher_ housing values are associated with lower crime. For every one unit increase in income, crime values decrease by 1.6 units.

We use the `residuals()` method to extract the vector of residuals from the model.
```{r}
( res <- residuals(model) )
```

There are 49 residuals one for each tract. The residuals are the difference between the observed crime rates and the predicted crime rates (observed - predicted). A residual that is greater than 0 indicates that the model _under_ predicts the observed crime rate in that tract and a residual less than 0 indicates that the model _over_ predicts the observed crime rate.

The aspatial variability of the residuals should follow a normal distribution. We check this with the `sm.density()` function from the {sm} package with the first argument the vector of residuals (`res`) and the argument `model =` set to "Normal".
```{r}
sm::sm.density(res, 
               model = "Normal")
```

We see that density curve of the residuals (black line) fits completely within the blue ribbon that defines a normal distribution.

Next we create a choropleth map of the model residuals. Do the residuals show any spatial pattern of clustering? Since the values in the vector of residuals are arranged in the same order as the rows in the simple feature data frame we simply create a new column in the data frame using the `$` syntax.
```{r}
CC.sf$res <- res

tm_shape(CC.sf) +
  tm_fill("res") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Linear model residuals")
```

The map shows contiguous tracts of negative residuals across the southwestern and southern part of the city and a group of contiguous tracts of positive residuals toward the center. 

These groups indicate some clustering but it appears to be less than the clustering we saw with the crime values themselves. That is, after accounting for regional factors related to crime the autocorrelation appears to be reduced.

To determine I on the residuals we use the `lm.morantest()` function, passing the regression model object and the weights object to it. Note that we use the default neighborhoods and weighting scheme.
```{r}
nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

lm.morantest(model, wts)
```

Moran I on the model residuals is .222. This compares with the value of .5 on the value of crime alone. Part of the autocorrelation in the crime rates is statistically 'absorbed' by the explanatory factors.

But do we need a spatial regression model? The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.

## Choosing the type of spatial regression model

Ordinary regression models fit to spatial data can lead to improper inference because observations are not independent. Thus it's necessary to check the residuals from an aspatial model for autocorrelation. If the residuals are strongly correlated the model is not specified properly. 

In this case we can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), we can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n$ by 1 vector of response variable values, $X$ is a $n$ by $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n$ by 1 vector of residuals (iid).

A couple options exist if the elements of the vector $\varepsilon$ are correlated. One is to include a spatial lag term so the model becomes
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values (spatial lag variable) with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable we compute with the `lag.listw()` function and $\rho$ is Moran I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is domain specific but motivated by a diffusion process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the _spatial signal_ term and $\beta X$ is called the _trend_ term.

Another option is to include a spatial error term so the model becomes
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the variable $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$

If $z$ is not observed, then the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly we would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another motivation for considering a spatial error model is heterogeneity. Suppose we have multiple observations for each unit. If we want our model to incorporate individual effects we can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since we will have more parameters than observations.

Instead we can treat $a$ as a vector of spatial random effects. We assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

In the absence of domain-specific knowledge of the process that might be responsible for the autocorrelated residuals, we can run some statistical tests on the linear model.

The tests are performed with the `lm.LMtests()` function. The `LM` stands for 'Lagrange multiplier' indicating that the technique simultaneously determines the coefficients on the explanatory variables AND the coefficient on the spatial lag variable.

The test type is specified as a character string. The tests should be considered in a sequence starting with the standard versions and moving to the 'robust' versions if the choice remains ambiguous.

To perform LM tests we specify the model object, the weights matrix, and the two model types using the `test =` argument. The model types are specified as character strings `"LMerr"` and `"LMlag"` for the spatial error and lag models, respectively.
```{r}
lm.LMtests(model, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag models are significant ($p$-value < .15). Ideally one model is significant and the other is not, and we choose the model that is significant.

Since both are significant, we should test again. This time we use the robust forms of the statistics. We do this by using the character strings `"RLMerr"` and `"RLMlag"` in the `test =` argument.
```{r}
lm.LMtests(model, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest information criteria (AIC). Another options is to include both a spatial lag term and a spatial error term. 
