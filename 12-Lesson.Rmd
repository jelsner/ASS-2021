---
title: "Lesson 12"
author: "James B. Elsner"
date: "February 17, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The most important single aspect of software development is to be clear about what you are trying to build."** – Bjarne Stroustrup

Friday is a mid-semester wellness day. No lesson and no assignment!

## Review of autocorrelation: Population and tornadoes

Tornado reports, not tornadoes.

Is the frequency of tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring region?

To answer these questions we quantify the non-spatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable in terms of computational time we restrict the focus to the state of Iowa.

We start by getting the U.S. Census data with functions from the {tidycensus} package. The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how we get county-level population for Iowa.
```{r}
library(tidycensus)

Counties.sf <- get_acs(geography = "county", 
                       variables = "B02001_001E", 
                       state = "IA", 
                       geometry = TRUE)
```

This returns a simple feature data frame with county borders as multi-polygons. The variable `B02001_001E` is the 2015 (mid year of the 5-year period 2013-2017) population in each county.

Now get the tornado locations and compute the annual tornado occurrence rate for each county. Start by determining the intersections of the county polygons and the tornado points. The join the counts to the simple feature data frame.
```{r}
library(tidyverse)

if(!"1950-2018-torn-initpoint" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/1950-2018-torn-initpoint.zip",
              destfile = "1950-2018-torn-initpoint.zip")
unzip("1950-2018-torn-aspath.zip")
}

Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint", 
                   layer = "1950-2018-torn-initpoint") %>%
  st_transform(crs = st_crs(Counties.sf)) %>%
  filter(yr >= 2013)

TorCounts.df <- Torn.sf %>%
  st_intersection(Counties.sf) %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarize(nT = n()) 

Counties.sf <- Counties.sf %>%
  left_join(TorCounts.df,
            by = "GEOID") %>%
  mutate(nT = replace_na(nT, 0)) %>%
  mutate(Area = st_area(Counties.sf),
         rate = nT/Area/(2018 - 2013 + 1) * 10^10,
         lpop = log10(estimate))
```

Note that some counties have no tornadoes and the `left_join()` returns a value of `NA` for those. We use `mutate()` with `replace_na()` to turn those counts to a value of 0.

Make a two-panel map displaying the log of the population and the tornado rates.
```{r}
map1 <- tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "lpop",
          title = "Log Population",
          palette = "Blues") +
  tm_layout(legend.outside = "TRUE")

map2 <- tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "rate",
          title = "Annual Rate\n[/10,000 sq. km]",
          palette = "Greens") +
  tm_layout(legend.outside = "TRUE")

tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function.
```{r}
lpop <- Counties.sf$lpop
rate <- as.numeric(Counties.sf$rate)

cor.test(lpop, rate)
```

The bi-variate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bi-variate spatial autocorrelation is done using a Monte Carlo simulation.
```{r}
nbs <- poly2nb(Counties.sf)
wts <- nb2listw(nbs)

lee_stat <- lee(lpop, rate, 
                listw = wts, 
                n = length(nbs))
lee_stat$L

lee.mc(lpop, rate, listw = wts, nsim = 9999)
```

Finally we map out the local variation in the bi-variate spatial autocorrelation.
```{r}
Counties.sf$localL <- lee_stat$localL

tm_shape(Counties.sf) +
  tm_fill("localL",
          title = "Local Bivariate\nSpatial Autocorrelation") +
  tm_borders(col = "gray70") +
  tm_layout(legend.outside = TRUE)
```

What might cause this? Compare with Kansas.

Also, compare local Lee with local Moran.
```{r}
Ii_stats <- localmoran(rate, 
                       listw = wts)
Counties.sf$localI = Ii_stats[, 1]

tm_shape(Counties.sf) +
  tm_borders(col = "gray70") +
  tm_fill(col = "localI",
          title = "Local Autocorrelation",
          palette = "Purples") +
  tm_layout(legend.outside = "TRUE")
```

## Constraining group membership based on autocorrelation

From: https://www.dshkol.com/post/spatially-constrained-clustering-and-regionalization/

The advantage of spatially constrained clustering methods is that it has a hard requirement that spatial objects in the same cluster are also geographically linked. 

There are a lot of cases that require separating geographies into discrete but contiguous regions (regionalization) such as designing communities, planning areas, amenity zones, logistical units, or even for the purpose of setting up experiments with real world geographic constraints. 

There are many situations where the optimal grouping, if solely using traditional cluster evaluation measures, is sub-optimal in practice because of real-world constraints.

Unconstrained grouping on data with spatial characteristics will result in contiguous regions because of autocorrelation, but if we want to ensure that all objects are in entirely spatially-contiguous groups we need a method specifically designed for the task. The 'skater' algorithm available in R via the {spdep} package is well-implemented and well-documented.

The 'skater' (spatial ’k’luster analysis by tree edge removal) builds a connectivity graph to represent spatial relationships between neighboring areas, where each area is represented by a node and edges represent connections between areas (see https://www.tandfonline.com/doi/abs/10.1080/13658810600665111). Edge costs are calculated by evaluating the dissimilarity between neighboring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates.

Consider crime data at the tract level in the city of Columbus, Ohio (Anselin, 1988: Spatial Econometrics. Boston, Kluwer Academic). The tract polygons are projected with arbitrary spatial coordinates.
```{r}
if(!"columbus" %in% list.files()) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = "columbus.zip")
unzip("columbus.zip")
}

( CC.sf <- read_sf(dsn = "columbus") )
```

We check if there is a clear spatial pattern to this data.
```{r}
plot(CC.sf[,7:9])
```

Promising start as there looks to be some fairly distinct regional patterns happening. 

Next, scale variable values and center them. This is done regardless of clustering approach.
```{r}
( CCs.df <- CC.sf %>% 
    mutate(HOVAL = scale(HOVAL),
           INC = scale(INC),
           CRIME = scale(CRIME)) %>%
    select(HOVAL, INC, CRIME) %>%
    st_drop_geometry() )
```

Next create the adjacency neighbor structure using rook contiguity.
```{r}
nbs <- poly2nb(CC.sf, 
               queen = TRUE)

plot(CC.sf$geometry)
plot(nbs, 
     st_centroid(st_geometry(CC.sf)),
     add = TRUE)
```

Next we combine the contiguity graph with our scaled attribute data to calculate edge costs based on the statistical distance between each node. The function `nbcosts()` from the {spdep} package provides distance methods for Euclidian, Manhattan, Canberra, binary, Minkowski, and Mahalanobis, and defaults to Euclidean if not specified.
```{r}
costs <- nbcosts(nbs, 
                 data = CC.sf)
```

Next we transform the edge costs into spatial weights using the `nb2list2()` function before constructing the minimum spanning tree with the weights list.
```{r}
wts <- nb2listw(nbs,
                glist = costs,
                style = "B")
mst <- mstree(wts)

plot(mst, 
     coordinates(as_Spatial(CC.sf)), 
     col = "blue")
```

Edges with higher dissimilarity are removed sequentially until left with a spanning tree that takes the minimum sum of dissimilarities across all edges of the tree, hence minimum spanning tree. At this point, any further reduction in edges would create disconnected sub-graphs which then lead to the resulting spatial clusters.

Once the minimum spanning tree is in place, the SKATER algorithm comes in to partition the MST. It partitions the graph identifying which edge to remove to maximize the quality of resulting clusters as measured by the sum of the inter-cluster square deviations SSD. Regions that are similar to one another have lower values. This is implemented with the `skater` function and the `ncuts =` argument indicates the number of partitions to make, resulting in ncuts + 1 groups.
```{r}
clus5 <- skater(edges = mst[,1:2], 
                data = CCs.df, 
                ncuts = 4)
```

Where are these groups located?
```{r}
CC.sf <- CC.sf %>%
  mutate(Group = clus5$groups)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(Group)))
```

Hierarchical clustering
```{r}
dd <- dist(CCs.df)
hc <- hclust(dd, 
             method = "ward.D")
hcGroup <- cutree(hc, k = 5)
CC.sf <- CC.sf %>%
  mutate(hcGroup = hcGroup)

ggplot() +
  geom_sf(data = CC.sf,
          mapping = aes(fill = factor(hcGroup)))

```

## Autocorrelation in model residuals

Knowledge of significant autocorrelation by itself is not typically useful. But knowledge of significant autocorrelation in the residuals from some statistical model can help us build a more precise model.

A spatial regression model may be needed whenever the residuals resulting from a aspatial regression model exhibit significant spatial autocorrelation. So a common way to proceed is to first regress the response variable onto the explanatory variables and check for spatial autocorrelation in the residuals.

If the explanatory variables remove the spatial autocorrelation then a spatial regression model is not needed.

Let's continue with the Columbus crime data and fit a linear regression model with `CRIME` as the response variable and `INC` and `HOVAL` as the explanatory variables.
```{r}
model <- lm(CRIME ~ INC + HOVAL, 
            data = CC.sf)
summary(model)
```

The model statistically explains 55% of the variation in crime. As income and housing values increase crime goes down. 

We use the `residuals()` method to extract the vector of residuals from the model.
```{r}
res <- residuals(model)
```

We then check on the distribution of the residuals relative to a normal distribution.
```{r}
sm::sm.density(res, 
               model = "Normal")
```

The next step is to create a choropleth map of the model residuals. Are the residuals clustered?
```{r}
CC.sf$res <- res

library(tmap)
tm_shape(CC.sf) +
  tm_fill("res") +
  tm_borders(col = "gray70") +
  tm_layout(title = "Linear Model Residuals")
```

Yes. There are clustered regions where the model over predicts crime conditional on household income and housing values and where it under predicts crime.

The amount of clustering is less than before. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced.

To determine I on the residuals we use the `lm.morantest()` function and pass the regression model object and the weights object to it.
```{r}
nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)
lm.morantest(model, wts)
```

Moran I on the model residuals is .222.  This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors.

Do we need a spatial regression model?  The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.

## Choosing a spatial regression model

Ordinary regression models fit to spatially aggregated data can lead to improper inference because observations are not independent. Thus it's necessary to check the residuals from an aspatial model for spatial autocorrelation. If the residuals are strongly correlated the model is mis-specified. 

In this case we can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), we can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n \times 1$ vector of response variable values, $X$ is a $n$ $\times$ $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n \times 1$ vector of residuals (iid).

Two options exist if the elements of the vector $\varepsilon$ are spatially correlated. The first is to rewrite the model adding a spatial lag term as
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable we compute with the `lag.listw()` function and $\rho$ is Moran I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is motivated by a diffusion process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the spatial signal term and $\beta X$ is called the trend term.

The second option is to rewrite the model by adding a spatial error term as
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$
If $z$ is not observed, the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly we would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another reason for fitting a spatial error model is spatial heterogeneity. Suppose we have multiple observations for each unit. If we want our model to incorporate individual effects we can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since we will have more parameters than observations.

Instead we can treat $a$ as a vector of spatial random effects. We assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

To help decide between the two spatial regression models described above we run a sequence of statistical tests on our linear model object. The tests are the Lagrange multiplier (LM) tests. LM refers to a technique to determine the coefficients on the explanatory variables while simultaneously determining the coefficient on the spatial regression term.

The tests are performed with the `lm.LMtests()` function. The test type is specified as a character string. The tests should be considered in order. Start with the standard version of both the spatial error model and spatial lag model LM tests.

For example, to perform a LM test for the spatial error and spatial lag model on the Columbus crime model we type
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag model terms are significant ($p$-value < .15). Ideally one term is significant and the other is not and we choose the model with the significant term.

Since both are significant we should test again. This time we use the robust forms of the statistics.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest AIC.