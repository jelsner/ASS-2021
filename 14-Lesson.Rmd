---
title: "Lesson 14"
author: "James B. Elsner"
date: "February 24, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

## Fitting a spatial regression model

Let's see how to choose between these two options with an example. Returning to the Columbus crime data, we import the data, fit a linear regression model to statistically explain crime rates using income and housing values. Further we first need to check whether a spatial regression model is warranted by testing whether there is significant spatial autocorrelation in the residuals of the aspatial model.
```{r}
if(!"columbus" %in% list.files()){
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")
}

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")
model.lm <- lm(CRIME ~ INC + HOVAL, 
               data = CC.sf)
CC.sf$residuals <- residuals(model.lm)

nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm, wts)
```

The answer is 'yes' a spatial regression model is warranted.

To help decide between the two spatial regression models described above we run a sequence of statistical tests on our linear model object. The tests are the Lagrange multiplier (LM) tests. LM refers to a technique to determine the coefficients on the explanatory variables while simultaneously determining the coefficient on the spatial regression term. It does this iteratively.

The tests are performed with the `lm.LMtests()` function. The test type is specified as a character string. The tests should be considered in order. Start with the standard version of both the spatial error model and spatial lag model LM tests.

For example, to perform a LM test for the spatial error and spatial lag model on the Columbus crime model we type
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag model terms are significant ($p$-value < .15). Ideally one term is significant and the other is not and we choose the model with the significant term.

Since both are significant we should test again. This time we use the robust forms of the statistics.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest AIC.

To fit a spatial lag model to the crime data we use the `lagsarlm()` function. The value for $\rho$ is found first using the `optimize()` function and then the $\beta$'s are obtained using generalized least squares. These functions are now in the {spatialreg} package.
```{r}
if(!require(spatialreg)) install.packages(pkgs = "spatialreg", repos = "http://cran.us.r-project.org")

model.lag <- spatialreg::lagsarlm(CRIME ~ INC + HOVAL, 
                      data = CC.sf, 
                      listw = wts)

summary(model.lag)
```

Let's break down the output starting with the coefficients.

#### Interpreting the model coefficients

The first batch of output concerns the model residuals and the coefficients on the explanatory variables. The model residuals are the observed crime rates minus the predicted crime rates.

The coefficients on income and housing have the same sign (negative) and they remain statistically significant.

The spatial lag model allows for 'spillover'. That is a change in an explantory variable anywhere in the study domain will affect the value of the response variable everywhere. Spillover occurs even when the neighborhood weights matrix represents simple (1st-order) local contiguity. The spillover makes interpreting the marginal effects more complicated.

The spatial lag model implies that for a change in the value of an explanatory variable there are direct and indirect effects on the response variable.

For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the $i$th tract's income on crime across neighboring tracts.

The indirect effect gives the impact of a change in income has on crime averaged over all OTHER tracts. The indirect effect represent spillovers. The influences on the dependent variable $y$ in a region rendered by change in $x$ in some other region. For example, if all tracts $i \ne j$ (i not equal to j) increase their income, what will be the impact on crime in region $i$?

The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract $j$ increasing its income over all other tracts (on average). It is given by
$$
\hbox{TE} = \left(\frac{\beta_k}{1-\rho^2}\right)\left(1 + \rho\right)
$$
where $\beta_k$ is the marginal effect of variable $k$ and $\rho$ is the spatial autocorrelation. With $\rho = 0$ TE is $\beta_k$.

Here $\beta_{INC}$ is -1.0487 so the total effect is
```{r}
( TE_INC <- -1.0487 / (1 - .4233^2) * (1 + .4233) )
```
where .4233 is the value for $\rho$.

The direct, indirect, and total effects are shown using the `impacts()` function from the {spatialreg} package.
```{r}
spatialreg::impacts(model.lag, 
                    listw = wts)
```

The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region.

The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3.

See https://youtu.be/b3HtV2Mhmvk
lmSLX() function

#### Interpreting the coefficient rho

The next set of output is about the estimate of spatial autocorrelation ($\rho$).  The value is .4233 and a likelihood ratio test gives a value of 9.41 which translates to a $p$-value of .002.  The null hypothesis is the autocorrelation is zero, so we confidently reject it. This is consistent with the significant Moran I value that we found in the linear model residuals.

Two other tests are performed on the value of $\rho$ including a z-test (t-test) using the asymptotic standard error and a Wald test. Both tests confirm that the lag term should be included in the model.

Overall model fit

The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model.

The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.lm))/49
x[1]
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

Residual spatial autocorrelation

The final bit of output is a Lagrange multiplier test for residual autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. We find a high $p$-value so you are satisfied that the lag term takes care of the autocorrelation.

Compare with a spatial error model. Here we use the `errorsarlm()` function.
```{r}
model.error <- errorsarlm(CRIME ~ INC + HOVAL, 
                          data = CC.sf, 
                          listw = wts)
summary(model.error)
```

Here we find the spatial autoregression ($\lambda$) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the LM tests indicating the spatial lag model is more appropriate.

We compare the log likelihoods from the two spatial regression models and find that the lag model is a good improvement over the error model. This result is consistent with our above decision to use the lag model.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.error))/49
x[1]
```

Predictions

The `predict()` method implements the `predict.sarlm()` function to calculate predictions from the spatial regression model. The prediction is decomposed into a "trend" term (explanatory variable effect) and a "signal" term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model.

We make predictions with the `predict()` method under the assumption that the mean response is known. We examine the structure of the corresponding predict object.
```{r}
pre <- predict(model.lag)
str(pre)

CC.sf$CRIME[1:5]
```

The trend term is $X\beta$ and the signal term is $\rho W y$.

The predictions are added to the simple features data frame.
```{r}
CC.sf$fit <- as.numeric(pre)
CC.sf$trend <- attr(pre, "trend")
CC.sf$signal <- attr(pre, "signal")
```

The components of the predictions are mapped and placed on the same page.
```{r}
( g3 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = fit)) +
    scale_fill_gradient(low = "white", high = "green") +
    ggtitle("Predicted Crime") )

( g4 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = trend)) +
    scale_fill_gradient(low = "white", high = "orange") +
    ggtitle("Trend (Covariate)") )

( g5 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = signal)) +
    scale_fill_gradient(low = "white", high = "blue") +
    ggtitle("Signal") )

gridExtra::grid.arrange(g3, g4, g5, nrow = 3)
```

The values in the map at the top are the sum of the values in the two maps below. The spatial smoother (signal) is about the same size as the explanatory variable effect especially for the most centrally located tracts.

Compare model residuals. How many tracts have a smaller residual when using the lag model versus the aspatial model?
```{r}
CC.sf <- CC.sf %>%
  mutate(residualsL = CRIME - fit,
         lagWins = abs(residuals) > abs(residualsL))

sum(CC.sf$lagWins)
```

In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the aspatial model.

Key references

* Haining, R. 1990 Spatial data analysis in the social and environmental sciences, Cambridge: Cambridge University Press, p. 258.
* Cressie, N. A. C. 1993 Statistics for spatial data, Wiley, New York.
* Michel Goulard, Thibault Laurent & Christine Thomas-Agnan, 2017 About predictions in spatial autoregressive models: optimal and almost optimal strategies, Spatial Economic Analysis Volume 12, Issue 2–3, 304–325 https://doi.org/10.1080/17421772.2017.1300679.
* Kelejian, H. H. and Prucha, I. R. 2007 The relative efficiencies of various predictors in spatial econometric models containing spatial lags, Regional Science and Urban Economics, Volume 37, Issue 3, 363–374.
* Bivand, R. 2002 Spatial econometrics functions in R: Classes and methods, Journal of Geographical Systems, Volume 4, No. 4, 405–421

Finally, Stata is the dominant software package for economists needing to fit spatial regression models. Outside of Economics, however, R is widely used and more versatile. This cheatsheet may help make the transition. https://www.r-bloggers.com/stata-to-r-cheatsheet-for-econometrics/

### Example: California house prices

The file `CHP.zip` contains information about housing values at the census tract level across the state of California.
```{r}
library(sf)

download.file("http://myweb.fsu.edu/jelsner/temp/data/CHP.zip",
              "temporary.zip")
unzip("temporary.zip")

CHP.sf <- read_sf(dsn = "CHP")
```

The object `CHP.sf` is a simple feature spatial data frame with 29 variables whose values are aggregated to census tracts. There are 7049 tracts. To see the spatial extent of the data and the tract boundaries we use `tm_borders()`.
```{r}
library(tmap)

tm_shape(CHP.sf) +
  tm_borders()
```

The warning message tells us that at least one of the 7049 geometries is invalid. 

Invalid geometries include null or empty geometries or geometries that are not simple (e.g., intersecting polygons). The `st_is_valid()` function returns a logical vector that lets us determine the number of invalid geometries.
```{r}
sum(!st_is_valid(CHP.sf))
```

To see the reason we include the argument `reason = TRUE`.
```{r}
x <- st_is_valid(CHP.sf, reason = TRUE)
x[!st_is_valid(CHP.sf)]
```

This tells us that two geometries containing self-intersecting polygons.

To fix this we can create a new set of geometries with the `st_buffer()` function while setting the buffer distance to zero.
```{r}
library(dplyr)

CHP.sf <- CHP.sf %>%
  st_buffer(dist = 0)

sum(!st_is_valid(CHP.sf))
```

Aggregate the attributes to the county level

We will analyze these data at the county level. In Lesson 6 we saw how to aggregate variables in a spatial data frame using functions in the **areal** package. This allows for greater flexibility since the aggregating units do not need to be in alignment.

Since the California tracts are completely contained within county boundaries (aligned) here we use the `aggregate()` function from the **raster** package.

We have county names (`County`) as a character variable so we can aggregate by name after converting the simple feature to an S4 class spatial data frame.
```{r}
CHP.sp <- as(CHP.sf, "Spatial")

CHPcounty.sp <- raster::aggregate(CHP.sp, 
                                  by = "County")
```

This produces a spatial polygons data frame where the boundaries are counties but the attribute table contains only the county name. 

To see this let's convert the S4 spatial data frames to simple features and create a map.
```{r}
CHPcounty.sf <- as(CHPcounty.sp, "sf")

library(tmap)

tmap_mode("view")
tm_shape(CHPcounty.sf) +
  tm_borders()

tmap_mode("plot")
```

Zoom and note that San Francisco County is close to Marin County (to the north) but not by contiguity.

We need to aggregate the variables to the county level. Although it is possible to do everything in one step with `aggregate()` it is best to be careful because how we want a variable aggregated depends on its type (extensive or intensive).

The simplest case is where we can sum the values (extensive variables like the number of houses). First create a simple feature data frame and select a subset of the columns.
```{r}
CHP1.sf <- CHP.sf %>%
    dplyr::select(nhousingUn, recHouses, nMobileHom, Population,
                  Males, Females, Under5, White, Black,
                  AmericanIn, Asian, Hispanic, PopInHouse,
                  nHousehold, Families)
```

Create a list of the county names by tract.
```{r}
CountyList <- list(County = CHP.sf$County)
CountyList[[1]][1:50]
```

Then aggregate by county name (`County`) with the `aggregate()` method applied to the simple feature data frame. We employ the base R `sum()` function through the argument `FUN = sum` and specify what to do with missing values (encountered by in the summation) with the  `na.rm = TRUE` argument.
```{r}
CHP1.sf <- aggregate(CHP1.sf, 
                     by = CountyList,
                     FUN = sum, 
                     na.rm = TRUE)
head(CHP1.sf)

tm_shape(CHP1.sf) +
  tm_fill(col = "nMobileHom") +
  tm_borders()
```

The result is a spatial data frame containing county-level totals aggregated from the original tract-level data.

In other cases like with home values we need to use a weighted average to take into account the fact that housing values may be high in a small tract in a county but they may be much lower in a larger tract in the same county (intensive variable). So here we create average values per household in three steps. 

First select the variables of interest.
```{r}
CHP2.sf <- CHP.sf %>%
  dplyr::select(houseValue, yearBuilt, nRooms,
                nBedrooms, medHHinc, MedianAge,
                householdS, familySize, nHousehold)
```

Second multiply these variables by the number of households (`nHousehold`) in each tract.
```{r}
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue * nHousehold,
         yearBuilt = yearBuilt * nHousehold,
         nRooms = nRooms * nHousehold,
         nBedrooms = nBedrooms * nHousehold,
         MedianAge = MedianAge * nHousehold)
```

Third aggregate to get county totals before dividing by the number of households.
```{r}
CHP2.sf <- aggregate(CHP2.sf, 
                     by = CountyList, 
                     FUN = sum, 
                     na.rm = TRUE)
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue / nHousehold,
         yearBuilt = yearBuilt / nHousehold,
         nRooms = nRooms / nHousehold,
         nBedrooms = nBedrooms / nHousehold,
         medHHinc = medHHinc / nHousehold,
         MedianAge = MedianAge / nHousehold,
         householdS = householdS / nHousehold,
         familySize = familySize / nHousehold)

head(CHP2.sf)
```

Map the county level house values together with the number of bedrooms.
```{r}
tm_shape(CHP2.sf) +
  tm_fill(col = c("houseValue", "nBedrooms")) +
  tm_layout(legend.position = c("right", "top")) +
  tm_borders()
```

Fit a nonspatial regression model to predict house values

We examine the residuals from an aspatial regression of house price on house age and the number of bedrooms. Here we first assign the model formula to the object `mf` and then use the `lm()` function.
```{r}
mf <- houseValue ~ yearBuilt + nBedrooms
model.lm <- lm(mf, data = CHP2.sf)
summary(model.lm)
```

According to the model, `yearBuilt` is highly significant. Older houses are more valuable. House values increase by 13K dollars for every additional year of existence. The number of bedrooms is also marginally significant. Every bedroom adds about 192K dollars to the value of a house.

What is the expected (predicted from the model) value of a house built in 1999 with four bedrooms?
```{r}
predict(model.lm, 
        newdata = data.frame(yearBuilt = 1999, 
                             nBedrooms = 4))
```

What is the expected value of a house in San Francisco? First, what row contains data from San Francisco County?
```{r}
which(CHP2.sf$County == "San Francisco")

predict(model.lm)[38]
```

How does this compare with the county average?
```{r}
CHP2.sf$houseValue[38]
```

The model under predicts house values in San Francisco by over $100K.

Next, examine the model residuals for spatial autocorrelation. We first compute the neighborhood topology based on contiguity. But then add two links: between San Francisco and Marin County and vice versa (to consider the fact that the Golden Gate bridge connects these two counties in a way that would likely influence housing prices). 

This is done by knowing the row numbers of these two counties and adding these numbers to the neighborhood list object. Note that since county 21 is a neighbor of county 38, county 38 must be a neighbor of county 21 so we need to adjust both.
```{r}
library(spdep)

nbs <- poly2nb(CHP2.sf)

CHP2.sf$County[21]

nbs[[21]] <- sort(as.integer(c(nbs[[21]], 38)))
nbs[[38]] <- sort(as.integer(c(21, nbs[[38]])))
```

Or we can use nearest neighbors. Start by extracting the coordinates of each county using the `st_coordinates()` function after applying the `st_centroid()` function. We use the `knearneigh()` function to compute the set of k nearest neighbors for each center. Since the CRS is geographic we include the `longlat = TRUE` argument and then  convert the k nearest neighbor object to a neighborhood object with the `knn2nb()` function.
```{r}
coords <- st_coordinates(st_centroid(CHP2.sf))

nearestNeighbors <- knearneigh(coords, 
                               k = 5,
                               longlat = TRUE)
knb <- knn2nb(nearestNeighbors)
summary(knb)
```

Create the spatial weights matrix and check the residuals from the linear model for spatial autocorrelation.
```{r}
wts <- nb2listw(nbs)
kwt <- nb2listw(knb)
lm.morantest(model.lm, wts)
lm.morantest(model.lm, kwt)
```

The large amount of spatial autocorrelation indicates that a spatial regression model is warranted. The model coefficients are not reliably precise.

Fit a spatial regression model using the same explanatory variables

As we saw in Lesson 11, we first need to choose between a lag and an error model. We do this sequentially first using the Lagrange multiplier test and then, if needed, the robust version of the LM test.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
lm.LMtests(model.lm,
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

The results from the LM tests do not help us discern between and a lag and an error model. However, the results from the RLM tests favor of a lag model. Recall the lag model includes a spatial lag term on the response variable.

This same decision is reached when using the nearest neighbor weights (`kwt`).

Fit the model. Here we need to specify a lower tolerance for numerically solving the inverse of the covariance matrix. This is needed in this case because the explanatory variables have very different scales. Actually it might be better to scale the variables first.
```{r}
#library(spatialreg)

model.lag <- lagsarlm(mf, 
                      data = CHP2.sf, 
                      listw = wts,
                      tol.solve = 1e-30)
summary(model.lag)
```

The coefficients on year the house was built and on the number of bedrooms have the same sign and remain statistically significant but the values are considerably different.

The value of $\rho$ (coefficient on the spatial lag term) is .774, which is large and significant as indicated by the likelihood ratio test value and its corresponding $p$-value. The z-value and Wald statistic similarly indicate a significant coefficient on the spatial lag term.

The difference in log likelihoods between the spatial and aspatial regression models indicates a large to huge improvement of the spatial model over the aspatial model.
```{r}
2 * (logLik(model.lag) - logLik(model.lm))/58
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

The AIC for the spatial lag model is lower than that for the linear model and there is no significant residual autocorrelation.

What is the expected value of a house in San Francisco predicted using the spatial lag model.
```{r}
predict(model.lag)[38]
```

The spatial lag model better captures the aggregated housing values in San Francisco than does the aspatial model.

The `trend` term is the collective influence of the two explanatory variables and the `signal` is the spatial smoothing.
```{r}
pre <- predict(model.lag)
str(pre)
CHP2.sf$houseValue[1:5]
```

Maps of the spatial signal and the trend terms are made separately.
```{r}
CHP2.sf$signal <- attr(pre, "signal")
CHP2.sf$trend <- attr(pre, "trend")

tm_shape(CHP2.sf) +
  tm_fill("signal")
```

The signal term 'explains' (statistically) the spatial influence of San Francisco on housing values across the state. Highest values are centered on San Francisco County and these high values decay northward and eastward quickly. The decay in the influence of San Francisco is more gradual to the south.

```{r}
tm_shape(CHP2.sf) +
  tm_fill("trend")

CHP2.sf$preLAG <- CHP2.sf$signal + CHP2.sf$trend

tm_shape(CHP2.sf) +
  tm_fill("preLAG", midpoint = NA)
```

The trend term shows that, beyond the spatial smoothed term, additional factors (year built and number of bedroom) make house values in San Francisco high.