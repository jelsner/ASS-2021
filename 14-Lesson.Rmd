---
title: "Lesson 14"
author: "James B. Elsner"
date: "February 24, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"We build our computer systems the way we build or cities; over time, without plan, on top of ruins."** â€“ Ellen Ullman

## Geographic regression

If the residuals from a statistical model have significant autocorrelation then a spatial modeling approach is called for. One approach is to assume that the relationships between the response variable and the explanatory variables are modified by contextual factors. 

Like with local variants of spatial autocorrelation metrics, which use only neighbors for estimates, we can fit separate regression models for each polygon using only values in neighborhoods. This approach is useful for exploratory analysis (e.g., to show where the explanatory variables are most strongly related to the response variable). This approach is called geographically weighted regression (GWR) or geographic regression.

GWR fits a separate regression model for every location in the dataset. Thus it is not a single model but rather a procedure for fitting a set of models. It fits the models by using the response and explanatory variables only from locations that fall within some prescribed distance (bandwidth). The bandwidth is pre-specified or determined by a cross-validation procedure. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies or health programs.

Let's see how GWR works with an example.

The file `south.zip` contains shapefiles with homicide rates and explanatory variables for counties in the southern United States. Download the file from my website and unzip it in your working directory. 

Import the data using the `read_sf()` from the {sf} package. The data have latitude/longitude coordinates but there is no projection so we set the CRS to long-lat with the `st_crs()` function.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/south.zip",
              destfile = "south.zip")
unzip("south.zip")

library(sf)

SH.sf <- st_read(dsn = "south", 
                 layer = "south", 
                 stringsAsFactors = FALSE)
st_crs(SH.sf) <- 4326
names(SH.sf)
```

Each row is a separate U.S. county in the southeast. There are 1412 counties.

We are interested in predicting homicide rates (`HR`) given as the number of homicides per 100,000 people. And we consider five explanatory variables including `RD`: resource deprivation index, `PS`: population structure index, `MA`: marriage age, `DV`: divorce rate, and `UE`: unemployment rate. The two digit number appended to the column names is the census year from the 20th century.

First use the `plot()` method on the `geometry` column to see the extent of the data and the spatial geometries.
```{r}
plot(SH.sf$geometry, col = "gray70")
```

Next we reduce the number of variables in the data frame keeping only the ones of interest using the `select()` function from {dplyr}.
```{r}
library(dplyr)

SH.sf <- SH.sf %>%
  dplyr::select(HR90, RD90, PS90, MA90, DV90, UE90)
```

We create a thematic map of the homicide rates from the 1990 census (`HR90`).
```{r}
library(tmap)

tm_shape(SH.sf) +
  tm_fill("HR90", title = "1990\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

We start with a linear regression model where we regress homicide rate onto resource deprivation, population structure, marriage age, divorce rate, and unemployment rate.
```{r}
model.lm <- lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, 
               data = SH.sf)
summary(model.lm)
```

We see that RD, PS, and DV have a positive relationship to HR while MA and UE have a negative relationship. 

Based on the $p$-values on the coefficients we suspect the model can be simplified by removing marriage age (MA). We check this supposition with the `drop1()` function.
```{r}
drop1(model.lm)
```

The single term delection table shows that when marriage age (`MA90`) is removed from the model the RSS (residual sum of squares) value increases by 35.2 units. This increase is not sufficient to justify the loss in the degrees of freedom by keeping it in the model. This is seen by an AIC value that is lower (4998.7) than the AIC when all terms are retained (4999.7) (see the row labeled `<none>`). 

Recall: The AIC is a way to balance the tradeoff between bias and variance. Choose a model that has the lowest AIC. A model may have too much bias (toward the particular dataset) if it has too many parameters and a model may have too much residual variance if there are too few parameters.

We therefore remove marriage age and refit the mode.
```{r}
model.lm2 <- lm(HR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = SH.sf)
```

The new model (`model.lm2`) can't be simplified further using this criteria.
```{r}
drop1(model.lm2)
```

All the AIC values below the first row are above those in the first row.

Next we map the predicted values after adding them to the simple features data frame. The predicted values from the model object are extracted with the `predict()` method.
```{r}
SH.sf$predLM2 <- predict(model.lm2)
head(cbind(SH.sf$HR90, SH.sf$predLM))
```

The first column printed to the console is the actual homicide rates in the first six counties and the second column printed is the predicted homicide rate from the linear regression model. The predictions do not appear to be very good.

A scatter plot of the observed versus the predicted shows this clearly.
```{r}
library(ggplot2)

ggplot(SH.sf, aes(x = HR90, y = predLM2)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

Since the homicide rates are non-negative, transforming them to logarithms is a good idea.

We create a new column in the `homicides.sf` data from called `logHR90`. Since there are some counties with no homicides we change that to the minimum observed value before taking logarithms. Here we first create a logical vector `x` corresponding to the rows with non-zero homicide rates. We then find the minimum non-zero rate and assign it to `e`. Next we subset on this value for all rates equal to zero and finally we create a new column as the logarithm of the non-zero rates.
```{r}
x <- SH.sf$HR90 != 0
e <- min(SH.sf$HR90[x])
SH.sf$HR90[!x] <- e
SH.sf$logHR90 <- log(SH.sf$HR90)
```

We then fit a model with `logHR90` as our response variable.
```{r}
model.lm3 <- lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                data = SH.sf)
summary(model.lm3)
```

We again compute the predicted values and include them in the data frame as `predLM3`. The predictions are on the natural logarithm scale so we use the exponential function `exp()`. We then create a scatter plot of the observed versus predicted as before.
```{r}
SH.sf$predLM3 <- exp(predict(model.lm3))

ggplot(SH.sf, aes(x = HR90, y = predLM3)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

The range of predicted values is much better.

It is likely that homicide rates are similiar in neighboring counties. It also might be the case that the similarity is statistically explained by the  variables in the model.

So our next step it to test for significant autocorrelation in the model residuals. We create a weights matrix using the functions from the {spdep} package and then use the `lm.morantest()` function.
```{r}
library(spdep)

nbs <- poly2nb(SH.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm3, wts)
```

Moran I is .11 but significant ($p$-value < .001).

Put the residuals on a map. First add the residuals as a column in the simple feature data frame.
```{r}
SH.sf$res3 <- residuals(model.lm3)

tm_shape(SH.sf) +
  tm_fill("res3", title = "Model\nResiduals") +
  tm_layout(legend.outside = TRUE)
```

There are small clusters of counties with positive residuals and other small clusters of negative residuals. Interestingly the pattern of these clusters appears to be different over western and northern areas compared to the deep South.

This suggests that the _relationships_ between homicide rates and the socioeconomic factors might vary across the domain. GWR is a procedure to fit local regression models.

Linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variable(s). Geographic regression might show how this dependency varies by location. It is an exploratory technique intended to indicate where local regression coefficients are different from the global values.

A model is fit at each location. All observations contribute to the fit but they are weighted inversely by their distance to the location. At the shortest distances observations are given the largest weights based on a Gaussian function. The process results in a set of regression coefficients for each observation.

We do this with functions from the {spgwr} package. The geometry information in simple feature data frames is not accessible by functions in this package so we need to create a new S4 spatial data frame.
```{r}
SH.sp <- as(SH.sf, "Spatial")
```

The spatial information in the `SH.sp` is separated from the data frame (attribute table) but accessible by the functions `gwr.sel()` and `gwr()`. The variables remain the same.

We obtain the optimal bandwidth with the `gwr.sel()` function specifying the model and the data object. Since the CRS is geographic we use the argument `longlat = TRUE` to get the distances in kilometers.
```{r}
if(!require(spgwr)) install.packages(pkgs = "spgwr", repos = "http://cran.us.r-project.org")

library(spgwr)

bw <- gwr.sel(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
              data = SH.sp,
              longlat = TRUE)
```

The automatic selection procedure makes an initial guess at the bandwidth distance then fits local regression models in each county using neighbors defined by that distance. A cross-validated (CV) skill score is computed as the root mean square prediction error. The cross-validation procedures successively removes one county from the modeling and that county's homicide rate is predicted. Each county takes turn getting removed.

The selection procedure continues by changing the initial guess at the bandwidth and computing the CV score. If the CV score is higher than wih the initial guess the bandwidth is changed in the other direction. If it is lower than the bandwidth is changed in the same direction. The entire procedure continues until no additional improvement is made to the CV score. This results in a minimum bandwith distance. In this case it is 165.5 km.

The bandwidth is assigned to the object `bw` as a single value.

Unfortunately we cannot use neighborhoods defined by contiquity. But to get a sense of what this bandwidth distance means in terms of the average number of neighbors per county we note that one-half the distance squared times pi is the area captured by the bandwidth.
```{r}
( bwA <- pi * (bw * 1000 /2)^2 ) 
```

In units of square meters. Or 21,519 square kilomenters.

County areas are computed using the `st_area()` function. The average size of the countys and the ratio of the bandwidth area to the average county area is also computed.
```{r}
areas <- st_area(SH.sf)
ctyA <- mean(areas)
bwA/ctyA
```

The ratio indicates that, on average, a neighborhood consists of 13 counties. For comparison, on a raster there are 8 first-order neighboring cells (queen contiguity) and 16 second-order neighboring cells (neighbors of neighbors) or a total of 24 neighbors.

We then use the `gwr()` function to includes the formula, data, and the `bandwith =` argument.
```{r}
model.gwr <- gwr(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                 data = SH.sp, 
                 bandwidth = bw)
```

The model and observed data are assigned to list object with element names extracted with the `names()` function.
```{r}
names(model.gwr)
```

The first element of the list named `SDF` contains the model output as a S4 spatial class data frame. The geometry of the spatial data frame is inherited from the type of data frame specified in the `data = ` argument.

The structure of the S4 spatial class is obtained with the `str()` function and by setting the `max.level` argument to 2.
```{r}
str(model.gwr$SDF, max.level = 2)
```

Here we see there are 5 slots with the first slot being the attribute table labeled `@data`. The dimension of the attribute table is retrieved with the `dim()` function.
```{r}
dim(model.gwr$SDF)
```

There are 1412 rows and 9 columns. Each row corresponds to a county and information about the regression localized to the county is given in the columns. The attribute names are extracted with the `names()` function.
```{r}
names(model.gwr$SDF)
```

They include the sum of the weights `sum.w` (the larger the sum the more often the county was included in the local regressions--favoring smaller counties and ones farther from the borders of the spatial domain), the five regression coefficients (one for each of the 4 explanatory variables and an intercept term), the residual (`gwr.e`), the predicted value (`pred`) and the local goodness-of-fit (`localR2`).

We put the predictions into the `SH.sf` simple feature data frame with the column name `predGWR`.
```{r}
SH.sf$predGWR <- exp(model.gwr$SDF$pred)

tm_shape(SH.sf) +
  tm_fill("predGWR", title = "Predicted\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

The geographic regressions similarly capture the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the linear model. The pattern is also a smoother.

With many more model parameters metrics of predictive skill will favor the geographic regression. For example, the root mean-square-error is lower for GWR.
```{r}
sqrt(sum(residuals(model.lm3)^2))
sqrt(sum(model.gwr$SDF$gwr.e^2))
```

Geographic regression is valuable for generating hypothesis. From the linear model we saw that homicide rates increased with resource deprivation. How does this relationship vary across the South.
```{r}
coef(model.lm3)[2]
range(model.gwr$SDF$RD90)
```

The global regression coefficient is .51 but locally the coefficients range from 0.08 to .98.

Importantly we can map where resource deprevation has the most influence on the response variable.
```{r}
SH.sf$RDcoef <- model.gwr$SDF$RD90

tm_shape(SH.sf) +
  tm_fill("RDcoef", title = "Resource\nDeprivation\nCoefficient", palette = 'Blues') +
  tm_layout(legend.outside = TRUE)
```

All values are above zero, but areas in darker blue indicate where resource deprivation plays a stronger role in explaining homicide rates.

How about the unemployment rate?
```{r}
SH.sf$UEcoef <- model.gwr$SDF$UE90

tm_shape(SH.sf) +
  tm_fill("UEcoef", title = "Unemployment\nCoefficient", palette = 'PiYG') +
  tm_layout(legend.outside = TRUE)
```

While the global coefficient is negative indicating homicide rates tend to be lower in areas with more unemployment, the opposite is the case over much of Texas into Oklahoma.

Where does the model for homicide rates provide the best fit to the data? This is answered with a map of local R squared values (`localR2`).
```{r}
SH.sf$localR2 <- model.gwr$SDF$localR2

tm_shape(SH.sf) +
  tm_fill("localR2", title = "Local\nR Squared", palette = 'Purples') +
  tm_layout(legend.outside = TRUE)
```

When we use a regression model to fit data that vary spatially we are assuming an underlying stationary process. This means we believe the explanatory variables 'provoke' the same statistical response across the entire domain. If this is not the case then it shows up in a map of correlated residuals. One approach to investigate things further is to use geographic regression. Another approach is to use a single spatial regression model.


