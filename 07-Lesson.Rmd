---
title: "Lesson 7"
author: "James B. Elsner"
date: "February 1, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

# Working with spatial data continued

## Attribute variables

Recall attributes are the variables (stored as columns) in a spatial data frame.

There is no harm in keeping the geometry column because an operation on a `sf` object only changes the geometry when appropriate (e.g. by dissolving borders between adjacent polygons following aggregation). This means that the speed of operations with attribute data in `sf` objects is the same as with columns in a data frames.

Subset functions include `[`, `subset()` and `$`. {dplyr} subset functions include `select()`, `filter()`, and `pull()`. Both sets of functions preserve the spatial components of attribute data in `sf` objects.

The `[` operator can subset rows and columns. We use indexes to specify the elements we wish to extract from an object, e.g. object[i, j], with i and j typically being numbers representing rows and columns. Leaving i or j empty returns all rows or columns, so `world[1:5, ]` returns the first five rows and all columns. Here are some examples.
```{r}
library(spData)

world[c(1, 5, 9), ] # subset rows by position
world[, 1:3] # subset columns by position
world[, c("name_long", "lifeExp")] # subset columns by name
```

Here we use logical vectors for subsetting. First we create a logical vector `sel_area` 
```{r}
sel_area <- world$area_km2 < 10000
head(sel_area)
summary(sel_area)
```

And then we select only cases from the `world` simple feature dsta frame where the elements of the `sel_area` vector are `TRUE`.
```{r}
small_countries <- world[sel_area, ]
```

This creates a new simple feature data frame, `small_countries`, containing nations whose surface area is smaller than 10,000 sq km.

The base R function `subset()` provides another way to get the same result.
```{r}
small_countries <- subset(world, area_km2 < 10000)
```

Importantly the {dplyr} verbs can also be used on {sf} objects. The main {dplyr} subsetting functions are `select()` and `filter()`.

But caution! The {dplyr} (and  {raster}) package has a function called `select()`. When using both packages, the function in the most recently attached package will be used, 'masking' the incumbent function. 

This can generate error messages containing text like: unable to find an inherited method for function 'select' for signature "sf". To avoid this error message, and prevent ambiguity, we use the long-form function name, prefixed by the package name and two colons `dplyr::select()`.

The `select()` function selects columns by name or position. For example, we can select only two columns, `name_long` and `pop`, with the following command.
```{r}
world1 <- world %>%
  select(name_long, pop)
names(world1)
```

Note that the result is a simple feature data frame with the geometry column.

The `select()` function lets us subset and rename columns at the same time.
```{r}
world2 <- select(world, 
                 name_long, 
                 population = pop)
world2 <- world %>%
  select(name_long,
         population = pop)
names(world2)
```

The `pull()` function will return a single vector.
```{r}
population <- world %>%
  pull(pop)
head(population)
```

The `filter()` function is {dplyr}'s equivalent of base R `subset()` function. It keeps only rows matching given criteria, e.g., only countries with a very high average life expectancy.
```{r}
world3 <- world %>%
  filter(lifeExp > 82)
head(world3)
```

Aggregation summarizes a data frame by a grouping variable, typically an attribute column. An example of attribute aggregation is calculating the number of people per continent based on country-level data (one row per country). 

The `world` dataset contains the necessary ingredients: the columns `pop` and `continent`, the population and the grouping variable, respectively. The goal is to find the `sum()` of country populations for each continent.

This is done with the `group_by()` and `summarize()` functions from the {dplyr} package. 
```{r}
library(dplyr)
world_agg <- world %>%
  group_by(continent) %>%
  summarize(pop = sum(pop, na.rm = TRUE))
head(world_agg)
```

This approach gives us control over the new column names. For example to calculate the Earth's population and the number of countries type
```{r}
world %>% 
  summarize(pop = sum(pop, na.rm = TRUE),
            nC = n())
```

The two columns in the resulting attribute table are `pop` and `nC`. The functions `sum()` and `n()` were the aggregating functions. 

The result is an `sf` object with a single row representing the world (this works thanks to the geometric operation called 'union').

We can chain together functions to find the world's three most populous continents and the number of countries they contain.
```{r}
world %>% 
  select(pop, continent) %>% 
  group_by(continent) %>% 
  summarize(pop = sum(pop, na.rm = TRUE), 
            nC = n()) %>% 
  top_n(n = 3, wt = pop) %>%
  st_set_geometry(value = NULL) 
```

Combining data from different sources based on a shared 'key' variable is common. The {dplyr} package has multiple join functions including `left_join()` and `inner_join()`. Names follow conventions used in the SQL database language.

Here we see how to join non-spatial datasets to `sf` objects. Join functions work the same on `data.frames` and `sf` objects. The most common type of attribute join on spatial data takes an `sf` object as the first argument and adds columns to it from a `data.frame` specified as the second argument.

For example, we combine data on coffee production with the world dataset. The coffee data is in a data frame called `coffee_data` from the {spData} package (see `?coffee_data` for details). It has 3 columns: `name_long` names major coffee-producing nations and `coffee_production_2016` and `coffee_production_2017` contain estimated values for coffee production in units of 60-kg bags in each year. 

The `left_join()` function, which preserves the first dataset, merges `world` with `coffee_data`.
```{r}
world_coffee <- left_join(world, 
                          coffee_data)
class(world_coffee)
```

Because the input data frames share a variable (`name_long`) the join works without using the `by =` argument (see `?left_join` for details). The result is an `sf` object identical to the original world object but with two new variables (with column indices 11 and 12) on coffee production.
```{r}
names(world_coffee)
```

For joining to work, a 'key' variable must be supplied in both datasets.

Another feature of `left_join()` is that the results have the same number of rows as the first dataset. Although there are only 47 rows of data in `coffee_data`, all 177 the country records in `world` are kept intact in `world_coffee` (and `world_coffee2`). Rows in the first dataset with no match are assigned `NA` values for the new coffee production variables. 

If we want to keep only countries that have a match in the key variable then we use `inner_join()`.
```{r}
world_coffee_inner <- inner_join(world, 
                                 coffee_data)
nrow(world_coffee_inner)
```

Note that the result of `inner_join()` has only 45 rows compared with 47 in coffee_data. What happened to the other two rows? We can identify the rows that did not match using the `setdiff()` function.
```{r}
setdiff(coffee_data$name_long, world$name_long)
```

`Others` accounts for one row not present in the `world_coffee` dataset and  name of the Democratic Republic of the Congo accounts for the other. The name has been abbreviated, causing the join to miss it. 

Here we use the `str_subset()` function (string matching) from the {stringr} package to confirm what 'Congo, Dem. Rep.' of should be.
```{r}
library(stringr)
str_subset(world$name_long, "Dem*.+Congo")
```

To fix this issue, we will create a new version of `coffee_data` and update the name. Note: `grepl()` returns a logical vector for names that contain 'Congo'.
```{r}
coffee_data$name_long[grepl("Congo", coffee_data$name_long)] <- str_subset(world$name_long, "Dem*.+Congo")
world_coffee_match <- inner_join(world, coffee_data)
nrow(world_coffee_match)
```

It is also possible to join in the other direction: starting with a regular data frame and adding variables from a simple features object. 

Often, we would like to create a new column based on already existing columns. For example, we want to calculate population density for each country. For this we need to divide a population column, here `pop`, by an area column, here `area_km2` with unit area in square kilometers. 

We use one of the verbs - `mutate()` or `transmute()`. `mutate()` adds new columns in the `sf` object (the last one is reserved for the geometry).
```{r}
world %>% 
  mutate(pop_dens = pop / area_km2)
```

The difference between `mutate()` and `transmute()` is that the latter skips all other existing columns (except for the sticky geometry column):

More information: https://geocompr.robinlovelace.net/attr.html

## Areal-weighted interpolation

Areal weighted interpolation estimates the value of some variable from a set of polygons to an overlapping but incongruent set of target polygons. For example, suppose we want demographic information given at the Census tract level to be estimated within the tornado damage path. Damage paths do not align with census tract boundaries so areal weighted interpolation is needed to get demographic estimates at the tornado level.

The function `st_interpolate_aw()` from the {sf} package performs areal-weighted interpolation of polygon data. As an example, consider the number of births by county in North Carolina in 1974.

The data are available as a shapefile as part of the {sf} package system file. We use the `st_read()` function together with the `system.file()` function to import the data to our current session. We then plot the geometry.
```{r}
nc <- st_read(system.file("shape/nc.shp", 
                          package = "sf"))
plot(nc$geometry)
```

Next we construct a 20 by 10 grid of polygons that overlaps the state using the `st_make_grid()` function. The function takes the bounding box from the `nc` simple feature data frame and constructs a two-dimension grid using the dimensions specified with the `n =` argument.
```{r}
g <- st_make_grid(nc, n = c(20, 10))

plot(g)
plot(nc$geometry, add = TRUE)
```

The result is a set of overlapping but incongruent set of polygons.

Then we use the `st_interpolate_aw()` function with the first argument a simple feature data frame for which we want to aggregate a particular variable and the argument `to =` set to the set of overlapping polygons. The argument `extensive =` if `FALSE` (default) assumes the variable is spatially intensive (like population density) and the mean is preserved. 
```{r}
a1 <- st_interpolate_aw(nc["BIR74"], 
                        to = g,
                        extensive = FALSE)
```

The result is a simple feature data frame with the same geometry as grid and a single variable (`BIR74`).

We note that the average number of births across the state at the county level matches (roughly) the average number of births across the grid of polygons, but the sums do not match.
```{r}
mean(a1$BIR74) / mean(nc$BIR74)

sum(a1$BIR74) / sum(nc$BIR74)
```

An *intensive* variable is independent of the spatial units (e.g., population density, percentages); a variable that has been normalized in some fashion. An *extensive* variable depends on the spatial unit (e.g., population totals). Assuming a uniform population density, the number of people will depend on the size of the spatial area.

Since the number of births in each county is an extensive variable, we toggle the `extensive =` argument to `TRUE`.
```{r}
a2 <- st_interpolate_aw(nc["BIR74"], 
                       to = g, 
                       extensive = TRUE)
```

In this case we preserve the total number of births across the domain. We verify this 'mass preservation' property (pycnophylactic property).
```{r}
sum(a2$BIR74) / sum(nc$BIR74)
```

Here we create a plot of both interplations.
```{r}
a1$intensive <- a1$BIR74
a1$extensive <- a2$BIR74
plot(a1[c("intensive", "extensive")], 
     key.pos = 4)
```

### Example: Tornadoes: people and property

Here we are interested in the number of housing units affected by tornadoes occurring in Florida 2014-2018. We begin by creating a polygon geometry for each tornado record.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/1950-2018-torn-aspath.zip",
              destfile = "1950-2018-torn-aspath.zip")
unzip("1950-2018-torn-aspath.zip")
```

Import the data, transform the native CRS to 3857 (pseudo-Mercator), and filter.
```{r}
Torn.sf <- st_read(dsn = "1950-2018-torn-aspath", 
                   layer = "1950-2018-torn-aspath") %>%
  st_transform(crs = 3857)

FL_Torn.sf <- Torn.sf %>%
  filter(yr >= 2014, st == "FL")
```

Next we add a buffer to the geometries to represent the tornado path ('footprint'). The width of the buffer is 1/2 the width given by the variable labeled `wid`. First we create new variables giving the width (and length) in units of meters and then use the `st_buffer()` function.
```{r}
FL_Torn.sf <- FL_Torn.sf %>%
  mutate(Width = wid * .9144,
         Length = len * 1609.34)
FL_Torn.sf <- st_buffer(FL_Torn.sf, 
                        dist = sfdfL$Width/2)
```

Then we get the census data using the `get_acs()` function from the {tidycensus} package. The package is an interface to the decennial US Census and American Community Survey APIs and the US Census Bureau's geographic boundary files. Functions return Census and ACS data as simple feature data frames for all Census geographies.
```{r}
if(!require(tidycensus)) install.packages(pkgs = "tidycensus", repos = "http://cran.us.r-project.org")

library(tidycensus)
```

The geometry is the tract level and the variable is the unweighted sample housing units (B00002_001). We transform the CRS to that of the tornadoes.
```{r}
Census.sf <- get_acs(geography = "tract", 
                     variables = "B00002_001",
                     state = "FL",
                     year = 2018,
                     geometry = TRUE) %>%
  st_transform(crs = st_crs(FL_Torn.sf))
```

Finally we use the `st_interpolate_aw()` function
```{r}
awi.sf <- st_interpolate_aw(Census.sf["estimate"],
                            to = FL_Torn.sf, 
                            extensive = TRUE)
head(awi.sf)
range(awi.sf$estimate, 
      na.rm = TRUE)
```

## Geographic subsetting

The {USAboundaries} package contains historical and contemporary boundaries for the United States with the data provided by the U.S. Census Bureau.

Individual states are extracted using the `us_states()` function. CAUTION: the function has the same name as the object `us_states` from the {spData} package.
```{r}
library(USAboundaries)

KS.sf <- us_states(states = "Kansas")
class(KS.sf)
plot(KS.sf$geometry)
```

We transform the CRS to a Web Mercator using the EPSG 3857 code.
```{r}
KS.sf <- KS.sf %>%
  st_transform(crs = 3857)
```

Suppose we want to subset geographically rather than based on some value in a column of the data frame. For example here we subset the tornado tracks by the boundaries defined by Kansas.

We use the `st_intersection()` function to determine the set of all tornadoes that intersect the Kansas polygon. The first argument is the simple feature data frame that we want to subset and the second defines the geometry over which the subset occurs.
```{r}
KS_Torn.sf <- st_intersection(Torn.sf, 
                              KS.sf$geometry)

plot(KS.sf$geometry)
plot(KS_Torn.sf$geometry, add = TRUE)
```

Note that no tornado track lies outside the state border. Line strings that lie outside the border are clipped at the border.

If we want the entire tornado track for all tornadoes that passed into (or through) the state, then we first use the geometric binary predict function `st_intersects()`. By specifying `sparse = FALSE` a matrix with a single column of `TRUE`s and `FALSE`s is returned, which is subsequently used to subset the tornado data frame.
```{r}
Intersects <- st_intersects(Torn.sf,
                            KS.sf$geometry,
                            sparse = FALSE)
KS_Torn2.sf <- Torn.sf[Intersects, ]

plot(KS.sf$geometry)
plot(KS_Torn2.sf$geometry, add = TRUE)
```

Suppose we want to determine the distance between the geographic center of the state and the centroid of all the tornadoes?

We start by computing the centroids for the state polygon and for the combined set of all Kansas tornadoes.
```{r}
KSgeocenter <- st_centroid(KS)
KStorcenter <- st_centroid(st_combine(KStors))
```

Finally we make a map and then compute the distance in meters using the `st_distance()` function.
```{r}
plot(KS$geometry)
plot(KSgeocenter$geometry, add = TRUE, col = "blue")
plot(KStorcenter, add = TRUE, col = "red")
st_distance(KSgeocenter, KStorcenter)
```

More examples: https://www.jla-data.net/eng/spatial-aggregation/