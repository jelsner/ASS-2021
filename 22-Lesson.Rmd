---
title: "Lesson 22"
author: "James B. Elsner"
date: "March 24, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"Rarely is anyone thanked for the work they did to prevent the disaster that didn't happen."** â€“ Mikko Hypponen

### Variogram Models

Computing the sample variogram is the first step in modeling geostatistical data. The next step is to fit a model to these variogram estimates. We replace a scatter of points with a statistical model. 

The model is important since the sample variogram is made only at specified lag distances (with specified lag tolerance and azimuth). We need a continuous function that varies smoothly across all lags.

Variogram models come from different families. The decision includes (1) choosing the model family and (2) determining the parameters: nugget, sill, and range.

The exponential model family reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill.
```{r}
c0 <- .1
c1 <- 2.1
a <- 1.3
curve(c0 + c1*(1 - exp(-3*x/a)), 
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The spherical model family is piece wise reaching the sill at x = 1 (here).
```{r}
curve(c0 + c1*(3*x/2 - x^3/2),
      from = .01, to = 1,
      xlab = "h",
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The Gaussian model family is sigmoidal. It is used when the data exhibit strong correlations at the shortest lag distances.  The inflection point of the model occurs at $\sqrt{a/6}$.
```{r}
curve(c0 + c1*(1 - exp(-3*x^2/a^2)),
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")),
      las = 1)
```

Other families include

* Linear models: $\hat \gamma(h)$ = c0 + b * h.
* Power models:  $\hat \gamma(h)$ = c0 + b * h$^\lambda$.

These models have no sill.

Choosing a variogram family is largely done by eyeballing the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by a weighted least-squares (WLS) algorithm. There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data.

### Kriging

The final step is kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine.

A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error variance. The type of kriging depends on the characteristics of the observations and the purpose of interpolation.

* Simple kriging assumes a known constant mean for the domain.  
* Ordinary kriging assumes an unknown constant mean.  
* Universal kriging assumes an unknown linear or nonlinear trend in the mean.  

The steps are:

1. Examine the observations for trends and isotropy.
2. Compute an empirical variogram.
3. Fit a variogram model to the empirical variogram.
4. Create an interpolated surface using kriging.

## Interpolation using functions from the {geoR} package

The {geoR} package has functions for doing geostatistics. There are others but it was one of the first. It is useful for learning how things work. 

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).
```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
df <- data.frame(sx, sy, zobs)

library(ggplot2)

ggplot(df, aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

Lag distance (distance between locations) is the independent variable in the variogram function. We get all pairwise distances by applying the `dist()` function to a matrix of spatial coordinates.
```{r}
dist(cbind(sx, sy))
max(dist(cbind(sx, sy)))
min(dist(cbind(sx, sy)))
```

The function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units.

The functions in the {geoR} package work with objects of class `geodata`. Thus we first need to convert the data frame of observations and locations into a `geodata` object.

This is done with the `as.geodata()` function. The default for the function is to assume that the first two columns of the data frame contain the coordinates and the third column contains the observed data values. This is the way we constructed the data frame so we can use the defaults.
```{r}
library(geoR)

gdf <- as.geodata(df)
str(gdf)
```

An object of class `geodata` contains two lists: the coordinates of the locations and the observed values as `data`. It may contain other elements like coordinate boundaries but unlike working with `ppp` objects with functions from the {spatstat} package a boundary defining the domain is not required.

For an arbitrary data frame we can specify where the coordinate and data columns by column numbers. For example, if the location coordinates are in columns five and six and the observed values are in column eight then use `coords.col = c(5, 6)` and `data.col = 8`.

With the class set to `geodata` methods like `summary` and `plot` provide attribute and spatial information about the observations. For example the `summary()` method outputs the number of observations, a summary of the coordinates and a summary of the observed values.
```{r}
summary(gdf)
```

We access the coordinates and the data using the `$` operator.
```{r}
gdf$coords
gdf$data
```

The `plot()` method produces a four panel plot.
```{r}
plot(gdf)
```

The upper left panel is a map with the locations of the observations given by colors and symbols according to their values. Green triangles have the smallest values and red crosses have the largest values. We can see an upward trend in values from northwest to southeast.

This trend is decomposed in the upper right and lower left panels. The upper-right panel shows the north-south (Y Coord) coordinate plotted against the data values. As the data values increase to the right the north-south coordinate decreases (smaller data values are in the north). The lower-left panel shows the data plotted against the east-west (X Coord). Moving from west to east we see the data values tend to increase.

The lower right panel is a non-spatial distribution of the data values shown with a histogram, and density curve, and a rug plot.

To plot the trend in the east-west direction, type
```{r}
ggplot(df, aes(x = sx, y = zobs)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()
```

The data can be plotted with the trend removed. The `trend = "1st"` refers to a linear (first-order) trend.
```{r}
plot(gdf, trend = "1st")
```

Now the plots show the residuals from the first-order trend model.

As another example, consider the dataset called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain.
```{r}
library(MASS)

data(topo)
topo.gdf <- as.geodata(topo)
plot(topo.gdf)
```

Note the trend in the north-south direction and the skewness in the observed values. 

Examine the residuals after removing a first-order trend from the observations.
```{r}
plot(topo.gdf, trend = "1st")
```

The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend in the east-west direction. 

Examine the residuals after removing a second-order trend.
```{r}
plot(topo.gdf, trend = "2nd")
```

The residuals from a second-order polynomial fit are symmetric and the trends are gone. However, the residuals appear to show spatial autocorrelations (areas with above and below residuals). 

### Empirical variograms

Consider the dataset `s100` available from the {geoR} package. The `points.geodata()` function produces a bubble plot showing the locations of the observations and the relative magnitude of the `z` variable.
```{r}
data(s100)
points.geodata(s100) 
```

The `variog()` function computes the empirical variogram. Here we save it in the object `s100.v`.
```{r}
s100.v <- variog(s100)
str(s100.v)
```

Information in the variogram object includes the lag distances (`u`), the values of the variogram at those distances (`v`), the number of distance pairs used to compute the variogram values at each lag (`n`), the standard deviation of the variogram values, and the coefficients of the trend surface (with no trend, the value is the overall mean of the data) (`beta.ols`) among other information.

Note: Mathematically lag distance is denoted with $h$ but in the {geoR} package it is `u`.

Verify the mean value.
```{r}
mean(s100$data)
```

Plot the variogram.
```{r}
plot(s100.v)
```

The semivariance ($\gamma(u)$) is plotted against lag distance ($u$). Values increase with increasing lag until a lag distance of about 1. 

At large lags there are fewer estimates so the values have greater variance. A model for the semivariance is fit only for the the increasing portion of the graph.

Another example: variogram of the `topo` dataset.
```{r}
topo.v <- variog(topo.gdf)
plot(topo.v, xlab = "Lagged Distance", 
     ylab = expression(paste(gamma, "(u) [ft", {}^2, "]")),
     las = 1, pch = 16)
```

The variogram values have units of square feet and are calculated using point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag so the variogram values are less precise at large distance.

Plot the number of point pairs used as a function of lag distance.
```{r}
ggplot(data.frame(u = topo.v$u, n = topo.v$n), aes(x = u, y = n)) +
  geom_point() +
  xlab("Lag Distance") + ylab("Number of Observation Pairs") +
  theme_minimal()
```

### Wolfcamp aquifer data

Some years ago there were three nuclear waste repository sites being proposed in Nevada, Texas, and Washington. The site needs to be larger enough for more than 68,000 high-level waste containers placed underground, about 9 m (~30 feet) apart, in trenches surrounded by salt. In July of 2002 the Congress approved [Yucca Mountain](https://en.wikipedia.org/wiki/Yucca_Mountain_nuclear_waste_repository), Nevada, as the nationâ€™s first long-term geological repository for spent nuclear fuel and high-level radioactive waste.

The site must isolate the waste for 10,000 years. Leaks could occur, however, or radioactive heat could cause tiny quantities of water in the salt to migrate toward the heat until eventually each canister is surrounded by 22.5 liters of water (~6 gallons). A chemical reaction of salt and water can create hydrochloric acid that might corrode the canisters.

The piezometric-head data at the site were obtained by drilling a narrow pipe into the aquifer and letting water seeks its own level in the pipe (piezometer). 

The head measurements, given in units of feet above sea level, are from drill stem tests and indicate the total energy of the water in units of height. The higher the head height, the greater the potential energy. 

Water flows away from areas of high potential energy so aquifer discharge is proportional to the gradient of the piezometric head.

The data are in `wolfcamp.csv` on my website.

#### Step 1: Examine the observed data for trends, check for normality

Get the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read.csv(L, header = TRUE)
```

Create a simple feature data frame and then map the locations and head heights.
```{r}
library(sf)
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(mapview)
mapView(wca.sf, 
        zcol = "head")
```

Convert the data frame to a `geodata` object. Note there is no method to do this from a simple feature.
```{r}
wca.gdf <- as.geodata(wca.df, 
                      coords.col = 1:2, 
                      data.col = 3)
```

Find the duplicate location(s).
```{r}
dup.coords(wca.gdf)
```

The locations are the same, but the data values are different. This may represent an error or multiple measurements at this one location.  

We can average the values or we can exclude a row. Here we remove the row 30 observation.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = 1:2, 
                      data.col = 3)
summary(wca.gdf)
```

There are 84 well sites. The spatial bounding box is given under coordinates summary. The minimum distance between sites is .01 degrees and the maximum distance is 4.6 degrees.

The data values are summarized. The values are piezometric head heights in units of feet.

The `plot()` method for `geodata` provides a panel of plots with information about the data useful for modeling them.
```{r}
plot(wca.gdf)
```

The upper left panel is a graph of the observed locations with symbols reflecting quartiles of the observed piezometric head heights. There is a clear trend in the data with the highest potential energy over the southwest (red crosses) and lowest over the northeast (blue circles). 

The nature of this trend can be seen in the upper right and lower left panels. The upper right panel plots the data against the y coordinate and the lower left panel plots the data against the x coordinate. Both graphs indicate a linear trend.

A histogram of the head heights is shown in the lower right panel. The rug locates the values along the data axis. The data are bi-modal and skewed to the right.

Repeat the plot after removing the 1st order trend. What happens?
```{r}
plot(wca.gdf, trend = "1st")
```

With the trend removed the variation of values appears to be roughly symmetric and the high and low values are mixed. However, there is some spatial grouping to the residuals.

#### Step 2: Compute empirical variograms

We noted the maximum distance between any two locations is 4.6 degrees. It is a good idea to plot the variogram values for distances only between 0 and about 1/2 the maximum distance.

Since there is a linear trend in the data over the spatial domain it is removed (trend argument) before computing the variogram values.
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
```

Here we see an increase in the variance with distance until about one degree, then the values fluctuate about a variance of about 41000 (ft$^2$).

What does the variogram look like if we do not first remove the trend?

This continuously increasing set of variances with little fluctuation about a best fit curve indicates a trend in the data that must be removed before the variogram is modeled. 

There are two sources of variation in the field values: trend and spatial correlation. Trends are modeled with smooth curves and correlations are modeled with the variogram.

The `variog()` function has options for specifying a classical or modulus estimator. By default the function uses the classical estimator. To override this, include the `estimator.type = "modulus"` argument. The modulus estimator of the variogram is more resistant to outliers in the data.

Compare the classical with the modulus variograms for the Wolfcamp aquifer data.
```{r}
par(mfrow = c(1, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3), 
     main = "Classical")
plot(variog(wca.gdf, trend  ="1st", 
            max.dist = 2.3, 
            estimator.type = "modulus"),
     main = "Modulus")
```

The two variograms are nearly identical.

### Variogram cloud

We can examine the makeup of the variogram in more detail using a 'variogram cloud'. Consider again the simulated data set `s100`.
```{r}
par(mfrow = c(1, 1))
plot(s100)
```

There appears to be a trend in the east-west direction with higher values in the east. This is seen clearly in the bottom left panel.

Remove this first-order trend.
```{r}
plot(s100, 
     trend = "1st")
```

We can see that the trend is adequately modeled with a 1st-order surface. Residuals appear symmetric about zero. 

A summary of the data indicates the maximum lagged distance of 1.3 so the `max.dist` argument is set to .6. 
```{r}
summary(s100)
```

Here we specify the eleven variogram estimates between 0 and .6 using the `uvec` argument.
```{r}
plot(variog(s100, 
            trend = "1st", 
            uvec = 11, 
            max.dist = .6))
```

We 'decompose' the variogram estimates with the `option = "cloud"`.
```{r}
c1 <- variog(s100, trend = "1st", 
             option = "cloud", 
             max.dist = .6)
plot(c1)
```

Each point on the plot is the difference (absolute value) between two observed values (y-axis) a lag distance apart (x-axis). There are 100 observations so there are 100 * 99/2 = 4950 possible points on this plot. Actually here only 3131 since we limit the distance to less than .6 degrees.

There is a tendency for more large differences as the lag increases but most differences are small for a given lag. The variability in observational differences is quite large especially for longer lag distances.

The variogram computes the sum of the squared differences as a function of lag distance grouped by a lag tolerance. Here we indirectly specify the lag tolerance with the `uvec` argument and compare the variogram with the variogram cloud.
```{r}
par(mfrow = c(1, 2))
v1 <- variog(s100, trend = "1st", 
            uvec = seq(0, .6, l = 11))
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
plot(v1); plot(c1)
```

Note the difference in the scale on the y-axis between the variogram cloud and the variogram is a factor of 10.

The variogram cloud can be grouped into lag distance classes (bins) and displayed with a box plot. This gives an idea of the general shape the variogram model should take.
```{r}
par(mfrow = c(1, 1))
b1 <- variog(s100, trend = "1st", 
            max.dist = .6, 
            bin.cloud = TRUE, 
            estimator.type = "classical")
plot(b1, bin.cloud = TRUE)
```

The box plot summarizes the squared differences for a given lag (and lag tolerance) with the median. There is a box plot for each lag but the lag distance is given as a bin number. 

This summary information helps us anticipate the type of variogram model. As an aside, note how to use the expression and paste functions to include symbols as part of the axis label.
```{r}
df <- data.frame(u = b1$u, v = b1$v)
ggplot(df, aes(u, v)) + 
  geom_point() + 
  geom_smooth(span = .8) +
  scale_y_continuous(limits = c(0, .6)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lagged distance (h)")
```

The blue line is a least-squares regression smoother through the variogram estimates. The fact that it is not horizontal indicates spatial autocorrelation in the values that is separate from the first-order trend. The shape of the blue line gives an idea of the type of variogram family of models we should consider.

Now we can guess at a family for the variogram model and eyeball the parameters. Recall the plot from Lesson 18. This is called variography. We will discuss this in Lesson 20.

```{r}
plot(variog(s100, max.dist = 1), 
     xlab = "Lagged Distance (h)",
     ylab = expression(paste(gamma,"(h)")), 
     las = 1, pch = 16)
abline(h = 0)
abline(h = .15, col = "red")
arrows(0, 0, x1 = 0, y1 = .15, length = .1)
text(0, y = .05, labels = "nugget", pos=4)
abline(h = .9, col = "red")
arrows(0, .15, x1 = 0, y1 = .9, length = .1)
text(0, y = .8, labels="sill (partial sill)", pos=4)
abline(v = .6, col = "red")
arrows(0, 1, x1 = .6, y1 = 1, length = .1)
text(.4, y = 1.04, labels = "range")
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.

### Directional variograms

Recall that the assumption of isotropy implies the same spatial autocorrelation function regardless of direction.

To examine the assumption of isotropy we compute variograms using observational pairs located along the same orientation. Instead of considering all observational pairs within a lag distance $h$ and lag tolerance $\delta h$, we consider only pairs within a wedge-shaped segment of this annulus.

This is done with the `variog()` function and specifying a `direction =` argument in units of radians. Radians are fractions of `pi`. We also need to specify the angle tolerance.

For example to compute and plot the variogram for the direction of 45 degrees (NE to SW) with a default tolerance of 22.5 degrees (default), type
```{r}
v45 <- variog(wca.gdf, 
              trend = "1st", 
              max.dist = 2.3, 
              direction = pi/4)
plot(v45)
```

Repeat for the other three quadrants (increments of $\pi$/4).
```{r}
par(mfrow = c(2, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 0),
     main = expression(0 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/4),
     main = expression(45 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/2),
     main = expression(90 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 3 * pi/4),
     main = expression(135 * degree))
```

The variograms appear similar but it's difficult to tell since the vertical scales are not all the same. The `variog4()` function makes the comparison easier by plotting them on the same graph. It works like `variog()` but takes a vector of directions. The default directions are 0, 45, 90, and 135 degrees.
```{r}
par(mfrow = c(1, 1))
v4 <- variog4(wca.gdf, 
              trend = "1st",
              max.dist = 2.3)
plot(v4)
```

This plot makes it clear that the assumption of isotropy is reasonable.

#### Step 3: Fit a variogram model to the empirical variogram

Next we fit a curve through the set of points that make up the empirical variogram. The curve is called the variogram model. 

Since the `s100` data were generated using a variogram model, we already know the answer. The model is exponential with a sill (partial) of 1, a range of .3, and a nugget of 0.

Plot the empirical variogram values and overlay the curve corresponding to the exponential model with the `lines.variomodel()` function. The sill and range parameters are given in order using the `cov.pars` argument.
```{r}
plot(variog(s100, uvec = seq(0, 1, l = 21)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), nugget = 0, 
                 max.dist = 1.2, lwd = 2, col = "red")
```

Note that the practical range for the exponential model is the distance along the horizontal axis at which the curve reaches 95% of the sill which is 3 times the range specified in the `cov.pars` argument.

The `uvec =` argument allows control over the range of distance values and the number of variogram estimates. We specify the maximum distance in the `lines.variomodel()` function.

With observed data we don't know the model so we estimate the parameters using information from the empirical variogram.

* By eye: Trial and error over several models and parameter values. The `lines.variomodel()` function can help.  
* By least squares fit:  Using ordinary least squares (OLS) or weighted least squares (WLS) methods available through the `variofit()` function.  
* By maximum likelihood methods: Options for maximum likelihood (ML) and restricted maximum likelihood (REML) are available through the `likfit()` function.  

Try various curves and choosing the one that looks the best. Here we plot three models on top of the `s100` empirical variogram.
```{r}
plot(variog(s100, max.dist = 1))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), 
                 nug = 0, max.dist = 1)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .5),
                 nug = .1, max.dist = 1, 
                 col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(.8, .8),
                 nug = .1, max.dist = 1, 
                 col = "blue")
```

The black line is the model used to generate the `s100` dataset. The red line is based on the same exponential (exp) function but has a range of .5 and a nugget of .1. The blue line is based on a spherical function with a sill and range of .8 and a nugget of .1. All three models fit the points reasonably well. This is important: In practice the choice often makes little difference in the quality of the spatial interpolation.

A function to help with the fitting is `eyefit`. This opens a widget with radio buttons and slider bars making it easier to see how the model changes through various parameter sets. CAUTION: Mac users must have X11 installed. X11 is no longer included in OSX.
```{r, eval=FALSE}
eyefit(variog(s100))
```

#### Variogram model for the Wolfcamp aquifer data

Let's fit a variogram model to the Wolfcamp aquifer data. First plot the empirical variogram. We've seen that directional variograms are not needed.
```{r}
wca.v <- variog(wca.gdf, 
                trend = "1st", 
                max.dist = 2.3)
plot(wca.v)
```

Begin with a spherical model with a range of .8, a sill of 40000, and a nugget of 10000.  
```{r}
plot(wca.v)
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(30000, .8),
                 nug = 10000, max.dist = 2.3)
```
 
In the `cov.pars` argument the first value is the "partial" sill and the second is the range. With the nugget set at 10000 and partial sill at 30000, the sill is 400000. The sill and nugget are variance measures so they have units of square feet. The range is distance so it has the corresponding spatial units.

The model looks reasonable. Perhaps the sill should be a bit higher, say 43000. We increase the partial sill accordingly,
```{r}
plot(wca.v)
lines.variomodel(cov.model="sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
```

Better still. We try an exponential model with the same sill but the same range.
```{r}
plot(wca.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(33000, .8/3),
                 nug = 10000, max.dist = 2.3,
                 col = "green")
```

Not as good. The exponential model does not have the sharp turn at the sill. We settle on the spherical model with a sill of 43000, a range of .8, and a nugget of 10000.

#### Fine tune the parameter values

Next we tune the parameter estimates using the `variofit()` function. The function takes a set of initial parameter values and improves upon them using the method of weighted least squares. Alternatively we can use the `likfit()` function to fine tune the parameters. Here we use it to adjust the variogram model parameters estimated above. We save the model in the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

The function improves on the initial set of model parameters until the log-likelihood value is maximized. Any other set of parameters will produce a log likelihood value smaller than -553.  

The output includes values for the trend surface. It is a linear trend in two dimensions so it's represented by a plane with a single z-intercept value (`beta0`) and two slope values corresponding to the x (`beta1`) and y (`beta2`) directions.  

The units on the slope parameters are data units per unit spatial distance.  Thus the `beta1` value is -400 ft/deg longitude.  For every one deg longitude east, the piezometric head height decreases by 400 ft. We saw a trend in the SW-NE direction (exploratory plot). The `beta1` (`beta2`) value quantifies this slope in the east-west (north-south) direction.

The output also includes the parameter values for the nugget (`tausq`), the partial sill (`sigmasq`) and the range (`phi`).

Overlay the maximum likelihood solution to the spherical model by typing
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
lines(wca.vm, col = "blue")
```

We see the maximum likelihood solution is better at fitting the points at lag distances between .5 and 1. At these lags, the variogram estimates are most reliable as n (number of observation pairs) is largest.
```{r}
variog(wca.gdf)$n
```

The `likfit()` function is iterative and should find the same solution using somewhat different starting values. Try it using a partial sill of 30000 a nugget of 5000 and a range of 1. 
```{r}
likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(30000, 1),
                 nug = 5000, 
                 cov.model = "spherical")
```

A summary function on the `likfit` output provides more information about the fitted variogram model.
```{r}
summary(wca.vm)
```

The summary provides parameters of the trend model and values of the spatial model parameters. Importantly the summary gives the log likelihood along with the AIC and BIC values. It also gives those values for a non-spatial model. A non-spatial model in this context is the trend plus spatially uncorrelated random variation.

Since the log likelihood value is larger for the spatial model and the AIC and BIC values are smaller, it is clear that a spatial model is better than the non-spatial model.

#### Profile likelihood

The `proflik()` function gives a matrix of log likelihood values for a range of model parameters. The matrix can then be plotted to get a synoptic view of the relationship between the parameters and the likelihood. This can take a few seconds to compute.
```{r}
prof <- proflik(wca.vm, geodata = wca.gdf, 
                sill.val = seq(30000, 60000, length = 6),
                range.val = seq(.3, 2.3, length = 6), 
                nug.val = 10000, uni.only = FALSE)
plot(prof, nlevels = 16)
```

The horizontal axis is the partial sill ($\sigma^2$) and the vertical axis is the range ($\phi$). The nugget is set at 10000. The best fit parameters are indicated by the circle. At this location the log-likelihood is maximized. The likelihood is near the maximum for a broad set of range and partial sill values.

The marginal profiles are also available and can be plotted alongside the contour plot using
```{r}
par(mfrow = c(1, 3))
plot(prof, nlevels = 8)
```