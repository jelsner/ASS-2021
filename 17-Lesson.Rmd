---
title: "Lesson 17"
author: "James B. Elsner"
date: "March 8, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"To me programming is more than an important practical art. It is also a gigantic undertaking in the foundations of knowledge."** â€“ Grace Hopper

Note on schedule: This week: Lessons 17 & 18 and Assignment 8. Next week Lessons 19 & 20 but _no assignment_. Friday March 19 will be the second wellness day for this class this semester.

Last week: Point pattern data where the focus is on spatial event locations and where the null hypothesis is CSR. The {spatstat} package has functions for analyzing and modeling this type of data. We looked at how to convert simple features to `ppp` objects and how to get the intensity with the `intensity()` function and the spatial varying intensity with the `density()` function.

We also saw that the local intensity might vary with some explanatory variable and we quantified this variation for the tornado reports with the `rhohat()` function using distance to nearest city as the covariate.

## Intensity trend as a possible confounding factor

Quantifying the tornado report bias is straightforward in Kansas since there is no trend in the local intensity. That is, there are no large gradients in the number of reports as we move across different regions of the state.

Things are different in Texas. Let's see how. Convert the tornado reports (EF1 or worse) occurring over the state as a `ppp` object. We use an EPSG code for a Texas-centric Lambert conic conformal projection.
```{r}
library(sf)
library(tidyverse)
library(spatstat)
library(maptools)
library(USAboundaries)

Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
  st_transform(crs = 3082) %>%
  filter(mag >= 0)

T.ppp <- Torn.sf %>%
  as_Spatial() %>%
  as.ppp() 

TX.sf <- us_states(states = "TX") %>%
  st_transform(crs = st_crs(Torn.sf)) 

W <- TX.sf %>%
  as_Spatial() %>%
  as.owin()

( T.ppp <- T.ppp[W] %>%
  spatstat::rescale(s = 1000, 
                    unitname = "km") )
intensity(T.ppp)
```

There are 8,736 tornado reports. The distance (length) unit is kilometer. The average intensity is .013 events per square kilometer over this 69-year period (1950-2018).

Next we plot the local intensity using a kernel smoother using the `plot()` method on the results of the `density()` function.
```{r}
T.ppp %>%
  density() %>%
  plot()

plot(T.ppp, 
     pch = '.', 
     add = TRUE)
```

Unlike the local intensity of tornadoes across Kansas and Iowa (Assignment 7), here we see a clear trend of tornado reports from a low in the southwest to a high in the northeast. The average intensity of .013 tornadoes per square km is above the local intensity over southwestern parts of the state and below in the northern parts.

Next we compute and plot the local intensity as a smoothed function of distance to nearest town or city as we did for Kansas. We start by removing the marks on the tornado events assigning the unmarked `ppp` object to `Tum.ppp`. We then create a `ppp` object from the city/town locations obtained from the `us_cities()` function from the {USAboundaries} package and then subset the tornado events by the window 
```{r}
Tum.ppp <- T.ppp %>%
  unmark()

C.ppp <- us_cities() %>%
  filter(population >= 1000) %>%
  st_transform(crs = st_crs(Torn.sf)) %>%
  as_Spatial() %>%
  as.ppp() %>%
  unmark()

C.ppp <- C.ppp[W] %>%
  spatstat::rescale(s = 1000,
                    unitname = "km")
```

Next we create a distance map of the city/town locations using the `distmap()` function. This distance map is the covariate.
```{r}
Zc <- distmap(C.ppp)
plot(Zc)
```

Finally we compute the intensity of tornadoes as a smoothed function of distance to nearest town/city with the `rhohat()` function. We then prepare the output and make a plot.
```{r}
rhat <- rhohat(Tum.ppp, 
               covariate = Zc,
               method = "transform")

data.frame(dist = rhat$Zc, 
           rho = rhat$rho, 
           hi = rhat$hi, 
           lo = rhat$lo) %>%
ggplot() +
  geom_ribbon(aes(x = dist, ymin = lo , ymax = hi), alpha = .3) +
  geom_line(aes(x = dist, y = rho), color = "red") +  
  scale_y_continuous(limits = c(0, NA)) +
  geom_hline(yintercept = intensity(Tum.ppp), color = "blue") +
  ylab("Tornado reports per sq. km") +
  xlab("Distance from nearest town center (km)") +
  theme_minimal()
```

We see that the intensity of the tornado reports is much higher than the average intensity in the vicinity of towns and cities. However caution needs to exercised in the interpretation because the trend of increasing tornado reports moving from southwest to northeast across the state mirrors the trend in the occurrence of cities/towns.

We can quantify this effect by specifying a function in the `covariate =` argument. Here we specify a flat surface with `x,y` as arguments and `x + y` inside the function.
```{r}
plot(rhohat(Tum.ppp, 
            covariate = function(x,y){x + y},
            method = "transform"))
```

Local intensity increases along the axis labeled `X` starting at a value of 7,400. At value of `X` equal to about 8,200 the spatial intensity stops increasing.

Units along the horizontal axis are kilometers but the reference (intercept) distance is at the far left. So we interpret the increase in spatial intensity going from southwest to northeast as a change across about 800 km (8200 - 7400).

The local intensity of cities/towns has the same property (increasing from southwest to northeast then leveling off).
```{r}
plot(rhohat(C.ppp, 
            covariate = function(x,y){x + y},
            method = "transform"))
```

So the population bias towards more reports near towns/cities is potentially confounded by the fact that there are more cities and towns in areas that have conditions more favorable for tornadoes.

Bottom line: We can only get so far by examining intensity estimates if our interest lies in inferring the cause of spatial variation in the local intensity. We need to look at second-order properties of the events. Before we go there, lets consider two more examples of how to make use of local intensity variations.

## Estimating the relative risk of events

Combining intensity maps of multitype events allow us to estimate the relative risk. More generally the relative risk is a conditional probability. For example, given a tornado in Texas what is the chance that it will cause at least EF3 damage? 

To answer this we import the data again then mutate and select the damage rating as a factor called `EF` before turning this into a planar point pattern subset by the boundary of Texas.
```{r}
Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
  st_transform(crs = 3082) %>%
  filter(mag >= 0) %>%
  mutate(EF = as.factor(mag)) %>%
  dplyr::select(EF)

T.ppp <- Torn.sf %>%
  as_Spatial() %>%
  as.ppp()

W <- TX.sf %>%
  as_Spatial() %>%
  as.owin()

T.ppp <- T.ppp[W]
summary(T.ppp)
```

Chance that a tornado anywhere in Texas will be at least EF3 or worse is the sum of the proportions for these types: .03594 + .00549 + .00069 = .042 (or 4.2%). But as we saw there is a gradient in intensity from the southwest to the northeast over the state.

We create two `ppp` objects one being the set of all tornado locations with damage ratings 0, 1, or 2 and the other the set of all tornado locations with damage ratings 3, 4, or 5.

We subset the object using brackets (`[]`) and the logical operator `|` (or) and then merge the two subsets assigning names `H` and `I` as marks.
```{r}
H.ppp <- unmark(T.ppp[T.ppp$marks == 2 | T.ppp$marks == 1 | T.ppp$marks == 0])
I.ppp <- unmark(T.ppp[T.ppp$marks == 3 | T.ppp$marks == 4 | T.ppp$marks == 5])
T2.ppp <- superimpose(H = H.ppp, 
                      I = I.ppp)
```

The probability that a tornado chosen at random is intense (EF3+) is 4%. Plot the event locations for the set of intense tornadoes.
```{r}
plot(I.ppp, pch = 25, cols = "red", main = "")
plot(T.ppp, add = TRUE, lwd = .1)
```

To obtain the relative risk we use the `relrisk()` function. If X is a multitype point pattern with factor marks and two levels of the factor then the events of the first type (the first level of `marks(X)`) are treated as controls or non-events, and events of the second type are treated as cases.

The function estimates the local probability of a case (i.e. the probability $p(u)$ that a point at $u$ will be a case) using a kernel density smoother. The bandwidth for the kernel can be specified or can be found through an iterative cross-validation procedure (recall the bandwidth selection procedure used in geographic regression) using the `bw.relrisk()` function. The bandwidth will have units of distance (here meters). We specify a minimum and maximum bandwidth with the `hmin =` and `hmax =` arguments. This takes a few seconds.
```{r}
( bw <- bw.relrisk(T2.ppp,
                   hmin = 1000,
                   hmax = 200000) )
```

The optimal bandwidth is 120 km. 

Here we compute the relative risk at points defined by a 256 by 256 grid. 
```{r}
rr <- relrisk(T2.ppp, 
              sigma = bw,
              dimyx = c(256, 256))
```

Th result is an object of class `im` (image) with values we can interpret as the conditional probability of an 'intense' tornado (see https://en.wikipedia.org/wiki/Enhanced_Fujita_scale).

We retrieve the range of probabilities with the `range()` function. Note that many of the values are `NA` corresponding pixels that are outside the window so we set `na.rm = TRUE`.
```{r}
range(rr, na.rm = TRUE)
```

The probabilities range from a low of .5% to a high of 6.2%.

We map the probabilities with the `plot()` method after assigning the projection and re-projecting to a geographic CRS.
```{r}
library(raster)

rr.r <- raster(rr)
projection(rr.r) <- st_crs(Torn.sf)$proj4string

rr.r2 <- projectRaster(rr.r, crs = 4326)
plot(rr.r2)
```

Since the relative risk is computed for any point it is interesting to extract these probabilities for cities and towns.

Here we again use the `us_cities()` function from the {USAboundaries} package that extracts a simple feature data frame of cities and towns for particular states. The CRS is 4326 and we filter to keep only cities with at least 100000 in 2010.
```{r}
Cities.sf <- us_cities(state = "TX") %>%
  dplyr::filter(population > 100000)
```

We use the `extract()` function from the {raster} package to get a single value for each city. We put these values into the simple feature data frame. 
```{r}
Cities.sf$rr <- raster::extract(rr.r2, 
                                Cities.sf)

Cities.sf %>%
  dplyr::arrange(desc(rr)) 
```

To illustrate the results we create a graph using the `geom_lollipop()` function from the {ggalt} package. We also use the package {scales} that allows for labels in percent.
```{r}
library(ggalt)
library(scales)

ggplot(Cities.sf, aes(x = reorder(city, rr), y = rr)) +
    geom_lollipop(point.colour = "steelblue", point.size = 3) +
    scale_y_continuous(labels = percent, limits = c(0, .0625)) +
    coord_flip() +
    labs(x = "", y = NULL, 
         title = "Chance that a tornado will cause at least EF3 damage",
         subtitle = "Cities in Texas with a 2010 population > 100,000",
         caption = "Data from SPC") +
  theme_minimal()
```

### Another example: Florida wildfires

Given a wildfire in Florida what is the probability that it was started by lightning? We import wildfire data (available here: https://www.fs.usda.gov/rds/archive/catalog/RDS-2013-0009.4) as a simple feature data frame and transform the native CRS to a Florida GDL Albers (EPSG 3086). Each row is a unique fire and the data spans the period 1992-2015.
```{r}
if(!"FL_Fires" %in% list.files()){
  download.file("http://myweb.fsu.edu/jelsner/temp/data/FL_Fires.zip",
                "FL_Fires.zip")
unzip("FL_Fires.zip")
}

FL_Fires.sf <- st_read(dsn = "FL_Fires") %>%
  st_transform(crs = 3086)
dim(FL_Fires.sf)
```

There are over 90K rows and 38 variables. To make things run faster in this lesson, we analyze a sample of the rows. We do this with the `sample_n()` function from the {tidyverse} set of package where the argument `size =` specifies the number of rows to choose at random. We save the sample of events to the object `FL_FiresS.sf`. Here we set the random number seed so that the set of rows chosen will be the same for everyone running the code.
```{r}
set.seed(78732)

FL_FiresS.sf <- FL_Fires.sf %>%
  sample_n(size = 2000)

dim(FL_FiresS.sf)
```

The result is a simple feature data frame with exactly 2000 rows.

The character variable `STAT_CAU_1` indicates the cause.
```{r}
FL_FiresS.sf$STAT_CAU_1 %>%
  table()
```

There are 13 causes (listed in alphabetical order) with different occurrence frequencies. Lightning is the most common.

To spatially analyze these data as point pattern events, we convert the simple feature data to a `ppp` object over a window defined by the state boundaries with the cause as a factor mark.
```{r}
F.ppp <- FL_FiresS.sf["STAT_CAU_1"] %>%
  as_Spatial() %>%
  as.ppp()

W <- us_states(states = "Florida") %>%
  st_transform(crs = st_crs(FL_Fires.sf)) %>%
  as_Spatial() %>%
  as.owin()

F.ppp <- F.ppp[W]

marks(F.ppp) <- as.factor(marks(F.ppp)) # make the character marks factor marks

summary(F.ppp)
```

Output from the `summary()` method displays a table of frequency by type including the proportion and the average spatial intensity (per square meters). 

The probability that a wildfire is caused by lightning is about 25% (`proportion` column of the frequency versus type table). How does this probability vary over the state?

Note that the window contains four separate polygons to capture the main boundary (`polygon 4`) and the Florida Keys.
```{r}
plot(W)
```

First we split the object `F.ppp` on whether or not the cause was lightning and then merge the two event types and assign names `NL` and `L` as marks.
```{r}
L.ppp <- F.ppp[F.ppp$marks == "Lightning"] %>%
  unmark()

NL.ppp <- F.ppp[F.ppp$marks != "Lightning"] %>%
  unmark()

LNL.ppp <- superimpose(NL = NL.ppp, 
                       L = L.ppp)

summary(LNL.ppp)
```

Now the two types are `NL` and `L` composing 75% and 25% of all wildfire events.

The function `relrisk()` computes the spatially-varying probability of a case (event type), (i.e. the probability $p(u)$ that a point at location $u$ will be a case).

Here we compute the relative risk on a 256 by 256 grid.
```{r}
wfr <- relrisk(LNL.ppp, 
               dimyx = c(256, 256))
```

We map the raster as before first converting the image object to a raster object and assigning the CRS with the `crs()` function from the {raster} package. We add the county borders for geographic reference.
```{r}
wfr.r <- raster(wfr)

crs(wfr.r) <- st_crs(FL_Fires.sf)$proj4string

FL.sf <- us_counties(state = "FL") %>%
  st_transform(crs = st_crs(FL_Fires.sf))

library(tmap)

tm_shape(wfr.r) +
  tm_raster(title = "Probability") +
tm_shape(FL.sf) +
  tm_borders(col = "gray70") +
tm_legend(position = c("left", "center") ) +
tm_layout(main.title = "Chance a wildfire was started by lightning (1992-2015)",
          main.title.size = 1) +
tm_compass(position = c("right", "top")) +
tm_credits(text = "Data source: Karen Short https://doi.org/10.2737/RDS-2013-0009.4",
           position = c("left", "bottom")) 
```

## Estimating second order properties of point pattern data

The first-order spatial intensity function describes the distribution of the event locations on a scale across the domain (trend and/or covariate terms). 

Clustering is a second-order property of point pattern data. It answers the question: is the probability of an event in the proximity of another event higher than expected by chance? 

Think about the location of trees where the tree's seed dispersal leads to a greater likelihood of another tree nearby.

Let $r$ be the distance between two event locations or the distance between an event and an arbitrary point within the domain of a spatial point pattern data set, then functions to describe clustering are:

The nearest neighbor distance function $G(r)$: The cumulative distribution of the distances from an event to the nearest other event (event-to-event function). It summarizes the distances between nearest neighbors.

The empty space function $F(r)$: The cumulative distribution of the distances from a point in the domain to the nearest event (point-to-event function). It summarizes the distance gaps between events (lacunarity--amount of gappiness).

The reduced second moment function (Ripley $K$) $K(r)$: Defined such that $\lambda \times K(r)$ is the expected number of additional events within a distance $r$ of an event, where $\lambda$ is the average intensity of the events. It is a measure of the spatial autocorrelation among the events.

To help evaluate clustering estimates of $G$, $F$, and $K$ computed on point pattern data (empirical estimates) are compared to theoretical curves assuming a homogeneous Poisson process. These theoretical curves are well defined for homogeneous point patterns (CSR--complete spatial randomness). A deviation of the empirical estimate from the theoretical curve is evidence against CSR. 

The theoretical functions assuming a homogeneous Poisson process are:
$K(r) = \pi r^2$
$F(r) = G(r) = 1 - \exp(-\lambda \pi r^2)$

Where $\lambda$ is the average spatial intensity.

Recall the Swedish pine saplings data from the {spatstat} package.
```{r}
data(swedishpines)
class(swedishpines)
```

Here we first assign the data to an object called `SP`. We then compute the nearest neighbor distance function using `Gest()` and assign the output of this computation to an object called `G`. List the output.
```{r}
SP <- swedishpines
( G <- Gest(SP) )
```

Output includes the distance `r` and estimates for the cumulative event-to-event distances. With many events the different estimates (e.g., Kaplan-Meier, border corrected, etc) will be similiar.

The output also includes theoretical values under the assumption of a homogeneous Poisson process (read: CSR). The `plot()` method makes it easy to compare the estimates against CSR.
```{r}
plot(G)
abline(h = c(.2, .5), 
       col = "black",
       lty = 2)
```

The graph shows $G$ as a function of distance $r$ starting at zero distance. We add two horizontal lines to help with interpretation. 

The horizontal dashed line at $G$ = .2 intersects the black line at a distance of .5 meter ($r$) [unit of length is .1 meters]. This means that 20% of the pairwise distances between saplings are within .5 meter. The horizontal dashed line at $G$ = .5 intersects the black line at .8 meters indicating that 50% of the pairwise distances are within .8 meter.

The blue dashed-dotted line is the theoretical homogeneous Poisson process model with the same intensity as the Swedish pines. We see that for a given radius, the actual value of $G$ is less than the theoretical value of $G$. There are fewer saplings in the vicinity of other saplings than expected by chance. 

For example, if the saplings were arranged under the model of CRS we would expect 20% of the pairwise distances to be within .3 meter and 50% of them to be within .55 meter.

For publication we convert the object `G` to a data frame and then use {ggplot2} functions. Here we do this then remove estimates for distances greater than 1.1 meter and convert the units to meters.
```{r}
G.df <- as.data.frame(G) %>%
  filter(r < 11) %>%
  mutate(r = r * .1)

ggplot(G.df, aes(x = r, y = km)) +
  geom_line() +
  geom_line(aes(y = theo), color = "blue") +
  geom_hline(yintercept = c(.2, .5), lty = 'dashed') +
  xlab("Distance (m)") +  ylab("G(r): Cumulative % of distances within a distance r of another event") +
  theme_minimal()
```