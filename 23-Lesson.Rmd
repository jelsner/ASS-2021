---
title: "Lesson 23"
author: "James B. Elsner"
date: "March 31, 2021"
output:
  pdf_document: default
  html_document: null
editor_options:
  chunk_output_type: console
---

**"Rarely is anyone thanked for the work they did to prevent the disaster that didn't happen."** – Mikko Hypponen

Last time we were introduced to the mathematics, terminology and procedures of spatial statistical interpolation. Today we consider some concrete examples using functions from the {geoR} package.

The work flow for spatial interpolation includes:

Step 1: Examine the observed data for trends, check for normality
Step 2: Compute empirical variograms
Step 3: Fit a variogram model to the empirical variogram
Step 4: Create the kriged surface. Kriging uses the variogram model and the observed data to estimate data values at any location of interest. The kriged estimates are a weighted average of the neighborhood values where the weights are taken from the variogram model. 

## Interpolating spatial data using functions from the {geoR} package

The {geoR} package was one of the first comprehensive packages devoted spatial statistical interpolation. The functions are helpful in learning the work flow.

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).
```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
df <- data.frame(sx, sy, zobs)

library(ggplot2)
library(tidyverse)

ggplot(data = df, 
       mapping = aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

The functions in the {geoR} package work only with objects of class `geodata`. We need to first convert the data frame of observations and locations into a `geodata` object with the `as.geodata()` function. The default for the function is to assume that the first two columns of the data frame contain the coordinates and the third column contains the observed data values. This is the way we constructed the data frame so we can use the defaults.
```{r}
library(geoR)

gdf <- as.geodata(df)
str(gdf)
```

An object of class `geodata` contains two lists: the coordinates of the locations and the observed values as `data`. It may contain other elements like coordinate boundaries but unlike working with `ppp` objects with functions from the {spatstat} package a boundary defining the domain is not required.

Starting with a regular data frame we can specify the coordinate and data columns by column number or by column name. For example, if the location coordinates are in columns five and six and the observed values are in column eight then use `coords.col = c(5, 6)` and `data.col = 8`.

With the class set to `geodata` methods like `summary` and `plot` provide attribute and spatial information about the observations. For example the `summary()` method outputs the number of observations, a summary of the coordinates and a summary of the observed values.
```{r}
summary(gdf)
```

We access the coordinates and the data using the `$` operator.
```{r}
gdf$coords
gdf$data
```

The `plot()` method produces a four panel plot.
```{r}
plot(gdf)
```

The upper left panel is a map with the locations of the observations given by colors and symbols according to their values. Blue circles have the lowest values and red crosses have the highest values. We can detect a slight upward trend in values from northwest to southeast.

This trend is decomposed in the upper right and lower left panels. The upper-right panel shows the north-south (`Y Coord`) coordinate plotted against the data values. As the data values increase to the right the north-south coordinate decreases (smaller data values are in the north). The lower-left panel shows the data plotted against the east-west (`X Coord`). Moving from west to east we see the data values tend to increase.

The lower right panel is the aspatial distribution of the data values shown with a histogram, density curve, and a rug plot.

To plot the trend in the east-west direction, type
```{r}
ggplot(df, aes(x = sx, y = zobs)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()
```

The geodata values can be plotted with the trend removed. The `trend = "1st"` refers to a first-order (linear) trend model.
```{r}
plot(gdf, 
     trend = "1st")
```

Now the plots show the residuals from the first-order trend model. The residuals are the observed minus the model predicted.

As another example, consider the data frame called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain. The coordinates are given in the columns labeled `x` and `y` and the heights are labeled `z`.
```{r}
library(MASS)
data(topo)

head(topo)
```

We turn this data frame into a `geodata` object with the `as.geodata()` function and then use the plot method.
```{r}
topo.gdf <- as.geodata(topo,
                       coord.col = c("x", "y"),
                       data.col = "z")
plot(topo.gdf)
```

We note the lowest heights (blue circles) are to the north and the highest heights (red crosses) are to the south. We also see that the distribution of heights is not symmetric. 

Because of this trend we examine the residuals from a first-order trend model.
```{r}
plot(topo.gdf, 
     trend = "1st")
```

The north-south trend is removed and the residuals are somewhat more symmetric. But there appears to be a non-linear trend in the east-west direction (lower-left panel).

So we examine the residuals from a second-order trend model.
```{r}
plot(topo.gdf, 
     trend = "2nd")
```

The residuals from a second-order (polynomial) fit are symmetric and the trends are gone.

The residuals appear to show spatial correlations. We see groupings of low and high residuals. 

Said another way, we can remove the large-scale non-linear trend in the topographic heights with a polynomial surface but we can make better interpolations of the height field by including a variogram function. The variogram function gets estimated on the residuals.

## Estimating the empirical variogram

Consider the `geodata` object `s100` from the {geoR} package. The `points.geodata()` function produces a bubble plot showing the locations of the observations and the relative magnitude of the `z` variable.
```{r}
data(s100)
class(s100)

points.geodata(s100) 
```

The `variog()` function computes an empirical variogram for the `s100` object. Here we save the empirical variogram in the object `s100.v`.
```{r}
s100.v <- s100 %>%
  variog()
str(s100.v)
```

Information in the variogram object includes the lag distances (`u`), the values of the variogram at those distances (`v`), the number of distance pairs used to compute the variogram values at each lag (`n`), the standard deviation of the variogram values, and the coefficients of the trend surface (with no trend, the value is the overall mean of the data) (`beta.ols`) among other information.

Note: Mathematically lag distance is denoted with $h$ but in the {geoR} package it is given in the vector `u`.

Verify the mean value.
```{r}
mean(s100$data)
```

Plot the variogram.
```{r}
s100.v %>%
  plot()
```

The semivariance ($\gamma(u)$) is plotted against lag distance ($u$). Values increase with increasing lag until a lag distance of about 1. 

At large lag distances there are fewer estimates so the values have greater variance. A model for the semivariance is fit only for the the increasing portion of the graph.

Here we use the `variog()` function to compute an empirical variogram of the `topo.gdf` object and then plot it using the `plot()` method including proper axis labels.
```{r}
topo.v <- topo.gdf %>%
  variog()

plot(topo.v, xlab = "Lag distance", 
     ylab = expression(paste(gamma, "(u) [ft", {}^2, "]")),
     las = 1, pch = 16)
```

The variogram values have units of square feet and are calculated using point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag so the variogram values are less precise at short and large distances.

As an aside, note how we use the `expression()` and `paste()` functions to include symbols as part of the axis label.

Plot the number of point pairs used as a function of lag distance.
```{r}
ggplot(data = data.frame(u = topo.v$u, n = topo.v$n), 
       mapping = aes(x = u, y = n)) +
  geom_point() +
  xlab("Lag distance") + ylab("Number of observation pairs") +
  theme_minimal()
```

## Interpolating head heights in the Wolfcamp aquifer

Let's look at another example. Some years ago there were three nuclear waste repository sites being proposed in Nevada, Texas, and Washington. The site needs to be larger enough for more than 68,000 high-level waste containers placed underground, about 9 m (~30 feet) apart, in trenches surrounded by salt. In July of 2002 the Congress approved [Yucca Mountain](https://en.wikipedia.org/wiki/Yucca_Mountain_nuclear_waste_repository), Nevada, as the nation’s first long-term geological repository for spent nuclear fuel and high-level radioactive waste.

The site must isolate the waste for 10,000 years. Leaks could occur, however, or radioactive heat could cause tiny quantities of water in the salt to migrate toward the heat until eventually each canister is surrounded by 22.5 liters of water (~6 gallons). A chemical reaction of salt and water can create hydrochloric acid that might corrode the canisters. The piezometric-head data at the site were obtained by drilling a narrow pipe into the aquifer and letting water seeks its own level in the pipe (piezometer).

The head measurements, given in units of feet above sea level, are from drill stem tests and indicate the total energy of the water in units of height. The higher the head height, the greater the potential energy.  Water flows away from areas of high potential energy so aquifer discharge is proportional to the gradient of the piezometric head.

The data are in `wolfcamp.csv` on my website.

Step 1: Examine the observed data for trends, check for normality

Import the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read_csv(L)
```

Create a simple feature data frame and then map the locations and head heights.
```{r}
library(sf)
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(tmap)
#tmap_mode("view")

tm_shape(wca.sf) +
  tm_dots("head")
```

Convert the data frame to a `geodata` object. Note there is no method to do this from a simple feature.
```{r}
wca.gdf <- as.geodata(wca.df, 
                      coords.col = c("lon", "lat"), 
                      data.col = "head")
```

Note we can also specify the coordinates and data by column names (`coords.col = c("lon", "lat")`, `data.col = "head"`).

Find the duplicate location(s).
```{r}
dup.coords(wca.gdf)
```

The locations are the same, but the data values are different. This may represent an error or it may represent multiple measurements at this location.  

We can average the values or we can exclude a row. Here we remove the row 30 observation.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = c("lon", "lat"), 
                      data.col = "head")
summary(wca.gdf)
```

There are 84 well sites. The spatial bounding box is given under coordinates summary. The minimum distance between sites is .01 degrees and the maximum distance is 4.6 degrees.

The data values are summarized. The values are piezometric head heights in units of feet.

The `plot()` method for `geodata` objects provides a panel of plots with information about the data useful for modeling them.
```{r}
plot(wca.gdf)
```

The upper left panel is a graph of the observed locations with symbols reflecting quartiles of the observed piezometric head heights. There is a clear trend in the data with the highest potential energy over the southwest (red crosses) and lowest over the northeast (blue circles). 

The nature of this trend can be seen in the upper right and lower left panels. The upper right panel plots the data against the y coordinate and the lower left panel plots the data against the x coordinate. Both graphs indicate a linear trend.

A histogram of the head heights is shown in the lower right panel. The rug locates the values along the data axis. The data are bi-modal and skewed to the right.

Repeat the plot after removing the 1st order trend. What happens?
```{r}
plot(wca.gdf, 
     trend = "1st")
```

With the trend removed the variation of values appears to be roughly symmetric and the high and low values are mixed. However, there is some spatial grouping to the residuals.

Step 2: Compute empirical variograms

We noted the maximum distance between any two locations is 4.6 degrees. It is a good idea to plot the variogram values for distances only between 0 and about 1/2 the maximum distance.

Since there is a linear trend in the data over the spatial domain it is removed (trend argument) before computing the variogram values.
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
```

Here we see an increase in the variance with lag distance until about one degree, then the values fluctuate about a variance of about 41000 (ft$^2$).

What does the variogram look like if we do not first remove the trend?

This continuously increasing set of variances with little fluctuation about a best fit curve indicates a trend in the data that must be removed before the variogram is modeled. 

There are two sources of variation in the field values: trend and spatial correlation. Trends are modeled with smooth curves and correlations are modeled with the variogram.

The `variog()` function has options for specifying a classical or modulus estimator. By default the function uses the classical estimator. To override this, include the `estimator.type = "modulus"` argument. The modulus estimator of the variogram is more resistant to outliers in the data.

Compare the classical with the modulus variograms for the Wolfcamp aquifer data.
```{r}
par(mfrow = c(1, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3), 
     main = "Classical")
plot(variog(wca.gdf, trend  ="1st", 
            max.dist = 2.3, 
            estimator.type = "modulus"),
     main = "Modulus")
```

In this case the two variograms are nearly identical.

We can examine the makeup of the variogram in some detail with a 'variogram cloud'. Consider again the simulated data set `s100`.
```{r}
par(mfrow = c(1, 1))
plot(s100)
```

There appears to be a trend in the east-west direction with higher values to the east. This is seen clearly in the bottom left panel of the plot.

We remove this first-order trend and plot again.
```{r}
plot(s100, 
     trend = "1st")
```

We can see that the trend is adequately modeled with a 1st-order surface. Residuals appear symmetric about zero. 

A summary of the data indicates the maximum lag distance of 1.3 so the `max.dist` argument is set to .6. 
```{r}
summary(s100)
```

Here we specify the eleven variogram estimates between 0 and .6 using the `uvec` argument.
```{r}
plot(variog(s100, 
            trend = "1st", 
            uvec = 11, 
            max.dist = .6))
```

We 'decompose' the variogram estimates with the `option = "cloud"`.
```{r}
c1 <- variog(s100, trend = "1st", 
             option = "cloud", 
             max.dist = .6)
plot(c1)
```

Each point on the plot is the difference (absolute value) between two observed values (y-axis) a lag distance apart (x-axis). There are 100 observations so there are 100 * 99/2 = 4950 possible points on this plot. Actually here only 3131 since we limit the distance to less than .6 degrees.

There is a tendency for more large differences as the lag increases but most differences are small for a given lag. The variability in observational differences is quite large especially for longer lag distances.

The variogram is the sum of the squared differences as a function of lag distance grouped by a lag tolerance. Here we indirectly specify the lag tolerance with the `uvec` argument and compare the variogram with the variogram cloud.
```{r}
par(mfrow = c(1, 2))
v1 <- variog(s100, 
             trend = "1st", 
             uvec = seq(0, .6, l = 11))
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
plot(v1); plot(c1)
```

Note the difference in the scale on the y-axis between the variogram cloud and the variogram is a factor of 10.

The individual variances can be grouped into lag distance classes (bins) and displayed with a box plot. This gives an idea of the general shape the variogram model should take.
```{r}
par(mfrow = c(1, 1))
b1 <- variog(s100, 
             trend = "1st", 
             max.dist = .6, 
             bin.cloud = TRUE, 
             estimator.type = "classical")
plot(b1, bin.cloud = TRUE)
```

The box plot summarizes the squared differences for a given lag (and lag tolerance) with the median. There is a box plot for each lag but the lag distance is given as a bin number. 

We can use this information to anticipate the type of variogram model, especially when combined with a local smoothed curve from a `geom_smooth()` layer.
```{r}
ggplot(data = data.frame(u = b1$u, v = b1$v), 
       mapping = aes(u, v)) + 
  geom_point() + 
  geom_smooth() +
  scale_y_continuous(limits = c(0, .6)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)")
```

The blue line is a least-squares regression smoother through the variogram estimates. The fact that it is not a flat horizontal line indicates spatial autocorrelation in the residuals that is separate from the first-order trend. The shape of the blue line gives an idea of the type of variogram family of models we should consider.

Now we can guess at a family for the variogram model and eyeball the parameters. Recall the plot from the last lesson.
```{r}
s100.v <- s100 %>%
  variog() 
df <- data.frame(h = s100.v$u[1:10], 
                 v = s100.v$v[1:10])

ggplot(data = df,
       mapping = aes(x = h, y = v)) +
  geom_point(size = 2) +
  scale_y_continuous(limits = c(0, 1.15), breaks = seq(0, 1.2, .2)) +
  scale_x_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = c(.9, .15), color = "red") +
  geom_vline(xintercept = .6, color = "red") +
  xlab("Lag distance (h)") + ylab(expression(paste(gamma,"(h)"))) +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = .15,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .06, y = .12, label = "nugget")) +
  geom_segment(aes(x = 0, y = .15, xend = 0, yend = .9,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .1, y = .87, label = "sill (partial sill)")) +
  geom_segment(aes(x = 0, y = .93, xend = .6, yend = .93,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .55, y = .97, label = "range")) +
  theme_minimal()
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.

Directional variograms

The assumption of isotropy implies the same spatial autocorrelation function in all directions.

We compute variograms using observational pairs located along the same orientation to examine this assumption. Instead of considering all observational pairs within a lag distance $h$ and lag tolerance $\delta h$, we consider only pairs within a wedge-shaped segment of this annulus (a ring bounded by two concentric circles).

This is done with the `variog()` function and specifying a `direction =` argument in units of radians (fractions of $\pi$). We also need to specify the angle tolerance.

For example to compute and plot the variogram for the direction of 45 degrees (NE to SW) with a default tolerance of 22.5 degrees (default), type
```{r}
v45 <- variog(wca.gdf, 
              trend = "1st", 
              max.dist = 2.3, 
              direction = pi/4)
plot(v45)
```

Repeat for the other three quadrants (increments of $\pi$/4).
```{r}
par(mfrow = c(2, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 0),
     main = expression(0 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/4),
     main = expression(45 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = pi/2),
     main = expression(90 * degree))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3, direction = 3 * pi/4),
     main = expression(135 * degree))
```

The variograms appear similar but it's difficult to tell since the vertical scales are not all the same. The `variog4()` function makes the comparison easier by plotting them on the same graph. It works like `variog()` but takes a vector of directions. The default directions are 0, 45, 90, and 135 degrees.
```{r}
par(mfrow = c(1, 1))
v4 <- variog4(wca.gdf, 
              trend = "1st",
              max.dist = 2.3)
plot(v4)
```

This plot makes it clear that the assumption of isotropy is reasonable.

Step 3: Fit a variogram model to the empirical variogram

Next we fit a curve through the set of points that make up the empirical variogram. The curve is called the variogram model. 

Since the `s100` data were generated using a variogram model, we already know the answer. The model is exponential with a sill (partial) of 1, a range of .3, and a nugget of 0.

Plot the empirical variogram values and overlay the curve corresponding to the exponential model with the `lines.variomodel()` function. The sill and range parameters are given in order using the `cov.pars` argument.
```{r}
plot(variog(s100, uvec = seq(0, 1, l = 21)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), nugget = 0, 
                 max.dist = 1.2, lwd = 2, col = "red")
```

Note that the practical range for the exponential model is the distance along the horizontal axis at which the curve reaches 95% of the sill which is 3 times the range specified in the `cov.pars` argument.

The `uvec =` argument allows control over the range of distance values and the number of variogram estimates. We specify the maximum distance in the `lines.variomodel()` function.

With observed data we don't know the model so we estimate the parameters using information from the empirical variogram.

* By eye: Trial and error over several models and parameter values. The `lines.variomodel()` function can help.  
* By least squares fit:  Using ordinary least squares (OLS) or weighted least squares (WLS) methods available through the `variofit()` function.  
* By maximum likelihood methods: Options for maximum likelihood (ML) and restricted maximum likelihood (REML) are available through the `likfit()` function.  

Try various curves and choosing the one that looks the best. Here we plot three models on top of the `s100` empirical variogram.
```{r}
plot(variog(s100, max.dist = 1))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), 
                 nug = 0, max.dist = 1)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .5),
                 nug = .1, max.dist = 1, 
                 col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(.8, .8),
                 nug = .1, max.dist = 1, 
                 col = "blue")
```

The black line is the model used to generate the `s100` dataset. The red line is based on the same exponential (exp) function but has a range of .5 and a nugget of .1. The blue line is based on a spherical function with a sill and range of .8 and a nugget of .1. All three models fit the points reasonably well. This is important: In practice the choice often makes little difference in the quality of the spatial interpolation.