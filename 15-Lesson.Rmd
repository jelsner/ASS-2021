---
title: "Lesson 15"
author: "James B. Elsner"
date: "March 1, 2021"
output:
  pdf_document: default
  html_document: null
editor_options:
  chunk_output_type: console
---

**"Give someone a program, you frustrate them for a day; teach them how to program, you frustrate them for a lifetime."** - David Leinweber

Programming approach to exposing you to spatial statistics. I want to demystify the process and give you confidence that you can analyze and fit such models. I do believe that investment in honing your programming skills will pay dividends. But, in taking this approach, I don't want to give you the false impression that statisticians have the answers. A _working_ knowledge of the model fitting process needs to be combined with a good understanding of the context in which you are working.

## Spatial data as point patterns

For the next several lessons we turn our attention to analyzing and modeling point pattern data.

We somewhat naturally seek to find patterns in a collection of events. The pattern that tends to catch our attention quickly is the grouping of events across space. Stars in the night sky as constellations.  A collection of events in a particular region begs for an explanation. Why do events occur more often in this particular region and not somewhere else?

Consider Kansas tornado reports. Let the start position of each tornado be an event location. And let the damage rating provide a mark for the event. Here we consider only events since 2007 with marks of 1, 2, 3, 4, and 5. Import and filter the data.
```{r}
library(tidyverse)
library(sf)

Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
             filter(st == "KS", 
                    yr >= 2007,
                    mag > 0) 
```

Next create a map using the functions in the {tmap} package. The state border is obtained as a simple feature data frame. The polygon geometry is plotted first with `tm_borders()` then the event locations are plotted with the `tm_bubbles()` and `size = "mag"`.
```{r}
library(tmap) 
library(USAboundaries)

KS.sf <- us_states(states = "Kansas")

tm_shape(KS.sf) +
   tm_borders(col = "grey70") +
tm_shape(Torn.sf) +
   tm_bubbles(size = "mag", 
              col = "red",
              alpha = .4,
              title.size = "EF Rating") +
tm_layout(legend.position = c("left", "top"),
           legend.outside = TRUE)
```

Based on this display of tornado genesis locations we ask: (1) Are certain areas of the state more likely to get a tornado? (2) Do tornadoes tend to cluster? (3) Are there places in the state that are safe from tornadoes?

These questions are similar but not identical. We explore these canonical questions about point pattern data in the next few lessons.

We need some definitions. 

* Event: An occurrence of interest (e.g., tornado, accident, wildfire). 
* Event location: Location of event (e.g., genesis latitude/longitude).
* Point: Any location in the study area where an event _could_ occur. Note: Event location is a particular point where an event _did_ occur. Example: trees in a forest with a river running through it
* Point pattern data: A collection of observed (or simulated) event locations together with a domain of interest.
* Domain: Study area that is often defined by data availability (e.g., state or county boundary) or by the extent of the events.
* Complete spatial randomness: Or CSR (not to be confused with CRS--coordinate reference system) defines the situation where an event has an equal chance of occurring at any point in the domain regardless of other nearby events. In this case we say they event locations have a uniform probability distribution (uniformly distributed) across space. Note: uniform chance does not mean that the events have an ordered pattern (e.g., trees in an orchard).

Consider a set of event locations that are randomly distributed within the unit plane. First we create two vectors containing the x and y coordinates, then we create a data frame that includes the name of the sample, and finally we graph the locations using the `ggplot()` method.
```{r}
x <- runif(n = 50, min = 0, max = 1)
y <- runif(n = 50, min = 0, max = 1)
df1 <- data.frame(x, y, name = "Point Pattern 1")
ggplot(data = df1, 
       mapping = aes(x, y)) +
  geom_point(size = 2)
```

The plot shows one sample from a spatial point pattern process. A spatial point process is a mechanism for producing a set of event locations across space. The pattern of locations produced by the point process is described as CSR. There are groups of event locations and some gaps. 

Let's repeat this process to create three additional samples. First we combine them into a single data frame with the `rbind()` function and then plot a four-panel figure using the `facet_wrap()` function.
```{r}
df2 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 2")
df3 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 3")
df4 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 4")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

Groups of nearby events illustrate that a certain degree of _clustering_ occurs by chance (without cause) making visual assessment of patterns in the data difficult to discern.

Complete spatial randomness sits on a spectrum between regularity and clustered. To illustrate this idea we generate point pattern data that have more regularity than CSR and point pattern data that are more clustered than CSR. Here we generate event locations using the `rMaternI()` and `rMaternClust()` functions from the {spatstat} package.
```{r}
if(!require(spatstat)) install.packages(pkgs = "spatstat", repos = "http://cran.us.r-project.org")
library(spatstat)

m1 <- rMaternI(kappa = 100, r = .02)
df1 <- data.frame(x = m1$x, y = m1$y, name = "Regular Pattern 1")
m2 <- rMaternI(kappa = 100, r = .02)
df2 <- data.frame(x = m2$x, y = m2$y, name = "Regular Pattern 2")
m3 <- rMatClust(kappa = 30, r = .15, mu = 4)
df3 <- data.frame(x = m3$x, y = m3$y, name = "Cluster Pattern 1")
m4 <- rMatClust(kappa = 30, r = .15, mu = 4)
df4 <- data.frame(x = m4$x, y = m4$y, name = "Cluster Pattern 2")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

The difference in the arrangement of event locations between a regular and a cluster process is clear. But the difference in the arrangement of event locations between a CSR and regular process and the difference in the arrangement of event locations between a CSR and cluster process is not.

And spatial scale matters. A set of event locations can be regular on a small scale but clustered on a larger scale.

Probability models for spatial patterns motivate methods for detecting event clustering. A probability model generates a point pattern process. For example, we can think of crime as a point pattern process defined by location and influenced by environmental factors. The probability of a crime occurring at a particular location is the random variable and we can estimate the probability of a crime event at any location given factors that influence crime. 

More formally, a spatial point pattern process is a _stochastic_ (read: statistical) process where event location is the random variable. A sample of the process is a collection of events generated under the probability model.

A spatial point process is said to be _stationary_ if the statistical properties of the events are invariant to translation. This means that the relationship between two events depends only on the relative event locations (not on where the events occur in the domain). Relative location (or spatial lag) refers to distance and orientation of the events relative to one another. 

In the case where the statistical properties are independent of the orientation of event pairs, the process is said to be _isotropic_. 

The properties of stationarity and isotropy allow for replication within a data set. Under the assumption of a stationary process, two event pairs that are separated by the same distance should have the same relatedness. This is analogous to the assumption we make when we define our weights matrix for spatially aggregated data. The assumptions of stationarity and isotropy are starting points for modeling point pattern data. 

The Poisson distribution defines a model for complete spatial randomness (CSR). A point process is said to be 'homogeneous Poisson' under the following two criteria: 

1. The number of events, N, occurring within a finite domain A is a random variable described by a Poisson distribution with mean $\lambda$|A| for some positive constant $\lambda$, with |A| denoting the area of the domain, and 
2. The locations of the N events represent a random sample where each point in A is _equally likely_ to be chosen as an event location.

The first criteria of a Poisson distribution refers to a probability model describing the number of events. It expresses the probability of a given number of events occurring in a fixed interval of space when the events occur with a known constant rate.

The Poisson parameter defines the _intensity_ of the point process. Given a set of events, an estimate for the mean (rate) parameter of the Poisson distribution is given by the number of events divided by the domain area. 

The second criteria ensures the events are scattered about the domain without clustering or regularity.

The procedure to create a homogeneous Poisson point process follows directly from its definition. Step 1: Sample the total number of events from a Poisson distribution with a mean that is proportional to the domain area. Step 2: Place each event within the domain with coordinates given by a _uniform distribution_.

For example, let area |A| = 1, and the rate of occurrence $\lambda$ = 20, then
```{r}
lambda <- 20
N <- rpois(1, lambda)
x <- runif(N)
y <- runif(N)
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point(size = 2) 
```

The set of events represents a sample from a homogeneous Poisson point process. The intensity of the events is specified first then the locations are placed uniformly inside the domain. The domain need not be regular. The actual number of events varies from one realization  to the next.

This point pattern is CSR by construction. However, we are typically in the opposite position. We observe a set of events and we want to know if the events are regular or clustered. Our null hypothesis is CSR and we need a test statistic that will guide our choice. The null models are simple so we can use Monte Carlo methods to generate many samples and compare summary statistics from those samples with our observed data.

In some cases the homogeneous Poisson model is not restrictive enough. This means that we can easily reject the null hypothesis but not learn anything interesting about our data. For example, with health events (locations of people with heart disease) CSR is not an appropriate model because a null hypothesis that incidences are equally likely does not consider that people cluster (locations at risk are not uniform).

Each person has the same risk of heart disease regardless of location, and we expect more cases in areas with more people at risk. Clusters of cases in high population areas violate the CSR but not necessarily the constant risk hypothesis. The constant risk hypothesis requires the intensity of the spatial process be defined as a spatially varying function. That is, we define the intensity as $\lambda(s)$, where $s$ denotes location.

The intensity (density) function is a first-order property of the random process. If intensity varies (significantly) across the domain the data-generating process is said to be heterogeneous. The intensity function describes the expected number of events at any location of the region. Events might be independent of one another, but groups of events appear because of the changing intensity.

## Working with point pattern objects using functions from the {spatstat} package

We will use functions from the {spatstat} package to analyze and model point pattern data. Point pattern data are represented in {spatstat} by an object of class `ppp` (for planar point pattern) which contains the coordinates of the events, optional values attached to the events (called 'marks'), and a description of the domain or 'window' over which the events are observed. See `?ppp.object()` for details.

Spatial statistics computed on a `ppp` object will be somewhat sensitive to the choice of the window (domain), so some thought should go into deciding what window should be used.

As an example, the data `swedishpines` is available in the package as a `ppp` object.
```{r}
library(spatstat)

class(swedishpines)
swedishpines
```

The data is a planar point pattern object with 71 events. Caution: Unfortunately the events in a `ppp` object are called 'points' rather than events. This is in contrast to the theory which was developed using the definition that a point represented a _potential_ event not an _observed_ event.

All the events are contained within a rectangle window of size 9.6 by 10 meters.

There is a `plot()` method for `ppp` objects that provides a quick way to view the data and the window.
```{r}
plot(swedishpines)
```

Events are plotted as open circles inside a window. The plot is labeled with the name of the `ppp` object.

The function `convexhull()` creates a convex hull around the events. Here we add the convex hull to our plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
```

Recall that a convex hull defines the minimum-area convex polygon that contains all the events. 

The domain (window) for analysis and modeling should be somewhat larger than the convex hull. The function `ripras()` computes a spatial domain based on the event locations alone assuming the locations are independent and identically distributed. Here we overlay this polygon on our plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
plot(ripras(swedishpines), 
     add = TRUE, lty = "dotted")
```

The window can have an arbitrary shape. A rectangle, a polygon, a collection of polygons including holes, or a binary image (mask). A window can be stored as a separate object of class `owin`. See `?owin.object()` for details.

Each event may carry information called a 'mark'. A mark can be continuous (e.g. tree height) or discrete (tree species).

A multitype point pattern is one in which the events are marked using a factor (e.g., tree species). The mark values are given in a vector of the same length as the vector of locations. That is, `marks[i]` is the mark attached to the location (`x[i]`, `y[i]`).

Consider the ppp object `demopat` from the {spatstat} package.
```{r}
plot(demopat)
marks(demopat)
```

Here the domain is defined as an irregular concave polygon with a hole. The distinction between inside and outside is important for all spatial statistics computed using the events.

For a multitype pattern (where the marks are factors) we use the `split()` function to separate the point pattern objects by mark type. Consider the Lansing Woods data set (`lansing`) with marks corresponding to tree species.
```{r}
data(lansing)
LW <- lansing

plot(split(LW))
```

## Quantifying the intensity of events

The average intensity of events is defined as the number of events per unit area of the domain. The `summary()` method applied to a `ppp` object gives the average intensity.
```{r}
summary(swedishpines)
```

There are 71 events over a window area (spatial domain) of 9600 square units giving an average intensity of 71/9600 = .0074.

The average intensity might not represent the intensity of events locally. We need a way to describe the expected number of events at any location of the region. Counting the number of events in equal areas is a simple way. The quadrat method divides the domain into a grid of rectangular cells and the number of events in each cell is counted. Quadrat counting is done with the `quadratcount()` function.
```{r}
quadratcount(swedishpines)
```

The default divides the data into a 5 x 5 grid. The event count in each grid cell is produced. To change the default number of cells in x and y directions we use the `nx =` and `ny =` arguments.
```{r}
quadratcount(swedishpines, 
             nx = 2, 
             ny = 3)
```

We plot the counts with the plot method.
```{r}
plot(quadratcount(swedishpines))
plot(swedishpines, pty = 19, col = "red", 
     add = TRUE, main = "")
```

Note that the cell areas will not be equal when the domain boundaries are irregular like with the `demopat` ppp object.
```{r}
plot(quadratcount(demopat))
```

When the number of events is large, hexagon cells are useful. The process is: (1) tessellate the domain by a regular grid of hexagons, (2) count the number of events in each hexagon, and (3) use a color ramp to display the events per hexagon.

Here we generate 20K random values from the standard normal distribution for the x coordinate and repeat for the y coordinate. We then use the `hexbin()` function from the {hexbin} package and specify 10 bins in the x direction to count the number of events in each hexagon and assign the result to the object `hbin`.
```{r}
if(!require(hexbin)) install.packages(pkgs = "hexbin", repos = "http://cran.us.r-project.org")
library(hexbin)

x <- rnorm(20000)
y <- rnorm(20000)
hbin <- hexbin(x, y, xbins = 10) 
str(hbin)
```

The package uses S4 data classes so the output is stored in slots. We use the `plot()` method to make a graph.
```{r}
plot(hbin)
```

Hexagons have symmetric nearest neighbors (there is only rook contiguity). They have the most sides of any polygon that can tessellate the plane. They are generally more efficient than rectangles at covering the events. In other words it takes fewer of them to cover the same number of events. They are visually less biased for displaying densities compared to squares/rectangles.

Here we generate a large number of random events in the two-dimensional plane. Use a normal distribution in the x-direction and a student t-distribution in the y-direction.
```{r}
set.seed(131)
x <- rnorm(7777)
y <- rt(7777, df = 3)
hbin2 <- hexbin(x, y, xbins = 25)
plot(hbin2)
```

The {ggplot2} package has the `stat_binhex()` function so that also can be used for display.
```{r}
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  stat_binhex()
```

Another way to quantify the spatial intensity is with kernel density estimation (KDE). Let $s_i$ be event locations, then an estimate for the intensity of the events at any location is given by
$$
\hat \lambda (s) = \frac{1}{nh}\sum_{i=1}^nK\Big(\frac{s - s_i}{h}\Big)
$$
where $K$ is the kernel (a non-negative function) and $h > 0$ is a smoothing parameter called the bandwidth. Typically the kernel is a Gaussian probability density function.

Here we generate 100 events uniformly on the real number line (one spatial dimension) between 0 and 1 and then use a kernel density estimation to get a continuous function of the intensity. The density estimation is is done using the function `density()` and here we compare increasing bandwidths specified with the `bw =` argument.
```{r}
e <- runif(100)
dd1 <- density(e, bw = .025)
dd2 <- density(e, bw = .05)
dd3 <- density(e, bw = .1)
df <- data.frame(x = c(dd1$x, dd2$x, dd3$x), 
                 y = c(dd1$y, dd2$y, dd3$y),
                bw = c(rep("h = .025", 512), 
                       rep("h = .05", 512),
                       rep("h = .1", 512)))
df2 <- data.frame(x = e, y = 0)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_line() +
  facet_wrap(~ bw, nrow = 3) +
  geom_point(mapping = aes(x, y), 
             data = df2, 
             color = "red")
```

As the bandwidth increases the curve (black line) representing the local intensity becomes smoother. The intensity is estimated at every location, not just at the location of the event. 

The density is a summation of the kernels with one kernel centered on top of each event location. Event locations are marked with a point along the x-axis and the kernel is a Gaussian probability density function. The kernel is placed on each event and the bandwidth specifies the distance between the inflection points of the kernel. The one-dimensional density estimate extends to two (or more) dimensions.

### Example: The distribution of trees in a tropical forest

The object `bei` is a planar point pattern object from the {spatstat} package containing the locations of trees in a tropical rain forest.
```{r}
summary(bei)
```

There are 3604 events (trees) over an area of 500,000 square meters giving an average intensity of the point pattern data of .0073 trees per unit area.

The distribution of trees is not homogeneous as can be seen on a plot.
```{r}
plot(bei)
```

There are localized clusters of trees and large areas with few if any trees.

Elevation and elevation slope are factors associated with tree occurrence.

The point pattern data is accompanied by data (`bei.extra`) on elevation (`elev`) and slope of elevation (`grad`) across the region.
```{r}
plot(bei.extra)
```

These data are stored as `im` (image) objects. 
```{r}
class(bei.extra$elev)
```

The image object contains a list with 10 elements including the matrix of values (`v`).
```{r}
str(bei.extra$elev)
```

Specifying a spatial domain (window) allows us to focus the analysis on a particular region. Suppose we want to model locations of a certain tree type but only for trees located at elevations above 145 meters. The `levelset()` function creates a window from an image object using `thresh =` and `compare =` arguments.
```{r}
W <- levelset(bei.extra$elev, 
              thresh = 145, 
              compare = ">")
class(W)
```

The result is an object of class `owin`. The plot method displays the window as a mask, which is the region in black.
```{r}
plot(W)
```

We subset the `ppp` object by the window using the bracket operator (`[]`). Here we assign the reduced `ppp` object to `beiW` and then make a plot.
```{r}
beiW <- bei[W]
plot(beiW)
```

Now the analysis window is white and the event locations are plotted on top.

As another example we create a window where altitude is lower than 145 m and slope exceeds .1 degrees. In this case we use the `solutionset()` function. 
```{r}
V <- solutionset(bei.extra$elev <= 145 & 
                 bei.extra$grad > .1)
beiV <- bei[V]
plot(beiV)
```

We compute the spatial intensity function over the domain with the `density()` method using the default Gaussian kernel and fixed bandwidth determined by the window size.
```{r}
den <- density(beiV)
plot(den)
```

The units of intensity are events per unit area (here square meters). The intensity values are computed on a grid ($v$) and are returned as a pixel image. 
```{r}
sum(is.na(den$v))
```

There are over 16K of the cells that have a value of `NA`.
