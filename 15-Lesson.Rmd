---
title: "Lesson 15"
author: "James B. Elsner"
date: "March 1, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"Give someone a program, you frustrate them for a day; teach them how to program, you frustrate them for a lifetime."** - David Leinweber

Mechanistic approach to introducing you to spatial statistics analyses and modeling. I want to demystify the process and give you confidence that you can analyze and fit appropriate models. At the expense of 

## Spatial data as point patterns

For the next several lessons we turn our attention to analyzing and modeling point pattern data.

We naturally tend to assign order to things. We seek patterns in a collection of events. Stars in the night sky as constellations. One pattern that tends to catch our attention is the spatial grouping of events. Groups of events in a particular region trigger us to look for an explanation. Why do events occur more often in this particular region?

Consider Kansas tornadoes. Let the genesis of a tornado be an event location and the maximum EF rating provide a mark for the event. We consider only events with marks of 1, 2, 3, 4, and 5.

Import and filter the data.
```{r}
library(tidyverse)
library(sf)

Torn.sf <- st_read(dsn = "1950-2018-torn-initpoint") %>%
             filter(st == "KS", yr >= 2007, mag > 0) 
```

Then create a map using the functions in the {tmap} package. The state border is obtained as a simple feature data frame. The polygon geometry is plotted first with `tm_borders()` then the event locations are plotted with the `tm_bubbles()` and `size = "mag"`.
```{r}
library(tmap) 
library(USAboundaries)

KS.sf <- us_states(states = "Kansas")

tm_shape(KS.sf) +
   tm_borders(col = "grey70") +
tm_shape(Torn.sf) +
   tm_bubbles(size = "mag", 
              col = "red",
              alpha = .4,
              title.size = "EF Rating") +
tm_layout(legend.position = c("left", "top"),
           legend.outside = TRUE)
```

Based on this display of tornado genesis locations as spatial events we ask: (1) Are certain areas of the state more likely to get a tornado? (2) Do tornadoes tend to cluster? (3) Are there places in the state that are safe from tornadoes?

These questions are similar but not identical. We explore these canonical questions about point pattern data in the next few lessons.

We start with some definitions. 

* Event: An occurrence of interest (e.g., tornado, accident, wildfire). 
* Event location: Location of event (e.g., genesis latitude/longitude).
* Point: Any location in the study area where an event could occur. Note: Event location is a particular point where an event did occur.
* Point pattern data: A collection of observed (or simulated) event locations and a spatial domain of interest.
* Domain: Defined by data availability (e.g., state or county boundary) or by the extent of the events.

Complete spatial randomness (CSR; not to be confused with CRS--coordinate reference system) defines the situation where an event has an equal chance of occurring at any point in the domain regardless of other nearby events. In this case we say they event locations have a uniform probability distribution (uniformly distributed) across space. Note: uniform chance does not necessarily mean that the events have an ordered pattern (e.g., trees in an orchard). Some parts of the domain may get lucky (or unlucky).

Consider a set of event locations that are randomly but uniformly distributed within the unit plane.
```{r}
x <- runif(n = 50, min = 0, max = 1)
y <- runif(n = 50, min = 0, max = 1)
df1 <- data.frame(x, y, name = "Point Pattern 1")
ggplot(data = df1, 
       mapping = aes(x, y)) +
  geom_point(size = 2)
```

The plot shows one sample from a spatial point pattern process. A spatial point process is a mechanism for producing a set of event locations across space. The pattern of locations produced by the point process is described as CSR. There are groups of event locations and some gaps. 

Let's repeat to create three additional samples. Here we concatenate them into a single data frame with the `rbind()` function and then plot a four-panel figure using the `facet_wrap()` function.
```{r}
df2 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 2")
df3 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 3")
df4 <- data.frame(x = runif(n = 30, min = 0, max = 1),
                  y = runif(n = 30, min = 0, max = 1),
                  name = "Point Pattern 4")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

Groups of nearby events illustrate that a certain degree of clustering occurs by chance (without cause) making visual assessment of clustering difficult.

Complete spatial randomness sits on a spectrum between regularity and clustered. To illustrate this idea we generate point pattern data that have more regularity than CSR and point pattern data that are more clustered than CSR. 

Event locations are generated using the `rMaternI()` and `rMaternClust()` functions from the {spatstat} package.
```{r}
library(spatstat)

m1 <- rMaternI(kappa = 100, r = .02)
df1 <- data.frame(x = m1$x, y = m1$y, name = "Regular Pattern 1")
m2 <- rMaternI(kappa = 100, r = .02)
df2 <- data.frame(x = m2$x, y = m2$y, name = "Regular Pattern 2")
m3 <- rMatClust(kappa = 30, r = .15, mu = 4)
df3 <- data.frame(x = m3$x, y = m3$y, name = "Cluster Pattern 1")
m4 <- rMatClust(kappa = 30, r = .15, mu = 4)
df4 <- data.frame(x = m4$x, y = m4$y, name = "Cluster Pattern 2")
df <- rbind(df1, df2, df3, df4)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point() +
  facet_wrap(~ name)
```

The difference in the arrangement of event locations between a regular and a cluster process is a bit more clear. But the difference between CSR and either of them is usually not.

And spatial scale matters. A set of event locations can be regular on a small scale but clustered on a larger scale.

Probability models for spatial patterns motivate methods for detecting event clustering. A probability model generates a stochastic process.

For example, we can think of crime as a stochastic process defined by location and influenced by environmental factors. The probability of a crime occurring at a particular location is the random variable and we can estimate the probability of a crime event at any location given factors that influence crime. 

A spatial point process is a stochastic process where event location is the random variable. A sample of the process is a collection of events generated under the probability model.

A spatial point process is said to be _stationary_ if the statistical properties of the events are invariant to translation across the domain. This means that the relationship between two events depends only on the relative event locations (not on where the events occur in the domain). Relative location (or spatial lag) refers to distance and orientation. 

In the case where the statistical properties do not depend on the orientation of event pairs the process is said to be _isotropic_. 

Stationarity and isotropy allow for replication within a data set. Under the assumption that the point pattern is generated from a stationary process, two event pairs that are separated by the same distance should have the same relatedness. This is analogous to the assumption we make when we define our weights matrix for spatially aggregated data. The assumptions of stationarity and isotropy are starting points for modeling point pattern data. 

The Poisson distribution defines a simple model for complete spatial randomness (CSR). A point process is said to be 'homogeneous Poisson' under the following two criteria: 

1. The number of events, N, occurring within a finite domain A is a random variable described by a Poisson distribution with mean $\lambda$|A| for some positive constant $\lambda$, with |A| denoting the area of the domain, and 
2. The locations of the N events represent a random sample where each point in A is _equally likely_ to be chosen as an event location.

The first criteria of a Poisson distribution refers to a probability model describing the number of events. It expresses the probability of a given number of events occurring in a fixed interval of space when the events occur with a known constant rate.

The Poisson parameter defines the _intensity_ of the point process. Given a set of events, an estimate for the mean (rate) parameter of the Poisson distribution is given by the number of events divided by the domain area. 

The second criteria ensures the events are scattered about the domain without clustering or regularity.

The procedure to create a homogeneous Poisson point process follows directly from its definition. Step 1: Sample the total number of events from a Poisson distribution with a mean that is proportional to the domain area. Step 2: Place each event within the domain with coordinates given by a _uniform distribution_.

For example, let area |A| = 1, and the rate of occurrence $\lambda$ = 20, then
```{r}
lambda <- 20
N <- rpois(1, lambda)
x <- runif(N)
y <- runif(N)
df <- data.frame(x, y)
ggplot(data = df, 
       mapping = aes(x, y)) +
  geom_point(size = 2) 
```

The set of events represents a sample from a homogeneous Poisson point process. The intensity of the events is specified first then the locations are placed uniformly inside the domain. The domain need not be regular. The actual number of events varies from one realization  to the next.

By construction this point pattern is CSR. However, we are typically in the opposite position. We observe a set of events and we want to know if the events are regular or clustered. Our null hypothesis is CSR and we need a test statistic that will guide our inference. The null models are simple so we can use Monte Carlo methods to generate many realizations.

In many cases the homogeneous Poisson model as the null hypothesis is not restrictive enough. This means that we can easily reject the null hypothesis but not learn anything interesting about our data. For example, often with health events (locations of people with heart disease) CSR is not an appropriate model because a null hypothesis that incidences are equally likely does not consider that people cluster (locations at risk are not uniform).

Each person has the same risk of heart disease regardless of location, and we expect more cases in areas with more people at risk. Clusters of cases in high population areas violate the CSR but not necessarily the constant risk hypothesis. The constant risk hypothesis requires the intensity of the spatial process be defined as a spatially varying function. That is, we define the intensity as $\lambda(s)$, where $s$ denotes location.

The intensity (density) function is a first-order property of the random process. If intensity varies (significantly) across the domain the process is said to be heterogeneous. The intensity function describes the expected number of events at any location of the region. Events might be independent of one another, but groups of events appear because of the changing intensity.

## Working with point pattern objects using functions from the {spatstat} package

We will use functions from the {spatstat} package to analyze and model point pattern data. Point pattern data are represented in {spatstat} by an object of class `ppp` (for planar point pattern) which contains the coordinates of the events, optional values attached to the events (called 'marks'), and a description of the domain or 'window' over which the events are observed. See `?ppp.object()` for details.

Spatial statistics computed on a `ppp` object will be somewhat sensitive to the choice of the window (domain), so some thought should go into deciding what it should be.

As an example, the data `swedishpines` is available in the package as a `ppp` object.
```{r}
library(spatstat)

class(swedishpines)
swedishpines
```

The data is a planar point pattern object with 71 events. Caution: Unfortunately called the events are called 'points' because as was explained above, the theory was developed using the definition that a point represented a potential event not necessarily an actual event location.

All the events are contained within a rectangle window of size 9.6 by 10 meters.

There is a `plot()` method for `ppp` objects that provides an easy way to view the data and the window.
```{r}
plot(swedishpines)
```

Events are plotted as open circles inside a window. The plot is labeled with the name of the `ppp` object.

The function `convexhull()` creates a convex hull around the events. Here we add the convex hull to our plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
```

Recall that the hull defines the minimum area convex polygon that contains all the events. 

The domain (window) for analysis and modeling should be somewhat larger than the convex hull. The function `ripras()` computes a spatial domain based on the event locations alone assuming the locations are independent and identically distributed. Here we add this polygon to our plot.
```{r}
plot(swedishpines)
plot(convexhull(swedishpines), 
     add = TRUE)
plot(ripras(swedishpines), 
     add = TRUE, lty = "dotted")
```

The window can have an arbitrary shape. A rectangle, a polygon, a collection of polygons including holes, or a binary image (mask). A window can be stored as a separate object of class `owin`. See `?owin.object()` for details.

Each event may carry information called a 'mark'. A mark can be continuous (e.g. tree height) or discrete (tree species).

A multitype point pattern is one in which the events are marked using a factor (e.g., tree species). The mark values are given in a vector of the same length as the vector of locations. That is, `marks[i]` is the mark attached to the point (`x[i]`, `y[i]`).
```{r}
plot(demopat)
marks(demopat)
```

Here the domain is defined as an irregular concave polygon with a hole. The distinction between inside and outside is important for all spatial statistics computed using the events.

For a multitype pattern (where the marks are factors) we use the `split()` function to separate the point pattern objects by mark type. Consider the Lansing Woods data set (`lansing`) with marks corresponding to tree species.
```{r}
data(lansing)
LW <- lansing

plot(split(LW))
```

## Quantifying the varying intensity of a point process

Given a set of events, how do we estimate the intensity function? Quadrat counting is a simple way. The spatial domain is divided into a grid of rectangular cells and the number of events in each cell is counted.

Quadrat counting is done with the `quadratcount()` function.
```{r}
quadratcount(swedishpines)
```

The default divides the data into a 5 x 5 grid. The event count in each grid cell is produced.

To change the default number of cells in x and y directions we use the `nx =` and `ny =` arguments.
```{r}
quadratcount(swedishpines, 
             nx = 2, 
             ny = 3)
```

We can plot the quadrat counts with the plot method.
```{r}
plot(quadratcount(swedishpines))
plot(swedishpines, pty = 19, col = "red", 
     add = TRUE, main = "")
```

Note that the cell areas will not be equal when the domain boundaries are irregular.
```{r}
plot(quadratcount(demopat))
```

When the number of events is large, hexagon cells are useful. The process is: (1) tessellate the domain by a regular grid of hexagons, (2) count the number of events in each hexagon, and (3) use a color ramp to display the events per hexagon.

Here we generate 20K random values from the standard normal distribution for the x coordinate and repeat for the y coordinate. We then use the `hexbin()` function from the {hexbin} package and specify 10 bins in the x direction to count the number of events in each hexagon and assign the result to the object `hbin`.
```{r}
library(hexbin)

x <- rnorm(20000)
y <- rnorm(20000)
hbin <- hexbin(x, y, xbins = 10) 
str(hbin)
```

The package uses S4 data classes so the output is stored in slots.

There is a plot method.
```{r}
plot(hbin)
```

Hexagons have symmetric nearest neighbors (there is only rook contiguity). They have the most sides of any polygon that can tessellate the plane. They are generally more efficient than rectangles at covering the events. In other words it takes fewer of them to cover the same number of events. They are visually less biased for displaying densities compared to squares/rectangles.

As another example, generate a large number of random events in the two-dimensional plane. Use a normal distribution in the x-direction and a student t-distribution in the y-direction.
```{r}
set.seed(131)
x <- rnorm(7777)
y <- rt(7777, df = 3)
hbin2 <- hexbin(x, y, xbins = 25)
plot(hbin2)
```

The {ggplot2} package has the `stat_binhex()` function so that also can be used for display.
```{r}
df <- data.frame(x, y)
ggplot(df, aes(x, y)) +
  stat_binhex()
```

Another way to quantify the spatial intensity of the process is with kernel density. A kernel density estimator is a smoothing function that gives the average number of events at any location in the domain.  

Here we generate 100 events uniformly on the real number line between 0 and 1. The kernel density estimator is applied using different bandwidths.
```{r}
e <- runif(100)
dd1 <- density(e, bw = .025)
dd2 <- density(e, bw = .05)
dd3 <- density(e, bw = .1)
df <- data.frame(x = c(dd1$x, dd2$x, dd3$x), 
                 y = c(dd1$y, dd2$y, dd3$y),
                bw = c(rep("Bandwidth = .025", 512), 
                       rep("Bandwidth = .05", 512),
                       rep("Bandwidth = .1", 512)))
df2 <- data.frame(x = e, y = 0)
ggplot(df, aes(x, y)) +
  geom_line() +
  facet_wrap(~ bw, nrow = 3) +
  geom_point(aes(x, y), data = df2, color = "red")
```

As the bandwidth increases the curve indicated by the black line becomes smoother. The density is estimated at every location on the number line, not just at the location of the event. 

The density is a summation of the kernels with one kernel centered on top of each event location. Event locations are marked with a point along the x-axis and the kernel is a Gaussian (normal) density. It is placed on each event and the bandwidth specifies the distance between the inflection points of the kernel. The one-dimensional density estimate extends to two (or more) dimensions.

### Example 1: The distribution of trees in a tropical forest

The object `bei` is a planar point pattern object with the locations of 3605 trees in a tropical rain forest. The data are part of the {spatstat} package.
```{r}
plot(bei)
```

The point pattern data is accompanied by a data set (`bei.extra`) of elevation (`elev`) and slope of elevation (`grad`) across the region. 
```{r}
plot(bei.extra)
```

These data are given as `im` (image) objects in a list format. 
```{r}
class(bei.extra$elev)
```

An image object contains a list with 10 elements including the matrix of values (`v`).
```{r}
str(bei.extra$elev)
```

The window focuses the analysis on a particular region. Suppose you want to model locations of a certain tree type but only for trees located at elevations above 145 meters. The `levelset()` function creates a window from an image object as follows:
```{r}
W <- levelset(bei.extra$elev, 
              thresh = 145, 
              compare = ">")
class(W)
```

The result is an object of class `owin`. The plot method displays the window which is the region in black.
```{r}
plot(W)
```

We subset the point pattern data by the window using the bracket operator (`[]`) as we do with a data frame.
```{r}
beiW <- bei[W]
plot(beiW)
```

Now the analysis window is white and the event locations are plotted on top.

As another example we create a window where altitude is lower than 145 m and slope exceeds .1 degrees. In this case we use the `solutionset()` function. 
```{r}
V <- solutionset(bei.extra$elev <= 145 & 
                 bei.extra$grad > .1)
beiV <- bei[V]
plot(beiV)
```

We compute the spatial intensity over the domain with the `density()` method using the default Gaussian kernel and fixed bandwidth determined by the window size.
```{r}
den <- density(beiV)
plot(den)
```

The units of intensity are events per unit area (here square meters). The intensity values are computed on a grid ($v$) and are returned as a pixel image. 
```{r}
sum(is.na(den$v))
```

There are over 16K of the cells that have a value of `NA`.
