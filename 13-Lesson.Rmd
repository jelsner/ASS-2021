---
title: "Lesson 13"
author: "James B. Elsner"
date: "February 22, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

## Geographic regression

If the residuals from a statistical model have significant autocorrelation then a spatial modeling approach is called for. One approach is to assume that the relationships between the response variable and the explanatory variables are modified by contextual factors. 

Like with local variants of spatial autocorrelation metrics, which use only neighbors for estimates, we can fit separate regression models for each polygon using only values in neighborhoods. This approach is useful for exploratory analysis (e.g., to show where the explanatory variables are most strongly related to the response variable). This approach is called geographically weighted regression (GWR) or geographic regression.

GWR fits a separate regression model for every location in the dataset. Thus it is not a single model but rather a procedure for fitting a set of models. It fits the models by using the response and explanatory variables only from locations that fall within some prescribed distance (bandwidth). The bandwidth is pre-specified or determined by a cross-validation procedure. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies or health programs.

Let's see how GWR works with an example.

### Example: Southern homicides

The file `south.zip` contains shapefiles with homicide rates and explanatory variables for counties in the southern United States. Download the file from my website and unzip it in your working directory. 

Import the data using the `read_sf()` from the {sf} package. The data have latitude/longitude coordinates but there is no projection so we set the CRS to long-lat with the `st_crs()` function.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/south.zip",
              destfile = "south.zip")
unzip("south.zip")

library(sf)

SH.sf <- st_read(dsn = "south", 
                 layer = "south", 
                 stringsAsFactors = FALSE)
st_crs(SH.sf) <- 4326
names(SH.sf)
```

Each row is a separate U.S. county in the southeast. There are 1412 counties.

We are interested in predicting homicide rates (`HR`) given as the number of homicides per 100,000 people. And we consider five explanatory variables including `RD`: resource deprivation index, `PS`: population structure index, `MA`: marriage age, `DV`: divorce rate, and `UE`: unemployment rate. The two digit number appended to the column names is the census year from the 20th century.

First use the `plot()` method on the `geometry` column to see the extent of the data and the spatial geometries.
```{r}
plot(SH.sf$geometry, col = "gray70")
```

Next we reduce the number of variables in the data frame keeping only the ones of interest using the `select()` function from {dplyr}.
```{r}
library(dplyr)

SH.sf <- SH.sf %>%
  dplyr::select(HR90, RD90, PS90, MA90, DV90, UE90)
```

We create a thematic map of the homcide rates from the 1990 census (`HR90`).
```{r}
library(tmap)

tm_shape(SH.sf) +
  tm_fill("HR90", title = "1990\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

We start with a linear regression model where we regress homicide rate onto resource deprivation, population structure, marriage age, divorce rate, and unemployment rate.
```{r}
model.lm <- lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, 
               data = SH.sf)
summary(model.lm)
```

We see that RD, PS, and DV have a positive relationship to HR while MA and UE have a negative relationship. 

Based on the $p$-values on the coefficients we suspect the model can be simplified by removing marriage age (MA). We check this supposition with the `drop1()` function.
```{r}
drop1(model.lm)
```

The single term delection table shows that when marriage age (`MA90`) is removed from the model the RSS (residual sum of squares) value increases by 35.2 units. This increase is not sufficient to justify the loss in the degrees of freedom by keeping it in the model. This is seen by an AIC value that is lower (4998.7) than the AIC when all terms are retained (4999.7) (see the row labeled `<none>`). 

Recall: The AIC is a way to balance the tradeoff between bias and variance. Choose a model that has the lowest AIC. A model may have too much bias (toward the particular dataset) if it has too many parameters and a model may have too much residual variance if there are too few parameters.

We therefore remove marriage age and refit the mode.
```{r}
model.lm2 <- lm(HR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = SH.sf)
```

The new model (`model.lm2`) can't be simplified further using this criteria.
```{r}
drop1(model.lm2)
```

All the AIC values below the first row are above those in the first row.

Next we map the predicted values after adding them to the simple features data frame. The predicted values from the model object are extracted with the `predict()` method.
```{r}
SH.sf$predLM2 <- predict(model.lm2)
head(cbind(SH.sf$HR90, SH.sf$predLM))
```

The first column printed to the console is the actual homicide rates in the first six counties and the second column printed is the predicted homicide rate from the linear regression model. The predictions do not appear to be very good.

A scatterplot of the observed versus the predicted shows this clearly.
```{r}
library(ggplot2)

ggplot(SH.sf, aes(x = HR90, y = predLM2)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

Since the homicide rates are non-negative, transforming them to logarithms is a good idea.

We create a new column in the `homicides.sf` data from called `logHR90`. Since there are some counties with no homicides we change that to the minimum observed value before taking logarithms. Here we first create a logical vector `x` corresponding to the rows with non-zero homicide rates. We then find the minimum non-zero rate and assign it to `e`. Next we subset on this value for all rates equal to zero and finally we create a new column as the logarithm of the non-zero rates.
```{r}
x <- SH.sf$HR90 != 0
e <- min(SH.sf$HR90[x])
SH.sf$HR90[!x] <- e
SH.sf$logHR90 <- log(SH.sf$HR90)
```

We then fit a model with `logHR90` as our response variable.
```{r}
model.lm3 <- lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                data = SH.sf)
summary(model.lm3)
```

We again compute the predicted values and include them in the data frame as `predLM3`. The predictions are on the natural logarithm scale so we use the exponential function `exp()`. We then create a scatter plot of the observed versus predicted as before.
```{r}
SH.sf$predLM3 <- exp(predict(model.lm3))

ggplot(SH.sf, aes(x = HR90, y = predLM3)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

The range of predicted values is much better.

It is likely that homicide rates are similiar in neighboring counties. It also might be the case that the similarity is statistically explained by the  variables in the model.

So our next step it to test for significant autocorrelation in the model residuals. We create a weights matrix using the functions from the {spdep} package and then use the `lm.morantest()` function.
```{r}
library(spdep)

nbs <- poly2nb(SH.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm3, wts)
```

Moran I is .11 but significant ($p$-value < .001).

Put the residuals on a map. First add the residuals as a column in the simple feature data frame.
```{r}
SH.sf$res3 <- residuals(model.lm3)

tm_shape(SH.sf) +
  tm_fill("res3", title = "Model\nResiduals") +
  tm_layout(legend.outside = TRUE)
```

There are small clusters of counties with positive residuals and other small clusters of negative residuals. Interestingly the pattern of these clusters appears to be different over western and northern areas compared to the deep South.

This suggests that the _relationships_ between homicide rates and the socioeconomic factors might vary across the domain. GWR is a procedure to fit local regression models.

Linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variable(s). Geographic regression might show how this dependency varies by location. It is an exploratory technique intended to indicate where local regression coefficients are different from the global values.

A model is fit at each location. All observations contribute to the fit but they are weighted inversely by their distance to the location. At the shortest distances observations are given the largest weights based on a Gaussian function. The process results in a set of regression coefficients for each observation.

We do this with functions from the {spgwr} package. The geometry information in simple feature data frames is not accessible by functions in this package so we need to create a new S4 spatial data frame.
```{r}
SH.sp <- as(SH.sf, "Spatial")
```

The spatial information in the `SH.sp` is separated from the data frame (attribute table) but accessible by the functions `gwr.sel()` and `gwr()`. The variables remain the same.

We obtain the optimal bandwidth with the `gwr.sel()` function specifying the model and the data object. Since the CRS is geographic we use the argument `longlat = TRUE` to get the distances in kilometers.
```{r}
if(!require(spgwr)) install.packages(pkgs = "spgwr", repos = "http://cran.us.r-project.org")

library(spgwr)

bw <- gwr.sel(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
              data = SH.sp,
              longlat = TRUE)
```

The automatic selection procedure makes an initial guess at the bandwidth distance then fits local regression models in each county using neighbors defined by that distance. A cross-validated (CV) skill score is computed as the root mean square prediction error. The cross-validation procedures successively removes one county from the modeling and that county's homicide rate is predicted. Each county takes turn getting removed.

The selection procedure continues by changing the initial guess at the bandwidth and computing the CV score. If the CV score is higher than wih the initial guess the bandwidth is changed in the other direction. If it is lower than the bandwidth is changed in the same direction. The entire procedure continues until no additional improvement is made to the CV score. This results in a minimum bandwith distance. In this case it is 165.5 km.

The bandwidth is assigned to the object `bw` as a single value.

Unfortunately we cannot use neighborhoods defined by contiquity. But to get a sense of what this bandwidth distance means in terms of the average number of neighbors per county we note that one-half the distance squared times pi is the area captured by the bandwidth.
```{r}
( bwA <- pi * (bw * 1000 /2)^2 ) 
```

In units of square meters. Or 21,519 square kilomenters.

County areas are computed using the `st_area()` function. The average size of the countys and the ratio of the bandwidth area to the average county area is also computed.
```{r}
areas <- st_area(SH.sf)
ctyA <- mean(areas)
bwA/ctyA
```

The ratio indicates that, on average, a neighborhood consists of 13 counties. For comparison, on a raster there are 8 first-order neighboring cells (queen contiguity) and 16 second-order neighboring cells (neighbors of neighbors) or a total of 24 neighbors.

We then use the `gwr()` function to includes the formula, data, and the `bandwith =` argument.
```{r}
model.gwr <- gwr(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                 data = SH.sp, 
                 bandwidth = bw)
```

The model and observed data are assigned to list object with element names extracted with the `names()` function.
```{r}
names(model.gwr)
```

The first element of the list named `SDF` contains the model output as a S4 spatial class data frame. The geometry of the spatial data frame is inherited from the type of data frame specified in the `data = ` argument.

The structure of the S4 spatial class is obtained with the `str()` function and by setting the `max.level` argument to 2.
```{r}
str(model.gwr$SDF, max.level = 2)
```

Here we see there are 5 slots with the first slot being the attribute table labeled `@data`. The dimension of the attribute table is retrieved with the `dim()` function.
```{r}
dim(model.gwr$SDF)
```

There are 1412 rows and 9 columns. Each row corresponds to a county and information about the regression localized to the county is given in the columns. The attribute names are extracted with the `names()` function.
```{r}
names(model.gwr$SDF)
```

They include the sum of the weights `sum.w` (the larger the sum the more often the county was included in the local regressions--favoring smaller counties and ones farther from the borders of the spatial domain), the five regression coefficients (one for each of the 4 explanatory variables and an intercept term), the residual (`gwr.e`), the predicted value (`pred`) and the local goodness-of-fit (`localR2`).

We put the predictions into the `SH.sf` simple feature data frame with the column name `predGWR`.
```{r}
SH.sf$predGWR <- exp(model.gwr$SDF$pred)

tm_shape(SH.sf) +
  tm_fill("predGWR", title = "Predicted\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

The geographic regressions similarly capture the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the linear model. The pattern is also a smoother.

With many more model parameters metrics of predictive skill will favor the geographic regression. For example, the root mean-square-error is lower for GWR.
```{r}
sqrt(sum(residuals(model.lm3)^2))
sqrt(sum(model.gwr$SDF$gwr.e^2))
```

Geographic regression is valuable for generating hypothesis. From the linear model we saw that homicide rates increased with resource deprivation. How does this relationship vary across the South.
```{r}
coef(model.lm3)[2]
range(model.gwr$SDF$RD90)
```

The global regression coefficient is .51 but locally the coefficients range from 0.08 to .98.

Importantly we can map where resource deprevation has the most influence on the response variable.
```{r}
SH.sf$RDcoef <- model.gwr$SDF$RD90

tm_shape(SH.sf) +
  tm_fill("RDcoef", title = "Resource\nDeprivation\nCoefficient", palette = 'Blues') +
  tm_layout(legend.outside = TRUE)
```

All values are above zero, but areas in darker blue indicate where resource deprivation plays a stronger role in explaining homicide rates.

How about the unemployment rate?
```{r}
SH.sf$UEcoef <- model.gwr$SDF$UE90

tm_shape(SH.sf) +
  tm_fill("UEcoef", title = "Unemployment\nCoefficient", palette = 'PiYG') +
  tm_layout(legend.outside = TRUE)
```

While the global coefficient is negative indicating homicide rates tend to be lower in areas with more unemployment, the opposite is the case over much of Texas into Oklahoma.

Where does the model for homicide rates provide the best fit to the data? This is answered with a map of local R squared values (`localR2`).
```{r}
SH.sf$localR2 <- model.gwr$SDF$localR2

tm_shape(SH.sf) +
  tm_fill("localR2", title = "Local\nR Squared", palette = 'Purples') +
  tm_layout(legend.outside = TRUE)
```

When we use a regression model to fit data that vary spatially we are assuming an underlying stationary process. This means we believe the explanatory variables 'provoke' the same statistical response across the entire domain. If this is not the case then it shows up in a map of correlated residuals. One approach to investigate things further is to use geographic regression. Another approach is to use a single spatial regression model.

## Spatial regression

Ordinary regression models fit to spatially aggregated data can lead to improper inference because observations are not independent. Thus it's necessary to check the residuals from an aspatial model for spatial autocorrelation. If the residuals are strongly correlated the model is mis-specified. 

In this case we can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), we can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

### Spatial lag model and spatial error models

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n \times 1$ vector of response variable values, $X$ is a $n$ $\times$ $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n \times 1$ vector of residuals (iid).

Two options exist if the elements of the vector $\varepsilon$ are spatially correlated. The first is to rewrite the model adding a spatial lag term as
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable we compute with the `lag.listw()` function and $\rho$ is Moran I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is motivated by a diffusion process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the spatial signal term and $\beta X$ is called the trend term.

The second option is to rewrite the model by adding a spatial error term as
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$
If $z$ is not observed, the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly we would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another reason for fitting a spatial error model is spatial heterogeneity. Suppose we have multiple observations for each unit. If we want our model to incorporate individual effects we can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since we will have more parameters than observations.

Instead we can treat $a$ as a vector of spatial random effects. We assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

### Lagrange multiplier test for the type of spatial autocorrelation

Let's see how to choose between these two options with an example. Returning to the Columbus crime data, we import the data, fit a linear regression model to statistically explain crime rates using income and housing values. Further we first need to check whether a spatial regression model is warranted by testing whether there is significant spatial autocorrelation in the residuals of the aspatial model.
```{r}
if(!"columbus" %in% list.files()){
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")
}

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")
model.lm <- lm(CRIME ~ INC + HOVAL, 
               data = CC.sf)
CC.sf$residuals <- residuals(model.lm)

nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm, wts)
```

The answer is 'yes' a spatial regression model is warranted.

To help decide between the two spatial regression models described above we run a sequence of statistical tests on our linear model object. The tests are the Lagrange multiplier (LM) tests. LM refers to a technique to determine the coefficients on the explanatory variables while simultaneously determining the coefficient on the spatial regression term. It does this iteratively.

The tests are performed with the `lm.LMtests()` function. The test type is specified as a character string. The tests should be considered in order. Start with the standard version of both the spatial error model and spatial lag model LM tests.

For example, to perform a LM test for the spatial error and spatial lag model on the Columbus crime model we type
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag model terms are significant ($p$-value < .15). Ideally one term is significant and the other is not and we choose the model with the significant term.

Since both are significant we should test again. This time we use the robust forms of the statistics.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest AIC.

### Model fitting

To fit a spatial lag model to the crime data we use the `lagsarlm()` function. The value for $\rho$ is found first using the `optimize()` function and then the $\beta$'s are obtained using generalized least squares. These functions are now in the {spatialreg} package.
```{r}
if(!require(spatialreg)) install.packages(pkgs = "spatialreg", repos = "http://cran.us.r-project.org")

model.lag <- spatialreg::lagsarlm(CRIME ~ INC + HOVAL, 
                      data = CC.sf, 
                      listw = wts)

summary(model.lag)
```