---
title: "Lesson 13"
author: "James B. Elsner"
date: "February 22, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

## Geographic regression

If the residuals from a statistical model have significant autocorrelation then a spatial modeling approach is called for. One approach is to assume that the relationships between the response variable and the explanatory variables are modified by contextual factors. 

Like with local variants of spatial autocorrelation metrics, which use only neighbors for estimates, we can fit separate regression models for each polygon using only values in neighborhoods. This approach is useful for exploratory analysis (e.g., to show where the explanatory variables are most strongly related to the response variable). This approach is called geographically weighted regression (GWR) or geographic regression.

GWR fits a separate regression model for every location in the dataset. Thus it is not a single model but rather a procedure for fitting a set of models. It fits the models by using the response and explanatory variables only from locations that fall within some prescribed distance (bandwidth). The bandwidth is pre-specified or determined by a cross-validation procedure. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies or health programs.

Let's see how GWR works with an example.

### Example: Southern homicides

The file `south.zip` contains shapefiles with homicide rates and explanatory variables for counties in the southern United States. Download the file from my website and unzip it in your working directory. 

Import the data using the `read_sf()` from the {sf} package. The data have latitude/longitude coordinates but there is no projection so we set the CRS to long-lat with the `st_crs()` function.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/south.zip",
              destfile = "south.zip")
unzip("south.zip")

library(sf)

SH.sf <- st_read(dsn = "south", 
                 layer = "south", 
                 stringsAsFactors = FALSE)
st_crs(SH.sf) <- 4326
names(SH.sf)
```

Each row is a separate U.S. county in the southeast. There are 1412 counties.

We are interested in predicting homicide rates (`HR`) given as the number of homicides per 100,000 people. And we consider five explanatory variables including `RD`: resource deprivation index, `PS`: population structure index, `MA`: marriage age, `DV`: divorce rate, and `UE`: unemployment rate. The two digit number appended to the column names is the census year from the 20th century.

First use the `plot()` method on the `geometry` column to see the extent of the data and the spatial geometries.
```{r}
plot(SH.sf$geometry, col = "gray70")
```

Next we reduce the number of variables in the data frame keeping only the ones of interest using the `select()` function from {dplyr}.
```{r}
library(dplyr)

SH.sf <- SH.sf %>%
  dplyr::select(HR90, RD90, PS90, MA90, DV90, UE90)
```

We create a thematic map of the homcide rates from the 1990 census (`HR90`).
```{r}
library(tmap)

tm_shape(SH.sf) +
  tm_fill("HR90", title = "1990\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

We start with a linear regression model where we regress homicide rate onto resource deprivation, population structure, marriage age, divorce rate, and unemployment rate.
```{r}
model.lm <- lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, 
               data = SH.sf)
summary(model.lm)
```

We see that RD, PS, and DV have a positive relationship to HR while MA and UE have a negative relationship. 

Based on the $p$-values on the coefficients we suspect the model can be simplified by removing marriage age (MA). We check this supposition with the `drop1()` function.
```{r}
drop1(model.lm)
```

The single term delection table shows that when marriage age (`MA90`) is removed from the model the RSS (residual sum of squares) value increases by 35.2 units. This increase is not sufficient to justify the loss in the degrees of freedom by keeping it in the model. This is seen by an AIC value that is lower (4998.7) than the AIC when all terms are retained (4999.7) (see the row labeled `<none>`). 

Recall: The AIC is a way to balance the tradeoff between bias and variance. Choose a model that has the lowest AIC. A model may have too much bias (toward the particular dataset) if it has too many parameters and a model may have too much residual variance if there are too few parameters.

We therefore remove marriage age and refit the mode.
```{r}
model.lm2 <- lm(HR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = SH.sf)
```

The new model (`model.lm2`) can't be simplified further using this criteria.
```{r}
drop1(model.lm2)
```

All the AIC values below the first row are above those in the first row.

Next we map the predicted values after adding them to the simple features data frame. The predicted values from the model object are extracted with the `predict()` method.
```{r}
SH.sf$predLM2 <- predict(model.lm2)
head(cbind(SH.sf$HR90, SH.sf$predLM))
```

The first column printed to the console is the actual homicide rates in the first six counties and the second column printed is the predicted homicide rate from the linear regression model. The predictions do not appear to be very good.

A scatterplot of the observed versus the predicted shows this clearly.
```{r}
library(ggplot2)

ggplot(SH.sf, aes(x = HR90, y = predLM2)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

Since the homicide rates are non-negative, transforming them to logarithms is a good idea.

We create a new column in the `homicides.sf` data from called `logHR90`. Since there are some counties with no homicides we change that to the minimum observed value before taking logarithms. Here we first create a logical vector `x` corresponding to the rows with non-zero homicide rates. We then find the minimum non-zero rate and assign it to `e`. Next we subset on this value for all rates equal to zero and finally we create a new column as the logarithm of the non-zero rates.
```{r}
x <- SH.sf$HR90 != 0
e <- min(SH.sf$HR90[x])
SH.sf$HR90[!x] <- e
SH.sf$logHR90 <- log(SH.sf$HR90)
```

We then fit a model with `logHR90` as our response variable.
```{r}
model.lm3 <- lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                data = SH.sf)
summary(model.lm3)
```

We again compute the predicted values and include them in the data frame as `predLM3`. The predictions are on the natural logarithm scale so we use the exponential function `exp()`. We then create a scatter plot of the observed versus predicted as before.
```{r}
SH.sf$predLM3 <- exp(predict(model.lm3))

ggplot(SH.sf, aes(x = HR90, y = predLM3)) +
  geom_point() +
  geom_abline(slope = 1) +
  geom_smooth(method = lm, se = FALSE)
```

The range of predicted values is much better.

It is likely that homicide rates are similiar in neighboring counties. It also might be the case that the similarity is statistically explained by the  variables in the model.

So our next step it to test for significant autocorrelation in the model residuals. We create a weights matrix using the functions from the {spdep} package and then use the `lm.morantest()` function.
```{r}
library(spdep)

nbs <- poly2nb(SH.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm3, wts)
```

Moran I is .11 but significant ($p$-value < .001).

Put the residuals on a map. First add the residuals as a column in the simple feature data frame.
```{r}
SH.sf$res3 <- residuals(model.lm3)

tm_shape(SH.sf) +
  tm_fill("res3", title = "Model\nResiduals") +
  tm_layout(legend.outside = TRUE)
```

There are small clusters of counties with positive residuals and other small clusters of negative residuals. Interestingly the pattern of these clusters appears to be different over western and northern areas compared to the deep South.

This suggests that the _relationships_ between homicide rates and the socioeconomic factors might vary across the domain. GWR is a procedure to fit local regression models.

Linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variable(s). Geographic regression might show how this dependency varies by location. It is an exploratory technique intended to indicate where local regression coefficients are different from the global values.

A model is fit at each location. All observations contribute to the fit but they are weighted inversely by their distance to the location. At the shortest distances observations are given the largest weights based on a Gaussian function. The process results in a set of regression coefficients for each observation.

We do this with functions from the {spgwr} package. The geometry information in simple feature data frames is not accessible by functions in this package so we need to create a new S4 spatial data frame.
```{r}
SH.sp <- as(SH.sf, "Spatial")
```

The spatial information in the `SH.sp` is separated from the data frame (attribute table) but accessible by the functions `gwr.sel()` and `gwr()`. The variables remain the same.

We obtain the optimal bandwidth with the `gwr.sel()` function specifying the model and the data object. Since the CRS is geographic we use the argument `longlat = TRUE` to get the distances in kilometers.
```{r}
library(spgwr)

bw <- gwr.sel(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
              data = SH.sp,
              longlat = TRUE)
```

The automatic selection procedure makes an initial guess at the bandwidth distance then fits local regression models in each county using neighbors defined by that distance. A cross-validated (CV) skill score is computed as the root mean square prediction error. The cross-validation procedures successively removes one county from the modeling and that county's homicide rate is predicted. Each county takes turn getting removed.

The selection procedure continues by changing the initial guess at the bandwidth and computing the CV score. If the CV score is higher than wih the initial guess the bandwidth is changed in the other direction. If it is lower than the bandwidth is changed in the same direction. The entire procedure continues until no additional improvement is made to the CV score. This results in a minimum bandwith distance. In this case it is 165.5 km.

The bandwidth is assigned to the object `bw` as a single value.

Unfortunately we cannot use neighborhoods defined by contiquity. But to get a sense of what this bandwidth distance means in terms of the average number of neighbors per county we note that one-half the distance squared times pi is the area captured by the bandwidth.
```{r}
( bwA <- pi * (bw * 1000 /2)^2 ) 
```

In units of square meters. Or 21,519 square kilomenters.

County areas are computed using the `st_area()` function. The average size of the countys and the ratio of the bandwidth area to the average county area is also computed.
```{r}
areas <- st_area(SH.sf)
ctyA <- mean(areas)
bwA/ctyA
```

The ratio indicates that, on average, a neighborhood consists of 13 counties. For comparison, on a raster there are 8 first-order neighboring cells (queen contiguity) and 16 second-order neighboring cells (neighbors of neighbors) or a total of 24 neighbors.

We then use the `gwr()` function to includes the formula, data, and the `bandwith =` argument.
```{r}
model.gwr <- gwr(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                 data = SH.sp, 
                 bandwidth = bw)
```

The model and observed data are assigned to list object with element names extracted with the `names()` function.
```{r}
names(model.gwr)
```

The first element of the list named `SDF` contains the model output as a S4 spatial class data frame. The geometry of the spatial data frame is inherited from the type of data frame specified in the `data = ` argument.

The structure of the S4 spatial class is obtained with the `str()` function and by setting the `max.level` argument to 2.
```{r}
str(model.gwr$SDF, max.level = 2)
```

Here we see there are 5 slots with the first slot being the attribute table labeled `@data`. The dimension of the attribute table is retrieved with the `dim()` function.
```{r}
dim(model.gwr$SDF)
```

There are 1412 rows and 9 columns. Each row corresponds to a county and information about the regression localized to the county is given in the columns. The attribute names are extracted with the `names()` function.
```{r}
names(model.gwr$SDF)
```

They include the sum of the weights `sum.w` (the larger the sum the more often the county was included in the local regressions--favoring smaller counties and ones farther from the borders of the spatial domain), the five regression coefficients (one for each of the 4 explanatory variables and an intercept term), the residual (`gwr.e`), the predicted value (`pred`) and the local goodness-of-fit (`localR2`).

We put the predictions into the `SH.sf` simple feature data frame with the column name `predGWR`.
```{r}
SH.sf$predGWR <- exp(model.gwr$SDF$pred)

tm_shape(SH.sf) +
  tm_fill("predGWR", title = "Predicted\nHomicide Rates\n[/100,000]") +
  tm_layout(legend.outside = TRUE)
```

The geographic regressions similarly capture the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the linear model. The pattern is also a smoother.

With many more model parameters metrics of predictive skill will favor the geographic regression. For example, the root mean-square-error is lower for GWR.
```{r}
sqrt(sum(residuals(model.lm3)^2))
sqrt(sum(model.gwr$SDF$gwr.e^2))
```

Geographic regression is valuable for generating hypothesis. From the linear model we saw that homicide rates increased with resource deprivation. How does this relationship vary across the South.
```{r}
coef(model.lm3)[2]
range(model.gwr$SDF$RD90)
```

The global regression coefficient is .51 but locally the coefficients range from 0.08 to .98.

Importantly we can map where resource deprevation has the most influence on the response variable.
```{r}
SH.sf$RDcoef <- model.gwr$SDF$RD90

tm_shape(SH.sf) +
  tm_fill("RDcoef", title = "Resource\nDeprivation\nCoefficient", palette = 'Blues') +
  tm_layout(legend.outside = TRUE)
```

All values are above zero, but areas in darker blue indicate where resource deprivation plays a stronger role in explaining homicide rates.

How about the unemployment rate?
```{r}
SH.sf$UEcoef <- model.gwr$SDF$UE90

tm_shape(SH.sf) +
  tm_fill("UEcoef", title = "Unemployment\nCoefficient", palette = 'PiYG') +
  tm_layout(legend.outside = TRUE)
```

While the global coefficient is negative indicating homicide rates tend to be lower in areas with more unemployment, the opposite is the case over much of Texas into Oklahoma.

Where does the model for homicide rates provide the best fit to the data? This is answered with a map of local R squared values (`localR2`).
```{r}
SH.sf$localR2 <- model.gwr$SDF$localR2

tm_shape(SH.sf) +
  tm_fill("localR2", title = "Local\nR Squared", palette = 'Purples') +
  tm_layout(legend.outside = TRUE)
```

When we use a regression model to fit data that vary spatially we are assuming an underlying stationary process. This means we believe the explanatory variables 'provoke' the same statistical response across the entire domain. If this is not the case then it shows up in a map of correlated residuals. One approach to investigate things further is to use geographic regression. Another approach is to use a single spatial regression model.

## Spatial regression

Ordinary regression models fit to spatially aggregated data can lead to improper inference because observations are not independent. Thus it's necessary to check the residuals from an aspatial model for spatial autocorrelation. If the residuals are strongly correlated the model is mis-specified. 

In this case we can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), we can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

### Spatial lag model and spatial error models

The equation for a regression model in vector notation is
$$
y = X \beta + \varepsilon
$$
where $y$ is a $n \times 1$ vector of response variable values, $X$ is a $n$ $\times$ $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n \times 1$ vector of residuals (iid).

Two options exist if the elements of the vector $\varepsilon$ are spatially correlated. The first is to rewrite the model adding a spatial lag term as
$$
y = \rho W y + X \beta + \varepsilon
$$
where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

Note: $Wy$ is the spatial lag variable we compute with the `lag.listw()` function and $\rho$ is Moran I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model is motivated by a diffusion process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the spatial signal term and $\beta X$ is called the trend term.

The second option is to rewrite the model by adding a spatial error term as
$$
y = X\beta + \lambda W \epsilon + u
$$
where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted variable bias. Suppose the $y$ is statistically described by two variables $x$ and $z$ each centered on zero and independent. Then
$$
y = \beta x + \theta z
$$
If $z$ is not observed, the vector $\theta z$ is nested in the error term $\epsilon$.
$$
y = \beta x + \epsilon
$$

Examples of an unobserved latent variable $z$ include local culture, social capital, neighborhood readiness. Importantly we would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods), so let
$$
z = \lambda W z + r\\
z = (I - \lambda W)^{-1} r
$$
where $r$ is a vector of random independent residuals (e.g., culture is similar but not identical), $W$ is the spatial weights matrix and $\lambda$ is a scalar spatial correlation parameter. Substituting into the equation above
$$
y = \beta x + \theta z \\
y = \beta x +   \theta (I - \lambda W)^{-1} r\\
y = \beta x + (I - \lambda W)^{-1} \varepsilon
$$
where $\varepsilon = \theta r$.

Another reason for fitting a spatial error model is spatial heterogeneity. Suppose we have multiple observations for each unit. If we want our model to incorporate individual effects we can include a $n \times 1$ vector $a$ of individual intercepts for each unit.
$$
y = a + X\beta
$$
where now $X$ is a $n$ $\times$ $p$ matrix.

In a cross-sectional setting with one observation per unit (typically the case in observational studies), this approach is not possible since we will have more parameters than observations.

Instead we can treat $a$ as a vector of spatial random effects. We assume that the intercepts follows a spatially smoothed process
$$
a = \lambda W a + \epsilon \\
a = (I - \lambda W)^{-1} \epsilon
$$
which leads to the previous model
$$
y = X\beta + (I - \lambda W)^{-1} \epsilon
$$

### Lagrange multiplier test for the type of spatial autocorrelation

Let's see how to choose between these two options with an example. Returning to the Columbus crime data, we import the data, fit a linear regression model to statistically explain crime rates using income and housing values. Further we first need to check whether a spatial regression model is warranted by testing whether there is significant spatial autocorrelation in the residuals of the aspatial model.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip")
unzip("columbus.zip")

CC.sf <- read_sf(dsn = "columbus", 
                 layer = "columbus")

model.lm <- lm(CRIME ~ INC + HOVAL, 
               data = CC.sf)
CC.sf$residuals <- residuals(model.lm)

nbs <- poly2nb(CC.sf)
wts <- nb2listw(nbs)

lm.morantest(model.lm, wts)
```

The answer is 'yes' a spatial regression model is warranted.

To help decide between the two spatial regression models described above we run a sequence of statistical tests on our linear model object. The tests are the Lagrange multiplier (LM) tests. LM refers to a technique to determine the coefficients on the explanatory variables while simultaneously determining the coefficient on the spatial regression term. It does this iteratively.

The tests are performed with the `lm.LMtests()` function. The test type is specified as a character string. The tests should be considered in order. Start with the standard version of both the spatial error model and spatial lag model LM tests.

For example, to perform a LM test for the spatial error and spatial lag model on the Columbus crime model we type
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag model terms are significant ($p$-value < .15). Ideally one term is significant and the other is not and we choose the model with the significant term.

Since both are significant we should test again. This time we use the robust forms of the statistics.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so we choose the lag model for our spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significance for both models, then we can fit both models and check which one results in the lowest AIC.

### Model fitting

To fit a spatial lag model to the crime data we use the `lagsarlm()` function. The value for $\rho$ is found first using the `optimize()` function and then the $\beta$'s are obtained using generalized least squares. These functions are now in the {spatialreg} package.
```{r}
#library(spatialreg)

model.lag <- lagsarlm(CRIME ~ INC + HOVAL, 
                      data = CC.sf, 
                      listw = wts)

summary(model.lag)
```

Let's break down the output starting with the coefficients.

#### Interpreting the model coefficients

The first batch of output concerns the model residuals and the coefficients on the explanatory variables. The model residuals are the observed crime rates minus the predicted crime rates.

The coefficients on income and housing have the same sign (negative) and they remain statistically significant.

The spatial lag model allows for 'spillover'. That is a change in an explantory variable anywhere in the study domain will affect the value of the response variable everywhere. Spillover occurs even when the neighborhood weights matrix represents simple (1st-order) local contiguity. The spillover makes interpreting the marginal effects more complicated.

The spatial lag model implies that for a change in the value of an explanatory variable there are direct and indirect effects on the response variable.

For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the $i$th tract's income on crime across neighboring tracts.

The indirect effect gives the impact of a change in income has on crime averaged over all OTHER tracts. The indirect effect represent spillovers. The influences on the dependent variable $y$ in a region rendered by change in $x$ in some other region. For example, if all tracts $i \ne j$ (i not equal to j) increase their income, what will be the impact on crime in region $i$?

The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract $j$ increasing its income over all other tracts (on average). It is given by
$$
\hbox{TE} = \left(\frac{\beta_k}{1-\rho^2}\right)\left(1 + \rho\right)
$$
where $\beta_k$ is the marginal effect of variable $k$ and $\rho$ is the spatial autocorrelation. With $\rho = 0$ TE is $\beta_k$.

Here $\beta_{INC}$ is -1.0487 so the total effect is
```{r}
( TE_INC <- -1.0487 / (1 - .4233^2) * (1 + .4233) )
```
where .4233 is the value for $\rho$.

The direct, indirect, and total effects are shown using the `impacts()` function from the {spatialreg} package.
```{r}
spatialreg::impacts(model.lag, 
                    listw = wts)
```

The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region.

The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3.

See https://youtu.be/b3HtV2Mhmvk
lmSLX() function

#### Interpreting the coefficient rho

The next set of output is about the estimate of spatial autocorrelation ($\rho$).  The value is .4233 and a likelihood ratio test gives a value of 9.41 which translates to a $p$-value of .002.  The null hypothesis is the autocorrelation is zero, so we confidently reject it. This is consistent with the significant Moran I value that we found in the linear model residuals.

Two other tests are performed on the value of $\rho$ including a z-test (t-test) using the asymptotic standard error and a Wald test. Both tests confirm that the lag term should be included in the model.

#### Overall model fit

The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model.

The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.lm))/49
x[1]
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

#### Residual spatial autocorrelation

The final bit of output is a Lagrange multiplier test for residual autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. We find a high $p$-value so you are satisfied that the lag term takes care of the autocorrelation.

Compare with a spatial error model. Here we use the `errorsarlm()` function.
```{r}
model.error <- errorsarlm(CRIME ~ INC + HOVAL, 
                          data = CC.sf, 
                          listw = wts)
summary(model.error)
```

Here we find the spatial autoregression ($\lambda$) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the LM tests indicating the spatial lag model is more appropriate.

We compare the log likelihoods from the two spatial regression models and find that the lag model is a good improvement over the error model. This result is consistent with our above decision to use the lag model.
```{r}
x <- 2 * (logLik(model.lag) - logLik(model.error))/49
x[1]
```

### Predictions

The `predict()` method implements the `predict.sarlm()` function to calculate predictions from the spatial regression model. The prediction is decomposed into a "trend" term (explanatory variable effect) and a "signal" term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model.

We make predictions with the `predict()` method under the assumption that the mean response is known. We examine the structure of the corresponding predict object.
```{r}
pre <- predict(model.lag)
str(pre)

CC.sf$CRIME[1:5]
```

The trend term is $X\beta$ and the signal term is $\rho W y$.

The predictions are added to the simple features data frame.
```{r}
CC.sf$fit <- as.numeric(pre)
CC.sf$trend <- attr(pre, "trend")
CC.sf$signal <- attr(pre, "signal")
```

The components of the predictions are mapped and placed on the same page.
```{r}
( g3 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = fit)) +
    scale_fill_gradient(low = "white", high = "green") +
    ggtitle("Predicted Crime") )

( g4 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = trend)) +
    scale_fill_gradient(low = "white", high = "orange") +
    ggtitle("Trend (Covariate)") )

( g5 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = signal)) +
    scale_fill_gradient(low = "white", high = "blue") +
    ggtitle("Signal") )

gridExtra::grid.arrange(g3, g4, g5, nrow = 3)
```

The values in the map at the top are the sum of the values in the two maps below. The spatial smoother (signal) is about the same size as the explanatory variable effect especially for the most centrally located tracts.

Compare model residuals. How many tracts have a smaller residual when using the lag model versus the aspatial model?
```{r}
CC.sf <- CC.sf %>%
  mutate(residualsL = CRIME - fit,
         lagWins = abs(residuals) > abs(residualsL))

sum(CC.sf$lagWins)
```

In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the aspatial model.

#### References

* Haining, R. 1990 Spatial data analysis in the social and environmental sciences, Cambridge: Cambridge University Press, p. 258.
* Cressie, N. A. C. 1993 Statistics for spatial data, Wiley, New York.
* Michel Goulard, Thibault Laurent & Christine Thomas-Agnan, 2017 About predictions in spatial autoregressive models: optimal and almost optimal strategies, Spatial Economic Analysis Volume 12, Issue 2–3, 304–325 https://doi.org/10.1080/17421772.2017.1300679.
* Kelejian, H. H. and Prucha, I. R. 2007 The relative efficiencies of various predictors in spatial econometric models containing spatial lags, Regional Science and Urban Economics, Volume 37, Issue 3, 363–374.
* Bivand, R. 2002 Spatial econometrics functions in R: Classes and methods, Journal of Geographical Systems, Volume 4, No. 4, 405–421

Finally, Stata is the dominant software package for economists needing to fit spatial regression models. Outside of Economics, however, R is widely used and more versatile. This cheatsheet may help make the transition. https://www.r-bloggers.com/stata-to-r-cheatsheet-for-econometrics/

### Example: California house prices

The file `CHP.zip` contains information about housing values at the census tract level across the state of California.
```{r}
library(sf)

download.file("http://myweb.fsu.edu/jelsner/temp/data/CHP.zip",
              "temporary.zip")
unzip("temporary.zip")

CHP.sf <- read_sf(dsn = "CHP")
```

The object `CHP.sf` is a simple feature spatial data frame with 29 variables whose values are aggregated to census tracts. There are 7049 tracts. To see the spatial extent of the data and the tract boundaries we use `tm_borders()`.
```{r}
library(tmap)

tm_shape(CHP.sf) +
  tm_borders()
```

The warning message tells us that at least one of the 7049 geometries is invalid. 

Invalid geometries include null or empty geometries or geometries that are not simple (e.g., intersecting polygons). The `st_is_valid()` function returns a logical vector that lets us determine the number of invalid geometries.
```{r}
sum(!st_is_valid(CHP.sf))
```

To see the reason we include the argument `reason = TRUE`.
```{r}
x <- st_is_valid(CHP.sf, reason = TRUE)
x[!st_is_valid(CHP.sf)]
```

This tells us that two geometries containing self-intersecting polygons.

To fix this we can create a new set of geometries with the `st_buffer()` function while setting the buffer distance to zero.
```{r}
library(dplyr)

CHP.sf <- CHP.sf %>%
  st_buffer(dist = 0)

sum(!st_is_valid(CHP.sf))
```

#### Aggregate the attributes to the county level

We will analyze these data at the county level. In Lesson 6 we saw how to aggregate variables in a spatial data frame using functions in the **areal** package. This allows for greater flexibility since the aggregating units do not need to be in alignment.

Since the California tracts are completely contained within county boundaries (aligned) here we use the `aggregate()` function from the **raster** package.

We have county names (`County`) as a character variable so we can aggregate by name after converting the simple feature to an S4 class spatial data frame.
```{r}
CHP.sp <- as(CHP.sf, "Spatial")

CHPcounty.sp <- raster::aggregate(CHP.sp, 
                                  by = "County")
```

This produces a spatial polygons data frame where the boundaries are counties but the attribute table contains only the county name. 

To see this let's convert the S4 spatial data frames to simple features and create a map.
```{r}
CHPcounty.sf <- as(CHPcounty.sp, "sf")

library(tmap)

tmap_mode("view")
tm_shape(CHPcounty.sf) +
  tm_borders()

tmap_mode("plot")
```

Zoom and note that San Francisco County is close to Marin County (to the north) but not by contiguity.

We need to aggregate the variables to the county level. Although it is possible to do everything in one step with `aggregate()` it is best to be careful because how we want a variable aggregated depends on its type (extensive or intensive).

The simplest case is where we can sum the values (extensive variables like the number of houses). First create a simple feature data frame and select a subset of the columns.
```{r}
CHP1.sf <- CHP.sf %>%
    dplyr::select(nhousingUn, recHouses, nMobileHom, Population,
                  Males, Females, Under5, White, Black,
                  AmericanIn, Asian, Hispanic, PopInHouse,
                  nHousehold, Families)
```

Create a list of the county names by tract.
```{r}
CountyList <- list(County = CHP.sf$County)
CountyList[[1]][1:50]
```

Then aggregate by county name (`County`) with the `aggregate()` method applied to the simple feature data frame. We employ the base R `sum()` function through the argument `FUN = sum` and specify what to do with missing values (encountered by in the summation) with the  `na.rm = TRUE` argument.
```{r}
CHP1.sf <- aggregate(CHP1.sf, 
                     by = CountyList,
                     FUN = sum, 
                     na.rm = TRUE)
head(CHP1.sf)

tm_shape(CHP1.sf) +
  tm_fill(col = "nMobileHom") +
  tm_borders()
```

The result is a spatial data frame containing county-level totals aggregated from the original tract-level data.

In other cases like with home values we need to use a weighted average to take into account the fact that housing values may be high in a small tract in a county but they may be much lower in a larger tract in the same county (intensive variable). So here we create average values per household in three steps. 

First select the variables of interest.
```{r}
CHP2.sf <- CHP.sf %>%
  dplyr::select(houseValue, yearBuilt, nRooms,
                nBedrooms, medHHinc, MedianAge,
                householdS, familySize, nHousehold)
```

Second multiply these variables by the number of households (`nHousehold`) in each tract.
```{r}
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue * nHousehold,
         yearBuilt = yearBuilt * nHousehold,
         nRooms = nRooms * nHousehold,
         nBedrooms = nBedrooms * nHousehold,
         MedianAge = MedianAge * nHousehold)
```

Third aggregate to get county totals before dividing by the number of households.
```{r}
CHP2.sf <- aggregate(CHP2.sf, 
                     by = CountyList, 
                     FUN = sum, 
                     na.rm = TRUE)
CHP2.sf <- CHP2.sf %>%
  mutate(houseValue = houseValue / nHousehold,
         yearBuilt = yearBuilt / nHousehold,
         nRooms = nRooms / nHousehold,
         nBedrooms = nBedrooms / nHousehold,
         medHHinc = medHHinc / nHousehold,
         MedianAge = MedianAge / nHousehold,
         householdS = householdS / nHousehold,
         familySize = familySize / nHousehold)

head(CHP2.sf)
```

Map the county level house values together with the number of bedrooms.
```{r}
tm_shape(CHP2.sf) +
  tm_fill(col = c("houseValue", "nBedrooms")) +
  tm_layout(legend.position = c("right", "top")) +
  tm_borders()
```

#### Fit a nonspatial regression model to predict house values

We examine the residuals from an aspatial regression of house price on house age and the number of bedrooms. Here we first assign the model formula to the object `mf` and then use the `lm()` function.
```{r}
mf <- houseValue ~ yearBuilt + nBedrooms
model.lm <- lm(mf, data = CHP2.sf)
summary(model.lm)
```

According to the model, `yearBuilt` is highly significant. Older houses are more valuable. House values increase by 13K dollars for every additional year of existence. The number of bedrooms is also marginally significant. Every bedroom adds about 192K dollars to the value of a house.

What is the expected (predicted from the model) value of a house built in 1999 with four bedrooms?
```{r}
predict(model.lm, 
        newdata = data.frame(yearBuilt = 1999, 
                             nBedrooms = 4))
```

What is the expected value of a house in San Francisco? First, what row contains data from San Francisco County?
```{r}
which(CHP2.sf$County == "San Francisco")

predict(model.lm)[38]
```

How does this compare with the county average?
```{r}
CHP2.sf$houseValue[38]
```

The model under predicts house values in San Francisco by over $100K.

Next, examine the model residuals for spatial autocorrelation. We first compute the neighborhood topology based on contiguity. But then add two links: between San Francisco and Marin County and vice versa (to consider the fact that the Golden Gate bridge connects these two counties in a way that would likely influence housing prices). 

This is done by knowing the row numbers of these two counties and adding these numbers to the neighborhood list object. Note that since county 21 is a neighbor of county 38, county 38 must be a neighbor of county 21 so we need to adjust both.
```{r}
library(spdep)

nbs <- poly2nb(CHP2.sf)

CHP2.sf$County[21]

nbs[[21]] <- sort(as.integer(c(nbs[[21]], 38)))
nbs[[38]] <- sort(as.integer(c(21, nbs[[38]])))
```

Or we can use nearest neighbors. Start by extracting the coordinates of each county using the `st_coordinates()` function after applying the `st_centroid()` function. We use the `knearneigh()` function to compute the set of k nearest neighbors for each center. Since the CRS is geographic we include the `longlat = TRUE` argument and then  convert the k nearest neighbor object to a neighborhood object with the `knn2nb()` function.
```{r}
coords <- st_coordinates(st_centroid(CHP2.sf))

nearestNeighbors <- knearneigh(coords, 
                               k = 5,
                               longlat = TRUE)
knb <- knn2nb(nearestNeighbors)
summary(knb)
```

Create the spatial weights matrix and check the residuals from the linear model for spatial autocorrelation.
```{r}
wts <- nb2listw(nbs)
kwt <- nb2listw(knb)
lm.morantest(model.lm, wts)
lm.morantest(model.lm, kwt)
```

The large amount of spatial autocorrelation indicates that a spatial regression model is warranted. The model coefficients are not reliably precise.

#### Fit a spatial regression model using the same explanatory variables

As we saw in Lesson 11, we first need to choose between a lag and an error model. We do this sequentially first using the Lagrange multiplier test and then, if needed, the robust version of the LM test.
```{r}
lm.LMtests(model.lm, 
           listw = wts, 
           test = c("LMerr", "LMlag"))
lm.LMtests(model.lm,
           listw = wts, 
           test = c("RLMerr", "RLMlag"))
```

The results from the LM tests do not help us discern between and a lag and an error model. However, the results from the RLM tests favor of a lag model. Recall the lag model includes a spatial lag term on the response variable.

This same decision is reached when using the nearest neighbor weights (`kwt`).

Fit the model. Here we need to specify a lower tolerance for numerically solving the inverse of the covariance matrix. This is needed in this case because the explanatory variables have very different scales. Actually it might be better to scale the variables first.
```{r}
#library(spatialreg)

model.lag <- lagsarlm(mf, 
                      data = CHP2.sf, 
                      listw = wts,
                      tol.solve = 1e-30)
summary(model.lag)
```

The coefficients on year the house was built and on the number of bedrooms have the same sign and remain statistically significant but the values are considerably different.

The value of $\rho$ (coefficient on the spatial lag term) is .774, which is large and significant as indicated by the likelihood ratio test value and its corresponding $p$-value. The z-value and Wald statistic similarly indicate a significant coefficient on the spatial lag term.

The difference in log likelihoods between the spatial and aspatial regression models indicates a large to huge improvement of the spatial model over the aspatial model.
```{r}
2 * (logLik(model.lag) - logLik(model.lm))/58
```

   1 | huge,
  .1 | large,
 .01 | good,
.001 | okay

The AIC for the spatial lag model is lower than that for the linear model and there is no significant residual autocorrelation.

What is the expected value of a house in San Francisco predicted using the spatial lag model.
```{r}
predict(model.lag)[38]
```

The spatial lag model better captures the aggregated housing values in San Francisco than does the aspatial model.

The `trend` term is the collective influence of the two explanatory variables and the `signal` is the spatial smoothing.
```{r}
pre <- predict(model.lag)
str(pre)
CHP2.sf$houseValue[1:5]
```

Maps of the spatial signal and the trend terms are made separately.
```{r}
CHP2.sf$signal <- attr(pre, "signal")
CHP2.sf$trend <- attr(pre, "trend")

tm_shape(CHP2.sf) +
  tm_fill("signal")
```

The signal term 'explains' (statistically) the spatial influence of San Francisco on housing values across the state. Highest values are centered on San Francisco County and these high values decay northward and eastward quickly. The decay in the influence of San Francisco is more gradual to the south.

```{r}
tm_shape(CHP2.sf) +
  tm_fill("trend")

CHP2.sf$preLAG <- CHP2.sf$signal + CHP2.sf$trend

tm_shape(CHP2.sf) +
  tm_fill("preLAG", midpoint = NA)
```

The trend term shows that, beyond the spatial smoothed term, additional factors (year built and number of bedroom) make house values in San Francisco high.
