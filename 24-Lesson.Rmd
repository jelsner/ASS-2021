---
title: "Lesson 24"
author: "James B. Elsner"
date: "April 5, 2021"
output:
  pdf_document: default
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The first step of any project is to grossly underestimate its complexity and difficulty."** â€“ Nicoll Hunt

Last two weeks of what has been a long (and lonely) semester. Here is my plan to bring this course in for a soft landing. 

This week: Last two synchronous lessons and the final assignment. The final assignment will be due on Friday, April 16 at 3p. I will be available this Friday for live help if needed. The last two lessons next week will be Rmd files that you can work through on your own if interested.

Last week: introduction to spatial statistical interpolation. It begins with separating trends from local spatial autocorrelation. The local spatial autocorrelation is quantified with an empirical variogram. The next two steps involve fitting a model to the empirical variogram and using the model variogram together with the data to interpolate values across the domain.

Today: modeling the variogram and creating the interpolated surface.

## Modeling the variogram

The variogram model is a curve through the set of values that make up the empirical variogram. The curve is described by a function with three parameters.

As an example, lets plot the empirical variogram values from the `s100` data over a set of lag distances from 0 to 1. The `uvec =` argument controls the set of lag distances and/or the number of variogram values. 
```{r}
library(geoR)

s100.v <- variog(s100, 
                 uvec = seq(0, 1, l = 21))
plot(s100.v)
```

We overlay a curve corresponding to a specific variogram model using the `lines.variomodel()` function. We specify the function type with the `cov.model` argument and values for the sill and range parameters (in that order) with the `cov.pars` argument. We specify the value for the nugget with the `nugget` argument.

Here we use an exponential function (`cov.model = "exp"`) having a sill of 1, a range of .3 and a nugget of 0. For the plot method we specify the maximum distance with the `max.dist` argument.
```{r}
#plot(s100.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), 
                 nugget = 0, 
                 max.dist = 1, 
                 lwd = 2, col = "red")
```

The red curve corresponds to an exponential model with those parameters. It looks like it fits the variogram values reasonably well. It should because the data were generated from this model.

Note: The practical range for the exponential model is the distance along the horizontal axis at which the curve reaches 95% of the sill, which is 3 times the range specified in the `cov.pars` (covariance parameters) argument (here .9). 

With observed data we don't know the model, so we estimate values for the parameters using information from the empirical variogram. The estimation procedure can involve

* By eye: Trial and error over several models and parameter values. The `lines.variomodel()` function can help.  
* By least squares fit:  Using ordinary least squares (OLS) or weighted least squares (WLS) methods available through the `variofit()` function.  
* By maximum likelihood methods: Options for maximum likelihood (ML) and restricted maximum likelihood (REML) are available through the `likfit()` function.  

Try various curves and choose the one that looks the best. Here we plot two other models on top of the empirical variogram and the original model.
```{r}
#plot(s100.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .5),
                 nug = .1, 
                 max.dist = 1, 
                 lwd = 2, col = "black")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(.8, .8),
                 nug = .1, 
                 max.dist = 1, 
                 lwd = 2, col = "gray70")
```

The black line is based on the same exponential function, but it has a range of .5 and a nugget of .1. The gray line is based on a spherical function with a sill and range of .8 and a nugget of .1. 

All three curves fit the points reasonably well. Key point: In practice, the choice often makes little difference in the quality of the spatial interpolation.

Let's fit a variogram model to the Wolfcamp aquifer data. Recall the  head heights represent the geopotential energy of the aquifer, the gradient of which is related to the speed and direction of water flow.  First plot the empirical variogram.
```{r}
library(tidyverse)
library(ggplot2)

L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read_csv(L)

wca.gdf <- as.geodata(wca.df[-30, ], # removed a duplicate measurement
                      coords.col = c("lon", "lat"), 
                      data.col = "head")

wca.v <- variog(wca.gdf, 
                trend = "1st", # compute the variogram values on the residuals from a trend model (1st order)
                max.dist = 2.3)
plot(wca.v)

ggplot(data = data.frame(h = wca.v$u, v = wca.v$v), 
       mapping = aes(x = h, y = v)) + 
  geom_point() + 
  geom_smooth() +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal() +
  geom_hline(yintercept = c(12000, 45000), color = "red") +
  geom_vline(xintercept = .85, color = "red") +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 12000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .1, y = 11000, label = "nugget")) +
  geom_segment(aes(x = 0, y = 12000, xend = 0, yend = 45000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .1, y = 44000, label = "sill")) +
  geom_segment(aes(x = 0, y = 47000, xend = .85, yend = 47000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .55, y = 48000, label = "range"))
```

Begin with a spherical model with a range of .8, a sill of 40000, and a nugget of 10000.  
```{r}
plot(wca.v)
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(30000, .8),
                 nug = 10000, 
                 max.dist = 2.3)
```
 
In the `cov.pars` argument the first value is the "partial" sill and the second is the range. With the nugget set at 10000 and partial sill at 30000, the sill is 400000. The sill and nugget are variance measures so they have units of feet squared. The range is distance so it has the corresponding spatial units.

The model looks reasonable. Perhaps the sill should be a bit higher, say 43000. We increase the partial sill accordingly,
```{r}
plot(wca.v)
lines.variomodel(cov.model="sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
```

Better still. We try an exponential model with the same sill but a different range.
```{r}
plot(wca.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(33000, .8/3),
                 nug = 10000, max.dist = 2.3,
                 col = "green")
```

Not as good. The exponential model does not have the sharp turn at the sill. We settle on the spherical model with a sill of 43000, a range of .8, and a nugget of 10000.

Next we tune the parameter estimates using the `variofit()` function. The function takes a set of initial parameter values that we estimate from looking at the shape and form of the empirical variogram and improves upon them using the method of weighted least squares. Alternatively we can use the `likfit()` function to fine tune the parameters. Here we use it to adjust the variogram model parameters estimated above. We save the model in the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

The function improves on the initial set of parameters until the log-likelihood value is maximized. Any other set of parameters will produce a log likelihood value smaller than -553.  

The output includes values for the trend surface. It is a linear trend in two dimensions so it's represented by a plane with a single z-intercept value (`beta0`) and two slope values corresponding to the x (`beta1`) and y (`beta2`) directions.  

The units on the slope parameters are data units per unit spatial distance.  Thus the `beta1` value is -400 ft/deg longitude.  For every one deg longitude east, the piezometric head height decreases by 400 ft. We saw a trend in the SW-NE direction (exploratory plot). The `beta1` (`beta2`) value quantifies this slope in the east-west (north-south) direction.

The output also includes the parameter values for the nugget (`tausq`), the partial sill (`sigmasq`) and the range (`phi`).

Overlay the maximum likelihood solution to the spherical model by typing
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
lines(wca.vm, col = "blue")
```

We see the maximum likelihood solution is a better fit for the variogram estimates at lag distances between .5 and 1. At these lags, the variogram estimates are most reliable as n (number of observation pairs) is largest.
```{r}
variog(wca.gdf)$n
```

The `likfit()` function is iterative and should find the same solution using somewhat different starting values. Try it using a partial sill of 30000 a nugget of 5000 and a range of 1. 
```{r}
likfit(wca.gdf, 
       trend = "1st", 
       ini = c(30000, 1),
       nug = 5000, 
       cov.model = "spherical")
```

A summary function on the variogram model object provides more information about the fitted variogram model.
```{r}
summary(wca.vm)
```

The summary gives the parameters of the trend model and parameter values for the variogram model. Importantly the summary gives the log likelihood along with the AIC and BIC values. It also gives those values for a non-spatial model. A non-spatial model in this context is the trend plus spatially _uncorrelated_ random variation.

Since the log likelihood value is larger for the spatial model and the AIC and BIC values are smaller, it is clear that a spatial model is better than the non-spatial model.

The `proflik()` function gives a matrix of log likelihood values for a range of model parameters. The matrix can then be plotted to get a synoptic view of the relationship between the parameters and the likelihood. This can take a few seconds to compute.
```{r}
prof <- proflik(wca.vm, geodata = wca.gdf, 
                sill.val = seq(30000, 60000, length = 6),
                range.val = seq(.3, 2.3, length = 6), 
                nug.val = 10000, uni.only = FALSE)
plot(prof, nlevels = 16)
```

The horizontal axis is the partial sill ($\sigma^2$) and the vertical axis is the range ($\phi$). The nugget is set at 10000. The best fit parameters are indicated by the circle. At this location the log-likelihood is maximized. The likelihood is near the maximum for a broad set of range and partial sill values.

The marginal profiles are also available and can be plotted alongside the contour plot using
```{r}
par(mfrow = c(1, 3))
plot(prof, nlevels = 8)
```

Let's consider another data set. Here we want to find a variogram model for April temperatures in the Midwest. The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April.
```{r}
library(tidyverse)

L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- read_table(L)
summary(t.df)
```

Map the values.
```{r}
library(sf)
library(tmap)

t.sf <- st_as_sf(x = t.df, 
                 coords = c("lon", "lat"),
                 crs = 4326)

library(USAboundaries)

sts <- us_states(states = c("WI", "MN", "IA", "IL", "MO", "KS", "NE", "SD"))

tm_shape(t.sf) +
  tm_text(text = "temp", size = .6) +
tm_shape(sts) +
  tm_borders() 
```

Create a `geodata` object from the data frame.
```{r}
t.gdf <- as.geodata(t.df)
summary(t.gdf)
plot(t.gdf)
```

Remove the trends and examine the residuals.
```{r}
plot(t.gdf, trend = "1st")
plot(t.gdf, trend = "2nd")
```

Plot the empirical variogram.
```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
```

Fit a variogram model.
```{r}
iv <- c(3, 4)  

( t.vm <- likfit(t.gdf, 
                 trend = "2nd",
                 ini = iv, 
                 cov.model = "exp") )
```

The beta values refer to the second order trend. The nugget is `tausq` the partial sill is `sigmasq` and the range is `phi`.
```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
lines(t.vm, col = "blue")
```

## Creating an interpolated surface

Returning to the Wolfcamp aquifer data. We fit a variogram model and saved it as `wca.vm`.

Kriging uses the variogram model together with the observed data to estimate data values at any location of interest. The kriged estimates are a weighted average of the neighborhood values where the weights are taken from the variogram model. 

Estimates are often made at locations defined on a regular grid.

(a) First create a grid of locations across the domain. Here we use the `expand.grid()` function. The coordinates names of the grid are those defined in the `geodata` object (here `lon` and `lat`). Grid spacing is defined by the `l =` argument in the sequence function.
```{r}
pgrid.df <- expand.grid(lon = seq(-105, -100, l = 161),
                        lat = seq(33, 37, l = 161))
head(pgrid.df)
```

The resulting data frame is a series of locations specified as in a raster starting in the southwest corner. Plot the grid. First convert it to a simple feature data frame. Do the same for the observations.
```{r}
library(sf)
pgrid.sf <- st_as_sf(x = pgrid.df,
                     coords = c("lon", "lat"),
                     crs = 4326)
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = 4326)

library(USAboundaries)
sts <- us_states()

library(tmap)
tm_shape(wca.sf) +
  tm_bubbles(size = .25) +
tm_shape(pgrid.sf) +
  tm_dots(col = "red") +
tm_shape(sts) +
  tm_borders()
```

(b) Next predict the head heights at the grid locations. 

The `krige.conv()` function performs the kriging. Predictions (interpolations) are made at the grid locations using the data and the variogram model (`wca.vm`). We specify a 1st-order trend in the data (`trend.d`) and we want the predictions to include the trend (`trend.l`) so these are included as parameters in the `krige.control()` function.
```{r}
wca.ks <- krige.conv(wca.gdf, 
                     loc = pgrid.df, 
                     krige = krige.control(trend.d = "1st", 
                                           trend.l = "1st", 
                                           obj.m = wca.vm))  
str(wca.ks)
```

The fitted values (`predict`) and the uncertainty (`krige.var`) are output as a vector of length  61 x 61 = 3721. The uncertainty is the standard deviation squared of the predicted value. Kriging performed using global neighborhood.

(c) Plot the results.

First add the fitted and uncertainty values as columns to the `pgrid.df` data frame.
```{r}
pgrid.df$height <- wca.ks$predict
pgrid.df$var <- wca.ks$krige.var
```

Map the predicted values and the uncertainty values using `ggplot()`. Start with the predicted values. Here we use the `geom_raster()` function.
```{r}
library(ggplot2)

ggplot(data = pgrid.df, 
       mapping = aes(x = lon, y = lat)) + 
  geom_raster(mapping = aes(fill = height)) +
  scale_fill_viridis_c()
```

The map shows the predicted values as a combination of the spatial gradient and spatial autocorrelation.

To use functions from {tmap} we first convert the grid data frame to a spatial pixels data frame (S4 spatial object). Then convert the spatial pixels data frame to a raster.
```{r}
library(sp)
spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(wca.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 10, palette = "viridis") +
tm_shape(wca.sf) +
  tm_text("head", size = .5) +
tm_shape(sts) +
  tm_borders()
```

Plot the uncertainty.
```{r}
ggplot(data = pgrid.df, aes(lon, lat)) + 
  geom_raster(mapping = aes(fill = var)) +
  scale_fill_gradient() +
  geom_point(data = wca.df, 
             mapping = aes(x = lon, y = lat))
```

The prediction variances are smallest at locations nearest to the observations. This makes sense since what we know about the field comes from these observations.

Evaluate the prediction. How do we evaluate how good the interpolated surface is? If we use the variogram model to predict at the observation locations, we will get the observed values back (when the nugget is fixed at zero). So this is not helpful. Instead we use cross validation.

Cross validation is a procedure for assessing how well a model will do at predicting values when observations specific to the prediction are removed. The procedure first partitions the data into disjoint subsets. The model is then fit to one subset of the data (training set) and the model is validated on a different subset (testing set). 

Leave-one-out cross validation uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns being left out. 

K-fold cross validation uses K observations for fitting and N-K for testing. With large K there are many ways to slice the sample so the procedure is not exhaustive like hold-one-out. 

With kriging, the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. Thus cross validation has two cases: weak and strong. Weak cross validation uses the entire dataset to estimate the variogram model. Then kriging is performed N times using a leave-one-out strategy with the predicted value saved only for the observation left out.

The `xvalid()` function from the {geoR} package computes the cross-validated prediction error of the Wolfcamp aquifer interpolation in this weak sense.
```{r}
xv.wk <- xvalid(wca.gdf, 
                model = wca.vm)
df <- data.frame(observed = xv.wk$data, 
                 predicted = xv.wk$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Weak Cross Validation") +
  theme_minimal()
```

The black line represents a perfect prediction and the red line is the best fit line when we regress the predicted head heights onto the observed head heights. The fact that the two lines nearly coincide indicates the interpolation is good.

We quantify how good using the mean squared error and mean absolute error as follows.
```{r}
mean(xv.wk$error^2)
mean(abs(xv.wk$error))
```

The mean squared cross-validated prediction error is 30636 ft^2 and the mean absolute cross-validated prediction error is 137 ft.

In contrast to weak cross validation, strong cross validation requires that the variogram be re-estimated each time an observation is removed. The model must be fit using the `variofit()` function and the call must include the empirical variogram object. This is done with the argument `reestimate = TRUE`.
```{r}
wca.vm2 <- variofit(wca.v, 
                    ini = c(33000, .8),
                    nug = 10000, 
                    cov.model = "spherical")

xv.st <- xvalid(wca.gdf, 
                model = wca.vm2,
                variog.obj = wca.v,
                reestimate = TRUE)
df <- data.frame(observed = xv.st$data, 
                 predicted = xv.st$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Strong Cross Validation") +
  theme_minimal()

mean(xv.st$error^2)
mean(abs(xv.st$error))
```

The prediction error estimated using the procedure of strong cross validation will be larger than the prediction error estimated using the procedure of weak cross validation.