---
title: "Lesson 24"
author: "James B. Elsner"
date: "April 5, 2021"
output:
  html_document: null
editor_options:
  chunk_output_type: console
---

**"The first step of any project is to grossly underestimate its complexity and difficulty."** â€“ Nicoll Hunt

Fitting a variogram model to the empirical variogram

After we have an empirical variogram we need to fit a curve through the set of points that make up the variogram. The curve is called the variogram model. 

Since the `s100` data were generated using a variogram model, we already know the answer. The model is exponential with a sill (partial) of 1, a range of .3, and a nugget of 0.

Plot the empirical variogram values and overlay the curve corresponding to the exponential model with the `lines.variomodel()` function. The sill and range parameters are given in order using the `cov.pars` argument.
```{r}
library(geoR)

plot(variog(s100, uvec = seq(0, 1, l = 21)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), nugget = 0, 
                 max.dist = 1.2, lwd = 2, col = "red")
```

Note that the practical range for the exponential model is the distance along the horizontal axis at which the curve reaches 95% of the sill, which is 3 times the range specified in the `cov.pars` argument.

The `uvec =` argument allows control over the range of distance values and the number of variogram estimates. We specify the maximum distance in the `lines.variomodel()` function.

With observed data we don't know the model so we estimate the parameters using information from the empirical variogram.

* By eye: Trial and error over several models and parameter values. The `lines.variomodel()` function can help.  
* By least squares fit:  Using ordinary least squares (OLS) or weighted least squares (WLS) methods available through the `variofit()` function.  
* By maximum likelihood methods: Options for maximum likelihood (ML) and restricted maximum likelihood (REML) are available through the `likfit()` function.  

Try various curves and choosing the one that looks the best. Here we plot three models on top of the `s100` empirical variogram.
```{r}
plot(variog(s100, max.dist = 1))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .3), 
                 nug = 0, max.dist = 1)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(1, .5),
                 nug = .1, max.dist = 1, 
                 col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(.8, .8),
                 nug = .1, max.dist = 1, 
                 col = "blue")
```

The black line is the model used to generate the `s100` dataset. The red line is based on the same exponential (exp) function but has a range of .5 and a nugget of .1. The blue line is based on a spherical function with a sill and range of .8 and a nugget of .1. All three models fit the points reasonably well. Key point: In practice, the choice often makes little difference in the quality of the spatial interpolation.

Variogram model for the Wolfcamp aquifer data

Let's fit a variogram model to the Wolfcamp aquifer data. First plot the empirical variogram. We've seen that directional variograms are not needed.
```{r}
library(tidyverse)

L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read_csv(L)

wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = c("lon", "lat"), 
                      data.col = "head")

wca.v <- variog(wca.gdf, 
                trend = "1st", 
                max.dist = 2.3)
plot(wca.v)

library(ggplot2)

ggplot(data = data.frame(h = wca.v$u, v = wca.v$v), 
       mapping = aes(x = h, y = v)) + 
  geom_point() + 
  geom_smooth() +
  scale_y_continuous(limits = c(0, NA)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lag distance (h)") +
  theme_minimal() +
  geom_hline(yintercept = c(12000, 45000), color = "red") +
  geom_vline(xintercept = .85, color = "red") +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = 12000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .1, y = 11000, label = "nugget")) +
  geom_segment(aes(x = 0, y = 12000, xend = 0, yend = 45000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .1, y = 44000, label = "sill")) +
  geom_segment(aes(x = 0, y = 47000, xend = .85, yend = 47000,), arrow = arrow(angle = 15, length = unit(.3, "cm"))) +
  geom_label(aes(x = .55, y = 48000, label = "range"))
```

Begin with a spherical model with a range of .8, a sill of 40000, and a nugget of 10000.  
```{r}
plot(wca.v)
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(30000, .8),
                 nug = 10000, max.dist = 2.3)
```
 
In the `cov.pars` argument the first value is the "partial" sill and the second is the range. With the nugget set at 10000 and partial sill at 30000, the sill is 400000. The sill and nugget are variance measures so they have units of square feet. The range is distance so it has the corresponding spatial units.

The model looks reasonable. Perhaps the sill should be a bit higher, say 43000. We increase the partial sill accordingly,
```{r}
plot(wca.v)
lines.variomodel(cov.model="sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
```

Better still. We try an exponential model with the same sill but the same range.
```{r}
plot(wca.v)
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(33000, .8/3),
                 nug = 10000, max.dist = 2.3,
                 col = "green")
```

Not as good. The exponential model does not have the sharp turn at the sill. We settle on the spherical model with a sill of 43000, a range of .8, and a nugget of 10000.

Fine tune the parameter values

Next we tune the parameter estimates using the `variofit()` function. The function takes a set of initial parameter values and improves upon them using the method of weighted least squares. Alternatively we can use the `likfit()` function to fine tune the parameters. Here we use it to adjust the variogram model parameters estimated above. We save the model in the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

The function improves on the initial set of model parameters until the log-likelihood value is maximized. Any other set of parameters will produce a log likelihood value smaller than -553.  

The output includes values for the trend surface. It is a linear trend in two dimensions so it's represented by a plane with a single z-intercept value (`beta0`) and two slope values corresponding to the x (`beta1`) and y (`beta2`) directions.  

The units on the slope parameters are data units per unit spatial distance.  Thus the `beta1` value is -400 ft/deg longitude.  For every one deg longitude east, the piezometric head height decreases by 400 ft. We saw a trend in the SW-NE direction (exploratory plot). The `beta1` (`beta2`) value quantifies this slope in the east-west (north-south) direction.

The output also includes the parameter values for the nugget (`tausq`), the partial sill (`sigmasq`) and the range (`phi`).

Overlay the maximum likelihood solution to the spherical model by typing
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(33000, .8),
                 nug = 10000, max.dist = 2.3,
                 col = "red")
lines(wca.vm, col = "blue")
```

We see the maximum likelihood solution is better at fitting the points at lag distances between .5 and 1. At these lags, the variogram estimates are most reliable as n (number of observation pairs) is largest.
```{r}
variog(wca.gdf)$n
```

The `likfit()` function is iterative and should find the same solution using somewhat different starting values. Try it using a partial sill of 30000 a nugget of 5000 and a range of 1. 
```{r}
likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(30000, 1),
                 nug = 5000, 
                 cov.model = "spherical")
```

A summary function on the `likfit` output provides more information about the fitted variogram model.
```{r}
summary(wca.vm)
```

The summary provides parameters of the trend model and values of the spatial model parameters. Importantly the summary gives the log likelihood along with the AIC and BIC values. It also gives those values for a non-spatial model. A non-spatial model in this context is the trend plus spatially uncorrelated random variation.

Since the log likelihood value is larger for the spatial model and the AIC and BIC values are smaller, it is clear that a spatial model is better than the non-spatial model.

Profile likelihood

The `proflik()` function gives a matrix of log likelihood values for a range of model parameters. The matrix can then be plotted to get a synoptic view of the relationship between the parameters and the likelihood. This can take a few seconds to compute.
```{r}
prof <- proflik(wca.vm, geodata = wca.gdf, 
                sill.val = seq(30000, 60000, length = 6),
                range.val = seq(.3, 2.3, length = 6), 
                nug.val = 10000, uni.only = FALSE)
plot(prof, nlevels = 16)
```

The horizontal axis is the partial sill ($\sigma^2$) and the vertical axis is the range ($\phi$). The nugget is set at 10000. The best fit parameters are indicated by the circle. At this location the log-likelihood is maximized. The likelihood is near the maximum for a broad set of range and partial sill values.

The marginal profiles are also available and can be plotted alongside the contour plot using
```{r}
par(mfrow = c(1, 3))
plot(prof, nlevels = 8)
```

## Another example: A variogram model for April temperatures in the Midwest

The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- read_table(L)
summary(t.df)
```

Map the observed values.
```{r}
library(tmap)

t.sf <- st_as_sf(x = t.df, 
                 coords = c("lon", "lat"),
                 crs = "+proj=longlat +datum=WGS84")

library(USAboundaries)

sts <- us_states()

tm_shape(t.sf) +
  tm_text(text = "temp", size = .6) +
tm_shape(sts) +
  tm_borders() 
```

Create a `geodata` object from the data frame.
```{r}
t.gdf <- as.geodata(t.df)
summary(t.gdf)
plot(t.gdf)
```

Remove the trends and examine the residuals.
```{r}
plot(t.gdf, trend = "1st")
plot(t.gdf, trend = "2nd")
```

Plot the empirical variogram.
```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
```

Fit a variogram model.
```{r}
iv <- c(3, 4)  

( t.vm <- likfit(t.gdf, 
                 trend = "2nd",
                 ini = iv, 
                 cov.model = "exp") )
```

The beta values refer to the second order trend. The nugget is `tausq` the partial sill is `sigmasq` and the range is `phi`.

```{r}
plot(variog(t.gdf, 
            trend = "2nd", 
            max.dist = 5.5))
lines(t.vm, col = "blue")
```

## Example 1: Wolfcamp Aquifer head heights

We've been examining piezometric head heights from the Wolfcamp Aquifer. Head heights measurements indicate the potential energy of the water in units of height.

The data are in `wolfcamp.csv` on my website.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read.csv(L, header = TRUE)
```

Step 1: Examine the data for trends and isotropy. Convert the data frame to a `geodata` object. The duplicate measurement is removed with the subset operator when converting from a `data.frame` to a `geodata` object.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = 1:2, 
                      data.col = 3)
```

Plot the residuals after removing the 1st-order trend.
```{r}
plot(wca.gdf, trend = "1st")
```

Step 2: Compute empirical variograms. To compute and plot the variogram on the residuals from a 1st-order trend type
```{r}
wca.v <- variog(wca.gdf, 
                trend = "1st",
                max.dist = 2.3)
plot(wca.v)
```

Step 3: Fit a variogram model. Eyeball the initial values for the nugget, partial sill and range. Then use the `likfit()` function together with the initial estimates to get a variogram model. Here we assign the model to the object `wca.vm`.
```{r}
wca.vm <- likfit(wca.gdf, 
                 trend = "1st", 
                 ini = c(33000, .8),
                 nug = 10000, 
                 cov.model = "spherical")
wca.vm
```

Step 4: Create the kriged surface. Kriging uses the variogram model and the observed data to estimate data values at any location of interest. The kriged estimates are a weighted average of the neighborhood values where the weights are taken from the variogram model. 

Estimates are often made at locations defined on a regular grid.

(a) First create a grid of locations across the domain. Here we use the `expand.grid()` function. The coordinates names of the grid are those defined in the `geodata` object (here `lon` and `lat`). Grid spacing is defined by the `l =` argument in the sequence function.
```{r}
pgrid.df <- expand.grid(lon = seq(-105, -100, l = 61),
                        lat = seq(33, 37, l = 61))
head(pgrid.df)
```

The resulting data frame is a series of locations specified as in a raster starting in the southwest corner. Plot the grid. First convert it to a simple feature data frame. Do the same for the observations.
```{r}
library(sf)
pgrid.sf <- st_as_sf(x = pgrid.df,
                     coords = c("lon", "lat"),
                     crs = "+proj=longlat +datum=WGS84")
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(USAboundaries)
sts <- us_states()

library(tmap)
tm_shape(wca.sf) +
  tm_bubbles(size = .25) +
tm_shape(pgrid.sf) +
  tm_dots(col = "red")
```

(b) Next predict the head heights at the grid locations. 

The `krige.conv()` function performs the kriging. Predictions are made at the grid locations using the data and the variogram model (`wca.vm`). We specify a 1st-order trend in the data (`trend.d`) and we want the predictions to include the trend (`trend.l`) so these are included as parameters in the `krige.control()` function.
```{r}
wca.ks <- krige.conv(wca.gdf, 
                     loc = pgrid.df, 
                     krige = krige.control(trend.d = "1st", 
                                           trend.l = "1st", 
                                           obj.m = wca.vm))  
str(wca.ks)
```

The fitted values (`predict`) and the uncertainty (`krige.var`) are output as a vector of length  61 x 61 = 3721. The uncertainty is the standard deviation squared of the predicted value. Kriging performed using global neighborhood.

(c) Plot the results.

First add the fitted and uncertainty values as columns to the `pgrid.df` data frame.
```{r}
pgrid.df$height <- wca.ks$predict
pgrid.df$var <- wca.ks$krige.var
```

Map the predicted values and the uncertainty values using `ggplot()`.

Start with the predicted values. Here we use the `geom_raster()` function.
```{r}
library(ggplot2)

ggplot(data = pgrid.df, aes(x = lon, y = lat)) + 
  geom_raster(aes(fill = height)) +
  scale_fill_viridis_c()
```

The map shows the predicted values as a combination of the spatial gradient and spatial autocorrelation.

Or use {tmap}. First convert the grid data frame to a spatial pixels data frame (S4 spatial object). Then convert the spatial pixels data frame to a raster.
```{r}
library(sp)
spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(wca.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9) +
tm_shape(wca.sf) +
  tm_text("head", size = .5)
```

Plot the uncertainty.
```{r}
ggplot(data = pgrid.df, aes(lon, lat)) + 
  geom_raster(aes(fill = var)) +
  scale_fill_gradient() +
  geom_point(data = wca.df, aes(x = lon, y = lat))
```

The map shows that the prediction variances are smallest in regions surrounding the observations. This makes sense since what we know about the field comes from the observations.

Step 5: Evaluate the prediction. How do we evaluate how good the interpolated surface is? If we use the variogram model to predict at the observation locations, we will get the observed values back when the nugget is fixed at zero. So this is not helpful. Instead we use cross validation.

Cross validation is a procedure for assessing how well a model will do at predicting values when observations specific to the prediction are removed. The procedure first partitions the data into disjoint subsets. The model is then fit to one subset of the data (training set) and the model is validated on a different subset (testing set). 

Leave-one-out cross validation uses all but one observation for fitting and the left-out observation for testing. The procedure is repeated with every observation taking turns in being left out. 

K-fold cross validation uses K observations for fitting and N-K for testing. With large K there are many ways to slice the sample so the procedure is not exhaustive like hold-one-out. 

With kriging, the data is used in two ways (1) to fit the variogram model, and (2) to interpolate the values. Thus cross validation has two cases: weak and strong. Weak cross validation uses the entire dataset to estimate the variogram model. Then kriging is performed N times using a leave-one-out strategy with the predicted value saved only for the observation left out.

The `xvalid()` function from the {geoR} package computes the cross-validated prediction error of the Wolfcamp aquifer interpolation in this weak sense.
```{r}
xv.wk <- xvalid(wca.gdf, model = wca.vm)
df <- data.frame(observed = xv.wk$data, 
                 predicted = xv.wk$predicted)

ggplot(df, aes(x = observed, y = predicted)) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_smooth(method = lm, color = "red") +
  ylab("Predicted head heights (ft)") +
  xlab("Observed head heights (ft)") +
  ggtitle("Weak Cross Validation") +
  theme_minimal()
mean(xv.wk$error^2)
mean(abs(xv.wk$error))
```

The mean squared cross-validated prediction error is 30636 ft^2 and the mean absolute cross-validated prediction error is 137 ft.

Strong cross validation requires that the variogram be re-estimated each time an observation is removed. The model must be fit using the `variofit()` function and the call must include the empirical variogram object. This is done with the argument `reestimate = TRUE`.
```{r, warning=FALSE}
xv.st <- xvalid(wca.gdf, model = wca.vm,
               variog.obj = wca.v,
               reestimate = TRUE)
df <- data.frame(observed = xv.st$data, 
                 predicted = xv.st$predicted)

mean(xv.st$error^2)
mean(abs(xv.st$error))
```

Strong cross validation will result in an error estimate that is larger than the error estimate from a weak cross validation.

## Example 2: April temperatures in the Midwest

The data in `MidwestTemps.txt` are average temperatures in and around the state of Iowa for the month of April. The goal is a spatial interpolation of these values onto a 56 by 35 grid.

Step 1: Examine the data for spatial trends and normality.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/MidwestTemps.txt"
t.df <- read.table(L, header = TRUE)
summary(t.df)
```

Map the values.
```{r}
t.sf <- st_as_sf(x = t.df, 
                 coords = c("lon", "lat"),
                 crs = "+proj=longlat +datum=WGS84")

tm_shape(t.sf) +
  tm_text(text = "temp", size = .6) +
tm_shape(sts) +
  tm_borders() 
```

Create a `geodata` object from the data frame.
```{r}
t.gdf <- as.geodata(t.df)
summary(t.gdf)
plot(t.gdf)
```

The maximum pairwise distance is 11.5 degrees. There is a pronounced 1st order trend in the north/south direction as we might expect with air temperatures.

Remove the trend and examine the residuals.
```{r}
plot(t.gdf, trend = "1st")
```

There is some evidence of a 2nd-order trend in the west-east direction. 
```{r}
plot(t.gdf, trend = "2nd")
```

By specifying a higher order trend, the lower order trends are taken care of. The distribution of residuals is approximately normal as we might expect. The data are monthly means.

Step 2: Compute empirical variograms. Check for anisotropy by plotting directional variograms.
```{r}
par(mfrow = c(1, 1))
plot(variog4(t.gdf, trend = "2nd", 
             max.dist = 5.5), 
     omni = TRUE, legend = FALSE)
```

There is no strong evidence to reject isotropy.

Step 3: Fit a variogram model to the data. Here we consider several likelihood fits to an exponential model and examine the AIC for final parameter selection. 

The AIC is used as a selection criterion and is a function of the maximized likelihood function but includes a penalty for model complexity that favors simpler models. Recall the best fit has the largest log-likelihood and smallest AIC.  

Set initial values for the sill and range. From the variograms start with 3 for the sill and 4 for the range.
```{r}
iv <- c(3, 4)  
summary(likfit(t.gdf, 
               ini = iv, 
               cov.model = "exp", 
               trend = "2nd",
               fix.nug = TRUE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, 
               ini = iv, 
               cov.model = "exp", 
               trend = "2nd",
               fix.nug = FALSE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, ini = iv, 
               cov.model = "sph", 
               trend = "2nd",
               fix.nug = TRUE, 
               message = FALSE))$likelihood$AIC
summary(likfit(t.gdf, ini = iv, 
               cov.model = "sph", 
               trend = "2nd",
               fix.nug = FALSE, 
               message = FALSE))$likelihood$AIC
```

It appears that a good variogram model would be on the residuals of a 2nd order trend using an exponential function with fixed nugget equal to zero. A spherical function with a nugget is also a reasonable model.

To obtain the model parameters, type
```{r}
likfit(t.gdf, ini = iv, cov.model = "exp", 
       trend = "2nd", fix.nug = TRUE)
likfit(t.gdf, ini = iv, cov.model = "sph", 
       trend = "2nd", fix.nug = FALSE)
```

Plot the competing models on the empirical variogram.
```{r}
plot(variog(t.gdf, trend = "2nd", 
            uvec = seq(0, 5.5, l = 29)))
lines.variomodel(cov.model = "exp", 
                 cov.pars = c(2.114, .4139), 
                 nug = 0, max.dist = 5.5, col = "red")
lines.variomodel(cov.model = "sph", 
                 cov.pars = c(1.638, 1.307),
                 nug =.5, max.dist = 5.5, col = "green")
```

Save the models.
```{r}
modelE <- likfit(t.gdf, ini = iv, cov.model = "exp", 
                  trend = "2nd", fix.nug = TRUE)
modelS <- likfit(t.gdf, ini = iv, cov.model = "sph", 
                  trend = "2nd", fix.nug = FALSE)
```

Step 4: Create an interpolated surface. We create a grid of locations at which we want the temperatures to be interpolated. Here we use the `expand.grid` function where the arguments are the sequence of longitudes and latitudes, respectively. We then use the `krige.conv()` function to interpolate the values to the grid. We save the interpolation in `kcE` when we use the exponential variogram model to weight the observations and save the interpolation in `kcS` when we use the spherical model to weight the observations.
```{r}
pgrid.df<- expand.grid(lon = seq(-99, -88, l = 224), 
                       lat = seq(38.4, 45.4, l = 136))
kcE <- krige.conv(t.gdf, 
                  loc = pgrid.df, 
                  krige = krige.control(trend.d = "2nd", 
                                       trend.l = "2nd",
                                       obj.m = modelE))
kcS <- krige.conv(t.gdf, 
                  loc = pgrid.df,
                  krige = krige.control(trend.d = "2nd", 
                                       trend.l = "2nd",
                                       obj.m = modelS))
```

Plot the interpolated surface generated using the exponential variogram. function.
```{r}
pgrid.df$temp <- kcE$predict
pgrid.df$var <- kcE$krige.var

spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(t.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9, palette = "OrRd") +
tm_shape(sts) +
  tm_borders() +
tm_shape(t.sf) +
  tm_text("temp", size = .5)
```

Plot the interpolated surface generated using the spherical variogram.
```{r}
pgrid.df$temp <- kcS$predict
pgrid.df$var <- kcS$krige.var

spdf <- pgrid.df
coordinates(spdf) <- c("lon", "lat")
spdf <- as(spdf, "SpatialPixelsDataFrame")
proj4string(spdf) <- st_crs(t.sf)$proj4string

library(raster)
r <- raster(spdf)

tm_shape(r) +
  tm_raster(n = 9, palette = "OrRd") +
tm_shape(sts) +
  tm_borders() +
tm_shape(t.sf) +
  tm_text("temp", size = .5)
```

The model with a non-zero nugget is smoother. The greater the nugget relative to the sill (relative nugget effect), the smoother the interpolation.

## Combine prediction and uncertainty in a single map

See `Pixelate.Rmd`.

## Synthetic data

Synthetic data are useful as input to a deterministic model. An example is rainfall as input to spatially-distributed rainfall-runoff model. Interpolated values of precipitation and their variances might be of little value, but running the rainfall-runoff model with a large number of simulated rainfall fields can give a realistic assessment of the uncertainty in runoff arising from the variability in the rainfall.

The `grf()` function in the {geoR} package generates synthetic data from Gaussian random fields on regular or irregular sets of locations. For example, to generate 100 randomly spaced points with values at the points being a sample from a Gaussian random field with an exponential variogram (default with zero nugget) having a sill of 1 and a range of .25, type
```{r}
set.seed(3042)
sim1 <- grf(100, cov.pars = c(1, .25))
```

Plot the points and the variograms.
```{r}
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
points.geodata(sim1, 
               main = "simulated locations and values")
plot(sim1, max.dist = .5, 
     main = "true and empirical variograms")
```

The random fields can be put on a regular grid.
```{r}
sim2 <- grf(441, grid = "reg", cov.pars = c(1, .25))
image(sim2, col = gray(seq(1, .1, l = 30)))
sim3 <- grf(4441, grid = "reg", cov.pars = c(1, .25))
image(sim3, col = gray(seq(1, .1, l = 30)))
par(mfrow = c(1, 1))
```

For an even higher resolution simulation it is necessary to have the {RandomFields} package.
```{r, eval=FALSE}
library(RandomFields)
sim4 <- grf(40401, grid = "reg", cov.pars = c(10, .2))
image(sim4, main = "simulation on a fine grid", 
      col = gray(seq(1, .1, l = 30)))
```